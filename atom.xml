<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Frey&#39;s blog</title>
  
  <subtitle>Frey&#39;s blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.vhcffh.com/"/>
  <updated>2020-02-06T10:02:03.067Z</updated>
  <id>https://www.vhcffh.com/</id>
  
  <author>
    <name>Frey</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数列求解总结</title>
    <link href="https://www.vhcffh.com/%E6%95%B0%E5%88%97%E6%B1%82%E8%A7%A3%E6%80%BB%E7%BB%93/"/>
    <id>https://www.vhcffh.com/数列求解总结/</id>
    <published>2020-02-06T09:56:00.000Z</published>
    <updated>2020-02-06T10:02:03.067Z</updated>
    
    <content type="html"><![CDATA[<iframe src="https://www.desmos.com/calculator/eh5hjtawf5?embed" width="100%" height="900px" frameborder="0"></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;iframe src=&quot;https://www.desmos.com/calculator/eh5hjtawf5?embed&quot; width=&quot;100%&quot; height=&quot;900px&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;
      
    
    </summary>
    
      <category term="基础知识" scheme="https://www.vhcffh.com/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="数列" scheme="https://www.vhcffh.com/tags/%E6%95%B0%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>Transformers</title>
    <link href="https://www.vhcffh.com/2019/Transformers/"/>
    <id>https://www.vhcffh.com/2019/Transformers/</id>
    <published>2019-12-21T09:44:00.000Z</published>
    <updated>2020-01-25T09:57:45.826Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a>（与<em>pytorch-transformers</em>和<em>pytorch-pretrained-bert</em>相似）是python的一个库，它提供了用于自然语言理解(NLU)和自然语言生成(NLG)的多种预训练模型（BERT，GPT-2，RoBERTa，XLM，DistilBert，XLNet…..），为100多种语言提供了超过32种的预训练模型，并实现Tensorflow 2.0和Pytorch的深度互操作。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>与pytorch-transformers一样容易使用</li><li>如Keras一样强大简洁</li><li>在NLU和NLG任务上有很好的表现</li><li>容易学习</li></ul><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ol><li><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">BERT</a> (from Google) released with the paper <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li><li><a href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener">GPT</a> (from OpenAI) released with the paper <a href="https://blog.openai.com/language-unsupervised" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.</li><li><a href="https://blog.openai.com/better-language-models" target="_blank" rel="noopener">GPT-2</a> (from OpenAI) released with the paper <a href="https://blog.openai.com/better-language-models" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a> by Alec Radford<em>, Jeffrey Wu</em>, Rewon Child, David Luan, Dario Amodei<strong> and Ilya Sutskever</strong>.</li><li><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">Transformer-XL</a> (from Google/CMU) released with the paper <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> by Zihang Dai<em>, Zhilin Yang</em>, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</li><li><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">XLNet</a> (from Google/CMU) released with the paper <a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> by Zhilin Yang<em>, Zihang Dai</em>, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.</li><li><a href="https://github.com/facebookresearch/XLM" target="_blank" rel="noopener">XLM</a> (from Facebook) released together with the paper <a href="https://arxiv.org/abs/1901.07291" target="_blank" rel="noopener">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li><li><a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank" rel="noopener">RoBERTa</a> (from Facebook), released together with the paper a <a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener">Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li><li><a href="https://huggingface.co/transformers/model_doc/distilbert.html" target="_blank" rel="noopener">DistilBERT</a> (from HuggingFace) released together with the paper <a href="https://arxiv.org/abs/1910.01108" target="_blank" rel="noopener">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into <a href="https://github.com/huggingface/transformers/tree/master/examples/distillation" target="_blank" rel="noopener">DistilGPT2</a>.</li><li><a href="https://github.com/pytorch/fairseq/tree/master/examples/ctrl" target="_blank" rel="noopener">CTRL</a> (from Salesforce), released together with the paper <a href="https://www.github.com/salesforce/ctrl" target="_blank" rel="noopener">CTRL: A Conditional Transformer Language Model for Controllable Generation</a> by Nitish Shirish Keskar<em>, Bryan McCann</em>, Lav R. Varshney, Caiming Xiong and Richard Socher.</li><li><a href="https://huggingface.co/transformers/model_doc/camembert.html" target="_blank" rel="noopener">CamemBERT</a> (from FAIR, Inria, Sorbonne Université) released together with the paper <a href="https://arxiv.org/abs/1911.03894" target="_blank" rel="noopener">CamemBERT: a Tasty French Language Model</a> by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suarez, Yoann Dupont, Laurent Romary, Eric Villemonte de la Clergerie, Djame Seddah, and Benoît Sagot.</li><li><a href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener">ALBERT</a> (from Google Research), released together with the paper a <a href="https://arxiv.org/abs/1909.11942" target="_blank" rel="noopener">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.</li><li><a href="https://github.com/pytorch/fairseq/tree/master/examples/xlmr" target="_blank" rel="noopener">XLM-RoBERTa</a> (from Facebook AI), released together with the paper <a href="https://arxiv.org/abs/1911.02116" target="_blank" rel="noopener">Unsupervised Cross-lingual Representation Learning at Scale</a> by Alexis Conneau<em>, Kartikay Khandelwal</em>, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</li></ol><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>对于每个模型，这个库主要涉及3个类</p><ul><li>model classes：提供模型的结构，如：<code>BertModel</code></li><li>configuration classes：存储构建模型的所有参数，如<code>BertConfig</code></li><li>tokenizer classes：提供模型的词汇表和用于对字符串解码/编码的方法，如<code>BertTokenizer</code></li></ul><p>所有的实例可以通过<code>from_pretrained()</code>加载预训练模型，通过<code>save_pretrained()</code>保存模型</p><h3 id="BERT-example"><a href="#BERT-example" class="headerlink" title="BERT example"></a>BERT example</h3><p>从字符串获得输入向量</p><pre><code class="lang-python">import torchfrom transformers import BertTokenizer, BertModel, BertForMaskedLM# OPTIONAL: if you want to have more information on what&#39;s happening under the hood, activate the logger as followsimport logginglogging.basicConfig(level=logging.INFO)# Load pre-trained model tokenizer (vocabulary)tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)# 中文bert模型&#39;bert-base-chinese&#39;# bert 特殊的符号# &#39;[MASK]&#39; 用于mask language model# &#39;[CLS]&#39; 开头，用于分类的标记# &#39;[PAD]&#39; # &#39;[SEP]&#39; 结尾，句子分隔符# &#39;[UNK]&#39;# Tokenize inputtext = &quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;tokenized_text = tokenizer.tokenize(text)# Mask a token that we will try to predict back with `BertForMaskedLM`masked_index = 8tokenized_text[masked_index] = &#39;[MASK]&#39;assert tokenized_text == [&#39;[CLS]&#39;, &#39;who&#39;, &#39;was&#39;, &#39;jim&#39;, &#39;henson&#39;, &#39;?&#39;, &#39;[SEP]&#39;, &#39;jim&#39;, &#39;[MASK]&#39;, &#39;was&#39;, &#39;a&#39;, &#39;puppet&#39;, &#39;##eer&#39;, &#39;[SEP]&#39;]# Convert token to vocabulary indicesindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]# Convert inputs to PyTorch tensorstokens_tensor = torch.tensor([indexed_tokens])segments_tensors = torch.tensor([segments_ids])</code></pre><p>对输入进行编码</p><pre><code class="lang-python"># Load pre-trained model (weights)model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)# Set the model in evaluation mode to deactivate the DropOut modules# This is IMPORTANT to have reproducible results during evaluation!model.eval()# If you have a GPU, put everything on cudatokens_tensor = tokens_tensor.to(&#39;cuda&#39;)segments_tensors = segments_tensors.to(&#39;cuda&#39;)model.to(&#39;cuda&#39;)# Predict hidden states features for each layerwith torch.no_grad():    # See the models docstrings for the detail of the inputs    outputs = model(tokens_tensor, token_type_ids=segments_tensors)    # Transformers models always output tuples.    # See the models docstrings for the detail of all the outputs    # In our case, the first element is the hidden state of the last layer of the Bert model    encoded_layers = outputs[0]# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)</code></pre><p>使用<code>BertForMaskedLM</code>预测masked token</p><pre><code class="lang-python"># Load pre-trained model (weights)model = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)model.eval()# If you have a GPU, put everything on cudatokens_tensor = tokens_tensor.to(&#39;cuda&#39;)segments_tensors = segments_tensors.to(&#39;cuda&#39;)model.to(&#39;cuda&#39;)# Predict all tokenswith torch.no_grad():    outputs = model(tokens_tensor, token_type_ids=segments_tensors)    predictions = outputs[0]# confirm we were able to predict &#39;henson&#39;predicted_index = torch.argmax(predictions[0, masked_index]).item()predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]assert predicted_token == &#39;henson&#39;</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1.<a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">https://huggingface.co/transformers/index.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/transformers&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;transformers&lt;/a&gt;（与&lt;em&gt;pytorch-transformers&lt;/em&gt;和&lt;em&gt;
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://www.vhcffh.com/tags/NLP/"/>
    
      <category term="transformer" scheme="https://www.vhcffh.com/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记</title>
    <link href="https://www.vhcffh.com/2019/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%9C%AF%E8%AF%AD/"/>
    <id>https://www.vhcffh.com/2019/机器学习中的术语/</id>
    <published>2019-11-15T12:57:47.000Z</published>
    <updated>2019-12-06T12:06:42.934Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>准确率:所有样本中预测正确的占比</p><script type="math/tex; mode=display">accuracy =\frac {TP+TN}{TP+TN+FP+FN} = \frac {T}{T+F}</script><p>精确率:<strong>预测为正的样本</strong>中真正的正样本占比</p><script type="math/tex; mode=display">precision = \frac {TP}{TP+FP} = \frac {TP}{P'}</script><p>召回率:<strong>正样本</strong>中预测为正的占比</p><script type="math/tex; mode=display">recall = \frac{TP}{TP+FN} = \frac{TP}{P}</script><p>F1:精确率和召回率的<strong>调和均值</strong></p><script type="math/tex; mode=display">\begin{align*}\frac{2}{F_1} & = \frac{1}{precision} + \frac{1}{recall}\\F_1 & = \frac{2\cdot precision \cdot recall}{precision+recall}\\F_1 & = \frac{2TP}{2TP + FP + FN} \\F_1 & = \frac{2TP}{P' + P} \\\end{align*}</script><p>F-score:</p><script type="math/tex; mode=display">F_{score}=(1+\beta^2)\cdot \frac{precision \cdot recall}{\beta^2\cdot precision + recall}</script><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">P</th><th style="text-align:center">N</th></tr></thead><tbody><tr><td style="text-align:center">P’</td><td style="text-align:center">TP</td><td style="text-align:center">FP</td></tr><tr><td style="text-align:center">N’</td><td style="text-align:center">FN</td><td style="text-align:center">TN</td></tr></tbody></table></div><h2 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h2><h3 id="BLEU-Bilingual-Evaluation-understudy"><a href="#BLEU-Bilingual-Evaluation-understudy" class="headerlink" title="BLEU(Bilingual Evaluation understudy)"></a>BLEU(Bilingual Evaluation understudy)</h3><script type="math/tex; mode=display">CP_n(C,S)=\frac {\sum_i\sum_k\min(h_k(c_i),max_{j \in m}h_k(s_{ij}))}{\sum_i\sum_kh_k(c_j)}</script><p><strong>惩罚因子BP(Brevity Penalty)</strong></p><script type="math/tex; mode=display">b(C,S)=\begin{cases}1,                        &l_c \lt l_s \\e^{1-\frac{l_s}{l_c}},    &l_c \geq l_s\end{cases}</script><script type="math/tex; mode=display">BLEU_N(C,S)=b(C,S)\exp(\sum_{n=1}^N\omega_n\log CP_n(C,S))</script><p>机器翻译</p><h3 id="ROUGE-Recall-Oriented-Understudy-for-Gisting-Evaluation"><a href="#ROUGE-Recall-Oriented-Understudy-for-Gisting-Evaluation" class="headerlink" title="ROUGE(Recall-Oriented Understudy for Gisting Evaluation)"></a>ROUGE(Recall-Oriented Understudy for Gisting Evaluation)</h3><div class="table-container"><table><thead><tr><th style="text-align:center">ROUGE-N</th><th style="text-align:left">基于N-gram公现性统计</th></tr></thead><tbody><tr><td style="text-align:center">ROUGE-L</td><td style="text-align:left">基于最长公有子句共现性精确度和召回率Fmeasure统计</td></tr><tr><td style="text-align:center">ROUGE-W</td><td style="text-align:left">代权重的最长公有子句共现性精确度和召回率Fmeasure统计</td></tr><tr><td style="text-align:center">ROUGE-S</td><td style="text-align:left">不连续二元组共现性精确度和召回率Fmeasure统计</td></tr></tbody></table></div><p><strong>ROUGE-N</strong></p><script type="math/tex; mode=display">ROUGE-N=\frac {\sum_{S \in ReferencesSummaries}\sum_{gram_n\in S}Count_{match}(gram_n)}{\sum_{S \in ReferencesSummaries}\sum_{gram_n\in S}Count(gram_n)}</script><p><strong>ROUGE-L</strong></p><p>最长公共子句longest common subsequence(LCS)</p><script type="math/tex; mode=display">R_{lcs}=\frac {LCS(X,Y)}{m} ,m=len(X)</script><script type="math/tex; mode=display">P_{lcs}=\frac {LCS(X,Y)}{n} ,n=len(Y)</script><script type="math/tex; mode=display">F_{lcs}=\frac{(1+\beta^2)R_{lcs}P_{lcs}}{R_{lcs}+\beta^2P_{lcs}}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h2&gt;&lt;p&gt;准确率:所有样本中预测正确的占比&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
accuracy =
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="边缘计算" scheme="https://www.vhcffh.com/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>RNN的前向传播与反向传播</title>
    <link href="https://www.vhcffh.com/2019/RNN%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>https://www.vhcffh.com/2019/RNN的前向传播与反向传播/</id>
    <published>2019-10-04T08:30:00.000Z</published>
    <updated>2020-01-25T10:01:57.170Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>pytorch 中RNN的<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN" target="_blank" rel="noopener">数学公式</a></p><script type="math/tex; mode=display">h_t = \text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})</script><h2 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h2><p>需要更新的参数有</p><script type="math/tex; mode=display">W_{ih}, \ \ \ \ b_{ih}, \ \ \ \ W_{hh}, \ \ \ \  b_{hh}</script><p>在这式子中要用到的偏导数</p><script type="math/tex; mode=display">\frac {\partial h_t} {\partial W_{ih}}, \ \ \ \\frac {\partial h_t} {\partial b_{ih}}, \ \ \ \\frac {\partial h_t} {\partial W_{hh}}, \ \ \ \\frac {\partial h_t} {\partial b_{hh}} \ \ \ \</script><script type="math/tex; mode=display">\begin{align}\frac {\partial h_t} {\partial W_{ih}} &= x_t + \frac {\partial h_{t-1}}{\partial W_{ih}} \\&= \sum_{i=1}^tx_i\end{align}</script><script type="math/tex; mode=display">\frac {\partial h_t} {\partial b_{ih}} = \sum_{i=0}^tW_{hh}^i</script><script type="math/tex; mode=display">\begin{align}\frac {\partial h_t} {\partial W_{hh}}&= h_{t-1} + \frac {\partial h_{t-1}}{\partial W_{hh}}\\&= \sum_{i=0}^th_i\end{align}</script><script type="math/tex; mode=display">\frac {\partial h_t} {\partial b_{hh}} = \sum_{i=0}^tW_{hh}^i</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前向传播&quot;&gt;&lt;a href=&quot;#前向传播&quot; class=&quot;headerlink&quot; title=&quot;前向传播&quot;&gt;&lt;/a&gt;前向传播&lt;/h2&gt;&lt;p&gt;pytorch 中RNN的&lt;a href=&quot;https://pytorch.org/docs/stable/_modules
      
    
    </summary>
    
    
      <category term="RNN" scheme="https://www.vhcffh.com/tags/RNN/"/>
    
      <category term="反向传播" scheme="https://www.vhcffh.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>矩阵中的求导</title>
    <link href="https://www.vhcffh.com/2019/%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E6%B1%82%E5%AF%BC/"/>
    <id>https://www.vhcffh.com/2019/矩阵中的求导/</id>
    <published>2019-10-02T12:46:00.000Z</published>
    <updated>2020-01-25T10:02:17.071Z</updated>
    
    <content type="html"><![CDATA[<h3 id="标量对向量求导"><a href="#标量对向量求导" class="headerlink" title="标量对向量求导"></a>标量对向量求导</h3><script type="math/tex; mode=display">y = f(x_1,\cdots,x_i,\cdots,x_n) \\X = [x_1,\cdots,x_i,\cdots,x_n]</script><script type="math/tex; mode=display">\frac {\partial y}{\partial X} = [\frac {\partial f}{\partial x_1},\cdots,\frac {\partial f}{\partial x_i},\cdots,\frac {\partial f}{\partial x_n}]</script><a id="more"></a><h3 id="向量对向量求导"><a href="#向量对向量求导" class="headerlink" title="向量对向量求导"></a>向量对向量求导</h3><script type="math/tex; mode=display">Y = [f_1(x_1,\cdots,x_i,\cdots,x_n),\cdots,f_i(x_1,\cdots,x_i,\cdots,x_n),\cdots,f_m(x_1,\cdots,x_i,\cdots,x_n)]</script><script type="math/tex; mode=display">X = [x_1,\cdots,x_i,\cdots,x_n]</script><script type="math/tex; mode=display">\frac {\partial Y}{\partial X} = \begin{bmatrix}\frac {\partial f_1}{\partial x_1} & \frac {\partial f_2}{\partial x_1} & \cdots & \frac {\partial f_m}{\partial x_1}\\\frac {\partial f_1}{\partial x_2} & \frac {\partial f_2}{\partial x_2} & \cdots & \frac {\partial f_m}{\partial x_1}\\\vdots & \vdots & \ddots & \vdots\\\frac {\partial f_1}{\partial x_n} & \cdots & \cdots & \frac {\partial f_m}{\partial x_n}\\\end{bmatrix}</script><p>这是一个n行m列的矩阵,有时也会写成m行n列,都是一样的,区别在于加不加转置</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;标量对向量求导&quot;&gt;&lt;a href=&quot;#标量对向量求导&quot; class=&quot;headerlink&quot; title=&quot;标量对向量求导&quot;&gt;&lt;/a&gt;标量对向量求导&lt;/h3&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
y = f(x_1,\cdots,x_i,\cdots,x_n) \\
X = [x_1,\cdots,x_i,\cdots,x_n]&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac {\partial y}{\partial X} = [\frac {\partial f}{\partial x_1},\cdots,\frac {\partial f}{\partial x_i},\cdots,\frac {\partial f}{\partial x_n}]&lt;/script&gt;
    
    </summary>
    
      <category term="基础知识" scheme="https://www.vhcffh.com/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="矩阵求导" scheme="https://www.vhcffh.com/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    
  </entry>
  
  <entry>
    <title>交叉熵损失的反向传播</title>
    <link href="https://www.vhcffh.com/2019/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>https://www.vhcffh.com/2019/交叉熵损失的反向传播/</id>
    <published>2019-10-02T11:24:56.000Z</published>
    <updated>2019-10-02T14:23:14.197Z</updated>
    
    <content type="html"><![CDATA[<p>对于一个单标签多分类问题,假设网络的输出层的输入为$Z_{in}=[z_1,\cdots,z_i,\cdots,z_n]$,输出为$\hat Y=[\hat y_1,\cdots,\hat y_i,\cdots,\hat y_n]$,真实类标签为$Y = [y_1,\cdots,y_i,\cdots,y_n]$,$n$为类别数(输出层神经元数),通常有:</p><script type="math/tex; mode=display">\hat Y = Softmax(Z_{in})\label{1}\tag{1}</script><script type="math/tex; mode=display">\hat y_i = \frac {e^{z_i}}{\sum_{j=0}^n e^{z_j}}\label{2}\tag{2}</script><a id="more"></a><p>其中$Softmax​$为:</p><script type="math/tex; mode=display">Softmax(Z_{in}) = [\cdots,\frac {e^{z_i}}{\sum_{j=1}^n e^{z_j}},\cdots]\label{3}\tag{3}</script><h3 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h3><script type="math/tex; mode=display">Loss(Y,\hat Y) = -\sum_{i=1}^ny_i*\ln(\hat y_i)\label{4}\tag{4}</script><p>损失对神经网络输出的偏导(<a href="https://www.vhcffh.com/2019/矩阵中的求导/">标量对向量求偏导</a>)为:</p><script type="math/tex; mode=display">\frac {\partial Loss(Y,\hat Y)}{\partial \hat Y} = [-\frac {y_1}{\hat y_1},\cdots,-\frac {y_i}{\hat y_i},\cdots,-\frac {y_n}{\hat y_n}]\label{5}\tag{5}</script><p>后向传播推导中遇到的所有量都是变量,最终的目的是找到损失关于某变量的偏导,程序中也只用这个公式求得对应输入点的梯度</p><h3 id="Softmax的偏导"><a href="#Softmax的偏导" class="headerlink" title="Softmax的偏导"></a>Softmax的偏导</h3><p>求$\hat y_i​$对$z_i​$的偏导,根据$\eqref{2}​$可得:</p><p>求$\hat Y$对$Z$的偏导(<a href="https://www.vhcffh.com/2019/矩阵中的求导/">向量对向量求导</a>)</p><p>这里把$y$的坐标写作$k$</p><p>$k=i$时:</p><script type="math/tex; mode=display">\begin{split}\frac {\partial \hat y_i}{\partial z_i}&= e^{z_i} * \frac {1}{\sum_{j=1}^ne^{z_j}} +   e^{z_i} * (-\frac {1}{(\sum_{j=1}^n e^{z_j})^2}) * e^{z_i} \\&= \frac {e^{z_i}}{\sum_{j=1}^n e^{z_j}} - (\frac {e^{z_i}}{\sum_{j=1}^n e^{z_j}})^2 \\&= \hat y_i - \hat y_i^2\end{split}\label{6}\tag{6}</script><p>$k \neq i$时:</p><script type="math/tex; mode=display">\begin{split}\frac {\partial \hat y_k}{\partial z_i}&= e^{z_k} * (-\frac {1}{(\sum_{j=1}^n e^{z_j})^2}) * e^{z_i} \\&= -\frac {e^{z_k}*e^{z_i}}{(\sum_{j=1}^n e^{z_j})^2} \\&= - \hat y_k\hat y_i\end{split}\label{7}\tag{7}</script><p>写成矩阵:</p><script type="math/tex; mode=display">\frac {\partial \hat Y}{\partial Z_{in}} = \begin{bmatrix}\hat y_1-\hat y_1^2 & -\hat y_2\hat y_1 & \cdots & -\hat y_n\hat y_1\\-\hat y_1\hat y_2 & \hat y_2-\hat y_2^2 & \cdots & -\hat y_n\hat y_2\\\vdots & \vdots & \ddots & \vdots\\-\hat y_1\hat y_n & -\hat y_2\hat y_n & \cdots & \hat y_n-\hat y_n^2\\\end{bmatrix}\label{8}\tag{8}</script><p>这是一个对称矩阵,在链式求导时加不加转置都一样</p><p>根据$\eqref{5}$和$\eqref{8}$,损失$L$对输入$Z$的偏导(<a href="https://www.vhcffh.com/2019/矩阵中的求导/">标量对向量求偏导</a>):</p><script type="math/tex; mode=display">\begin{align}\frac {\partial L(Y,\hat Y)}{\partial Z}&= \frac {\partial L(Y,\hat Y)}{\partial \hat Y} (\frac {\partial \hat Y}{\partial Z})^T\\&= [-\frac {y_1}{\hat y_1},\cdots,-\frac {y_i}{\hat y_i},\cdots,-\frac {y_n}{\hat y_n}]\begin{bmatrix}\hat y_1-\hat y_1^2 & -\hat y_2\hat y_1 & \cdots & -\hat y_n\hat y_1\\-\hat y_1\hat y_2 & \hat y_2-\hat y_2^2 & \cdots & -\hat y_n\hat y_2\\\vdots & \vdots & \ddots & \vdots\\-\hat y_1\hat y_n & -\hat y_2\hat y_n & \cdots & \hat y_n-\hat y_n^2\\\end{bmatrix}^T\\&= [(\hat y_1-1)y_1+\hat y_1y_2+\cdots+\hat y_1y_n,\hat y_2y_1+(\hat y_2-1)y_2+\cdots+\hat y_1y_n,\cdots]\\&= [\hat y_1\sum_{i=1}^ny_i-y_1,\cdots,\hat y_j\sum_{i=1}^ny_i-y_j,\cdots,\hat y_n\sum_{i=1}^ny_i-y_n]\\&= [\hat y_1-y_1,\cdots,\hat y_j-y_j,\cdots,\hat y_n-y_n]\ \ \ \ (\sum_{i=1}^ny_i=1)\\&= \hat Y-Y\end{align}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于一个单标签多分类问题,假设网络的输出层的输入为$Z_{in}=[z_1,\cdots,z_i,\cdots,z_n]$,输出为$\hat Y=[\hat y_1,\cdots,\hat y_i,\cdots,\hat y_n]$,真实类标签为$Y = [y_1,\cdots,y_i,\cdots,y_n]$,$n$为类别数(输出层神经元数),通常有:&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat Y = Softmax(Z_{in})\label{1}\tag{1}&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat y_i = \frac {e^{z_i}}{\sum_{j=0}^n e^{z_j}}\label{2}\tag{2}&lt;/script&gt;
    
    </summary>
    
      <category term="神经网络" scheme="https://www.vhcffh.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="反向传播" scheme="https://www.vhcffh.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
      <category term="交叉熵损失" scheme="https://www.vhcffh.com/tags/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1/"/>
    
      <category term="Softmax" scheme="https://www.vhcffh.com/tags/Softmax/"/>
    
  </entry>
  
  <entry>
    <title>日常笔记-1</title>
    <link href="https://www.vhcffh.com/2019/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0-1/"/>
    <id>https://www.vhcffh.com/2019/日常笔记-1/</id>
    <published>2019-09-24T09:37:41.000Z</published>
    <updated>2019-10-03T13:43:19.921Z</updated>
    
    <content type="html"><![CDATA[<h3 id="pickle序列化与反序列化"><a href="#pickle序列化与反序列化" class="headerlink" title="pickle序列化与反序列化"></a>pickle序列化与反序列化</h3><pre><code class="lang-python">import pickle as plkplk.dump(obj,file) # 将序列化后的二进制写入文件plk.dumps(obj) # 返回一个二进制序列plk.load(file) # 读文件对象中的二进制,转化成对象返回plk.loads(bytes_object) # 将二进制序列转化成对象obj1=dict(),obj2=dict()plk.dump(obj1,file)plk.dump(obj2,file)obj1=plk.load(file)obj2=plk.load(file)plk.load(file) # EOFError: Ran out of input</code></pre><h3 id="文本分类与关键词排名"><a href="#文本分类与关键词排名" class="headerlink" title="文本分类与关键词排名"></a>文本分类与关键词排名</h3><p>文本分类(Text Categorization, TC)</p><p>特征-倒文本频率(Term-Inverse Document Frequency, TF-IDF)</p><p>信息增益(Information Gain, IG)</p><p>互信息(Multi-Information, MI)</p><p>卡方统计 (Chi-square, CHI)</p><p>期 望 交 叉 熵(Expected Cross Entropy, ECE )</p><p>文本证据权( Weight of Evidence for Text, WET)</p><p>TextRank(与PageRank的原理相同)</p><p>HITS(Hyperlink-Induced Topic Search)</p><h4 id="卡方统计模型-CHI"><a href="#卡方统计模型-CHI" class="headerlink" title="卡方统计模型(CHI)"></a>卡方统计模型(CHI)</h4><p>卡方分布</p><script type="math/tex; mode=display">X = \sum_{i=1}^kZ_i^2</script><p>其中$Z_i  \sim N(0,1)$(标准正太分布)</p><p>则$X$被称为服从自由度为$k$的卡方分布,记作:</p><script type="math/tex; mode=display">X\ \sim \ \chi ^{2}(k)\,</script><p>卡方分布的概率密度函数:</p><script type="math/tex; mode=display">f_{k}(x)={\frac { {\frac {1}{2}}^{\frac {k}{2}} } {\Gamma ({ \frac {k}{2} })} }x^{ {\frac {k}{2} }-1}e^{\frac {-x}{2} }</script><p>期望和方差:</p><script type="math/tex; mode=display">E(X) = k \\Var(X) = 2k</script><p>模型通过观察值和理论值的偏差来确定理论是否正确</p><p>基于词袋模型(文档由词组成,不考虑词的顺序)</p><p>通过考虑词语与类别的相关度把由词语组成的文档归为某一类别</p><h4 id="HITS"><a href="#HITS" class="headerlink" title="HITS"></a>HITS</h4><ul><li>一个高质量的权威页面会被很多高质量的枢纽页面所指向。</li><li>一个高质量的枢纽页面会指向很多高质量的权威页面。</li></ul><p>由PageRank算法演变而来,将当前页面出链也做为考虑当前页面的重要性的一个因素</p><p>参考</p><p><a href="https://github.com/wangjiang0624/Note/blob/master/MachineLearning/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md" target="_blank" rel="noopener">https://github.com/wangjiang0624/Note/blob/master/MachineLearning/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md</a></p><p><a href="http://blog.zhengyi.one/PageRank-HITS.html" target="_blank" rel="noopener">http://blog.zhengyi.one/PageRank-HITS.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;pickle序列化与反序列化&quot;&gt;&lt;a href=&quot;#pickle序列化与反序列化&quot; class=&quot;headerlink&quot; title=&quot;pickle序列化与反序列化&quot;&gt;&lt;/a&gt;pickle序列化与反序列化&lt;/h3&gt;&lt;pre&gt;&lt;code class=&quot;lang-py
      
    
    </summary>
    
      <category term="日常笔记" scheme="https://www.vhcffh.com/categories/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="pickle" scheme="https://www.vhcffh.com/tags/pickle/"/>
    
  </entry>
  
  <entry>
    <title>nvidia显卡驱动错误</title>
    <link href="https://www.vhcffh.com/2019/nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E9%94%99%E8%AF%AF/"/>
    <id>https://www.vhcffh.com/2019/nvidia显卡驱动错误/</id>
    <published>2019-09-09T11:40:00.000Z</published>
    <updated>2020-01-25T13:35:13.560Z</updated>
    
    <content type="html"><![CDATA[<p>系统使用<code>bumblebee</code>实现双显卡(N卡和集成显卡)切换，<br>同时有N卡的开源驱动nouveau和专有显卡驱动nvidia</p><p>使用命令</p><pre><code class="lang-sh">optirun glxspheres64# 或者optirun glxspher</code></pre><p>出现错误<code>[XORG] (EE) Failed to load module &quot;nouveau&quot;</code><br>通过命令<code>lsmod |grep nouveau</code><br>显示模块已经加载</p><p>通过<code>/etc/modprobe.d/blacklist.conf</code>文件禁用<code>nouveau</code>(需要重建内核<code>mkinitcpio -P</code>并重启),但<code>nouveau</code>驱动仍然加载了，<code>nvidia</code>未加载</p><p>解决方法修改文件<code>/etc/bumblebee/xorg.conf.nouveau</code><br>去掉<code>BusID &quot;PCI:01:00:0&quot;</code>的注释<br><code>PCI:01:00:0</code>通过命令<code>lspci | egrep (3D|VGA)</code></p><p>重启后<code>nvidia</code>驱动加载成功,<code>nouveau</code>没有加载</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;系统使用&lt;code&gt;bumblebee&lt;/code&gt;实现双显卡(N卡和集成显卡)切换，&lt;br&gt;同时有N卡的开源驱动nouveau和专有显卡驱动nvidia&lt;/p&gt;
&lt;p&gt;使用命令&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;lang-sh&quot;&gt;optirun glxspher
      
    
    </summary>
    
      <category term="Linux" scheme="https://www.vhcffh.com/categories/Linux/"/>
    
    
      <category term="nvidia" scheme="https://www.vhcffh.com/tags/nvidia/"/>
    
      <category term="nouveau" scheme="https://www.vhcffh.com/tags/nouveau/"/>
    
  </entry>
  
  <entry>
    <title>基于统计的分词方法</title>
    <link href="https://www.vhcffh.com/2019/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95/"/>
    <id>https://www.vhcffh.com/2019/基于统计的分词方法/</id>
    <published>2019-09-08T10:26:38.000Z</published>
    <updated>2019-09-17T07:02:55.564Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-n元语言模型-n-gram"><a href="#1-n元语言模型-n-gram" class="headerlink" title="1.n元语言模型(n-gram)"></a>1.n元语言模型(n-gram)</h2><p>假设$S$表示长度为$i$，由$(w_1,w_2,\dots,w_m)$字序列组成的句子，则代表$S​$的概率为：</p><script type="math/tex; mode=display">P(S) = P(w_1,w_2,\dots,w_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_2,w_1)\cdots P(w_i|w_1,w_2,\dots,w_{m-1})</script><ul><li><p>$n=1$,uni-gram</p><script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i)</script></li><li><p>$n=2$,bi-gram</p><script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i|w_{i-1})</script></li><li><p>$n=3$,tri-gram</p><script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i|w_{i-2}w_{i-1})</script></li></ul><h2 id="2-基于HMM的分词"><a href="#2-基于HMM的分词" class="headerlink" title="2.基于HMM的分词"></a>2.基于HMM的分词</h2><p><strong>HMM的参数</strong></p><p>观测序列(输出状态序列),状态序列(隐藏状态序列),初始概率,转移概率(转移概率矩阵),发射概率(发射概率矩阵)<br><img src="/images/2019/9_8_ma.png"><br><strong>数学定义</strong></p><p>状态值集合(隐藏状态):$Q={q_1,q_2,\cdots,q_N}$,$N$为可能的状态数,对应状态序列$I​$</p><p>观测值集合(输出状态):$V={v_1,v_2,\cdots,v_M}$,$M$为可能的观测数,对应观测序列$O$</p><p>转移概率矩阵:$A=[a<em>{ij}];i,j\in{1,2,\cdots,N}$,从$i$状态到$j$状态的转换概率($\sum</em>{j=1}^Na_{ij}=1$)</p><p>发射概率矩阵(观测概率矩阵):$B=[b_j(k)];j\in{1,2,\cdots,N},k\in{1,2,\cdots,M}$,从状态$j$生成观测$k$的概率</p><p>初始状态分布:$\pi$</p><p>模型:$\lambda=(A,B,\lambda)$,状态序列$I$,观测序列$O$</p><p><strong>HMM中的三个问题:</strong></p><ul><li>概率计算问题:已知模型,求观测序列$O$出现的概率;前向后向算法<br><img src="/images/2019/9_8_ma.png"></li></ul><ul><li><p>学习问题:已知观测序列,求模型参数,最大化$P(O|\lambda)$;鲍姆-韦尔奇(Baum-Welch)算法</p><ul><li><p>已知隐藏序列和观测序列(通过频数估计)</p><p>$A_{ij}$表示隐藏状态$q_i$转移到$q_j$的频率计数</p><script type="math/tex; mode=display">A=[a_{ij}];a_{ij}=\frac{A_{ij}}{\sum_{s=1}^NA_{is}}</script><p>$B_{jk}$表示隐藏状态$q_j$转移到观测状态$v_k$的频率计数</p><script type="math/tex; mode=display">B=[b_j(k)];b_j(k)=\frac{B_{jk}}{\sum_{s=1}^MB_{js}}</script><p>$C(i)$为所有样本中初始隐藏状态$q_j$的频率计数</p><script type="math/tex; mode=display">\Pi = \pi(i)=\frac{C(i)}{\sum_{s=1}^NC(s)}</script></li><li><p>仅知观测序列(鲍姆-韦尔奇算法,EM算法)</p><p>EM算法:最大似然估计用于没有隐变量的概率模型,EM算法可以用于有隐变量的算法模型</p><p>模型参数:</p><script type="math/tex; mode=display">\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)</script></li></ul></li></ul><ul><li>解码问题:已知模型$\lambda$与观测序列$O$,求状态序列$I$最大化$P(I|O)$;维特比(Viterbi)算法</li></ul><p>HMM是一种序列模型,不仅可以用到自然语言处理这个领域,其他领域的应用也很常见:<a href="http://tecdat.cn/%e7%94%a8%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%86%e5%88%ab%e4%b8%8d%e6%96%ad%e5%8f%98%e5%8c%96%e7%9a%84%e8%82%a1%e5%b8%82%e7%8a%b6%e5%86%b5-%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab/" target="_blank" rel="noopener">股指预测</a>,<a href="https://www.jianshu.com/p/16fc3712fdf6" target="_blank" rel="noopener">语音识别</a>,<a href="http://www.c-a-m.org.cn/CN/abstract/abstract4637.shtml" target="_blank" rel="noopener">网络安全</a>,<a href="https://arxiv.org/abs/1901.06286" target="_blank" rel="noopener">基因序列</a>等方面</p><p>在自然语言处理中,HMM可以应用在分词,词性标注,命名实体识别等各个方面.</p><p>在分词方面可以这样理解HMM</p><p>观测序列(输出状态序列)—-序列构成的句子或短文</p><p>状态序列(隐藏状态序列)—-标注</p><p>初始概率—-统计的第一个字序的概率</p><p>转移概率(转移概率矩阵)—-第$i$个字序到第$i+1$个自序的标注变换概率</p><p>发射概率(发射概率矩阵)—-从隐藏状态到输出状态的转换概率</p><font color="red">如果把观测序列看作标注,状态序列看作句子,从解码问题转变成学习问题会怎样</font><p>在序列预测问题中</p><p>HMM模型:当前tag仅依赖前一个tag,当前输出仅依赖当前tag(文档的单词序列是由隐藏状态的标签决定的)</p><p>MEMM(最大熵马尔科夫模型)模型:当前tag取决于观察值x(单词)和前一个tag(序列的标签取决于前一个标签和当前的单词)</p><p>CRF模型:计算损失时把一句话看作一个整体计算损失</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.cnblogs.com/hiyoung/archive/2018/09/25/9703976.html" target="_blank" rel="noopener">https://www.cnblogs.com/hiyoung/archive/2018/09/25/9703976.html</a></p><p>2.<a href="http://www.52nlp.cn/hmm-learn-best-practices-one-introduction" target="_blank" rel="noopener">http://www.52nlp.cn/hmm-learn-best-practices-one-introduction</a></p><p>3.<a href="https://www.cnblogs.com/en-heng/p/6164145.html?utm_source=debugrun&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/en-heng/p/6164145.html?utm_source=debugrun&amp;utm_medium=referral</a></p><p>4.<a href="http://www.wanguanglu.com/2017/01/03/crf-introduction/" target="_blank" rel="noopener">http://www.wanguanglu.com/2017/01/03/crf-introduction/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-n元语言模型-n-gram&quot;&gt;&lt;a href=&quot;#1-n元语言模型-n-gram&quot; class=&quot;headerlink&quot; title=&quot;1.n元语言模型(n-gram)&quot;&gt;&lt;/a&gt;1.n元语言模型(n-gram)&lt;/h2&gt;&lt;p&gt;假设$S$表示长度为$i$，由$
      
    
    </summary>
    
      <category term="自然语言处理" scheme="https://www.vhcffh.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="分词" scheme="https://www.vhcffh.com/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="统计学" scheme="https://www.vhcffh.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>神经图灵机</title>
    <link href="https://www.vhcffh.com/2019/%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA/"/>
    <id>https://www.vhcffh.com/2019/神经图灵机/</id>
    <published>2019-09-07T02:56:31.000Z</published>
    <updated>2019-09-07T02:57:22.270Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Neural-Turing-Machines-原文"><a href="#Neural-Turing-Machines-原文" class="headerlink" title="Neural Turing Machines[原文]"></a>Neural Turing Machines[<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">原文</a>]</h3><ul><li><p>读记忆(Read Heads)</p><p>把时刻$t$的记忆看作是一个$N \times M$的矩阵$M_t$,读的过成首先生成长度为$N$的定位权重向量$w_t$,表示$N$个记忆位置的权值大小,读出的记忆向量为$r_t$:</p><script type="math/tex; mode=display">r_t=\sum_i^Nw_t(i)M_t(i) \ \ \ 其中 \sum_iw_t(i)=1</script><p>对$N$条记忆进行加权求和</p></li><li><p>写记忆(Write Heads)</p><p>类似LSTM:擦除向量$e_t$,增加向量$a_t$</p><ul><li><p>擦除操作:</p><script type="math/tex; mode=display">M_t'(i)=M_{t-1}(1-w_t(i)e_t(i))</script></li><li><p>增加操作:</p><script type="math/tex; mode=display">M_t(i)=M_t'(i)+w_t(i)a_t(i)</script></li></ul><p>神经图灵机的关键是定位向量$w_t$,其它的是由控制器(LSTM,MLP)输出</p></li><li><p>定位机制(Addressing Mechanism)</p><p>结合了基于内容和基于位置的两种方法</p><ul><li><p>基于内容(Content-based Addressing)</p><script type="math/tex; mode=display">w_t^c(i)=\frac {\exp(\beta_tK[k_t,M_t(i)])}{\sum_j\exp(\beta_tK[k_t,M_t(j)])}</script><p>$K[.,.]$是余弦相似度计算:</p><script type="math/tex; mode=display">K[u,v]=\frac{u \cdot v}{\Vert u\Vert \cdot \Vert v\Vert}</script><p>$\beta_t$是控制器输出</p></li><li><p>基于位置(Location-based Addressing)</p><ul><li><p>插值(Interpolation)</p><script type="math/tex; mode=display">w_t^g=g_tw_t^c+(1-g_t)w_{t-1}</script><p>$g_t$有控制器生成</p></li><li><p>偏移(shift)</p><script type="math/tex; mode=display">\tilde w_t(i)=\sum_{j=0}^{N-1}w_t^g(j)s_t(i-j)</script><p>每一个$\tilde w_t(i)$都与相邻元素有关</p></li><li><p>重塑(Sharping)</p><script type="math/tex; mode=display">w_t(i)=\frac{\tilde w_t(i)^{\gamma_t}}{\sum_j\tilde w_t(j)^{\gamma_t}}</script></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Neural-Turing-Machines-原文&quot;&gt;&lt;a href=&quot;#Neural-Turing-Machines-原文&quot; class=&quot;headerlink&quot; title=&quot;Neural Turing Machines[原文]&quot;&gt;&lt;/a&gt;Neural Tur
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="图灵机" scheme="https://www.vhcffh.com/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"/>
    
      <category term="神经网络" scheme="https://www.vhcffh.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>运用自注意力机制检测相关语境进行多论对话</title>
    <link href="https://www.vhcffh.com/2019/%E8%BF%90%E7%94%A8%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A3%80%E6%B5%8B%E7%9B%B8%E5%85%B3%E8%AF%AD%E5%A2%83%E8%BF%9B%E8%A1%8C%E5%A4%9A%E8%AE%BA%E5%AF%B9%E8%AF%9D/"/>
    <id>https://www.vhcffh.com/2019/运用自注意力机制检测相关语境进行多论对话/</id>
    <published>2019-09-07T02:49:03.000Z</published>
    <updated>2019-09-07T02:55:33.049Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-forMulti-turn-Dialogue-Generation-原文"><a href="#ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-forMulti-turn-Dialogue-Generation-原文" class="headerlink" title="ReCoSa: Detecting the Relevant Contexts with Self-Attention forMulti-turn Dialogue Generation[原文]"></a>ReCoSa: Detecting the Relevant Contexts with Self-Attention forMulti-turn Dialogue Generation[<a href="https://arxiv.org/abs/1907.05339" target="_blank" rel="noopener">原文</a>]</h3><p>模型</p><h4 id="Word-level-Encoder"><a href="#Word-level-Encoder" class="headerlink" title="Word-level Encoder"></a>Word-level Encoder</h4><ul><li>文本集</li></ul><script type="math/tex; mode=display">C = \{s_1,\dots,s_N\}</script><ul><li>句子</li></ul><script type="math/tex; mode=display">s_i=\{x_1,\dots,x_M\}</script><ul><li><p>输入句子$s_i$,编码输出$h_M$</p><script type="math/tex; mode=display">i_k=\sigma(W_i[h_{k-1},w_k]),\ \ f_k=\sigma(W_f[h_{k-1},w_k]), \\o_k=\sigma(W_o[h_{k-1},w_k]),\ \ l_k= tanh (W_l[h_{k-1},w_k]), \\c_k=f_kc_{k-1}+i_kl_k,\ \ h_i=o_ktanh(c_k)</script><ul><li>$i_k$:输入,$f_k$:记忆,$o_k$:输出.</li><li>$w_k$是$x_k$的词嵌入</li><li>$W_i,W_f,W_o,W_l$是模型参数</li><li>用$h_M$代表一个句子,得到句子表示集${h^{s_1},\dots,h^{s_N}}$</li></ul></li><li><p>定义位置嵌入向量$P_i \in \mathbb R^d,i=1,\dots,N$</p><p>得到句子表示${(h^{s_1},P_1),\dots,(h^{s_N},P_N)}$</p></li></ul><h4 id="注意力计算-参见论文attention-is-all-you-need"><a href="#注意力计算-参见论文attention-is-all-you-need" class="headerlink" title="注意力计算(参见论文attention-is-all-you-need)"></a>注意力计算(参见论文<a href="./7181-attention-is-all-you-need.pdf">attention-is-all-you-need</a>)</h4><ul><li><p>定义注意力分数</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac {QK^T}{\sqrt d}V)</script><p>其中$Q\in\mathbb R^{n \times d},K\in\mathbb R^{n\times d},V\in \mathbb R^{n\times d}$</p><p>$d$是隐藏单元</p></li><li><p><font color="red">multi-head attention</font>(多层scaled dot-product attention)</p><script type="math/tex; mode=display">M_i=Attention(QW_i^Q,KW_i^K,VW_i^V) \\M=Concat(M_1,\dots,M_H) \\O=MW</script><p>其中$W_i^Q\in\mathbb R^{n \times d/H},W_i^K\in\mathbb R^{n\times d/H},W_i^V\in \mathbb R^{n\times d/H}$,$M\in\mathbb R^{n\times d},W\in \mathbb R^{d\times d}$</p><p>${(h^{s_1},P_1),\dots,(h^{s_N},P_N)}$-&gt;$O_s$-&gt;<font color="red">feed-forward network</font>-&gt;$O_s^f$</p><p>是否可以将语境检测运用到机器翻译中?</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-forMulti-turn-Dialogue-Generation-原文&quot;&gt;&lt;a href=&quot;#ReCoSa-Detecting-the-Rele
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="多论对话" scheme="https://www.vhcffh.com/tags/%E5%A4%9A%E8%AE%BA%E5%AF%B9%E8%AF%9D/"/>
    
      <category term="Self-Attention" scheme="https://www.vhcffh.com/tags/Self-Attention/"/>
    
  </entry>
  
  <entry>
    <title>小样本学习的边缘标签图神经网络</title>
    <link href="https://www.vhcffh.com/2019/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BE%B9%E7%BC%98%E6%A0%87%E7%AD%BE%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://www.vhcffh.com/2019/小样本学习的边缘标签图神经网络/</id>
    <published>2019-09-06T12:31:53.000Z</published>
    <updated>2019-09-07T02:53:59.941Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文"><a href="#Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文" class="headerlink" title="Edge-Labeling Graph Neural Network for Few-shot Learning[原文]"></a>Edge-Labeling Graph Neural Network for Few-shot Learning[<a href="https://arxiv.org/abs/1905.01436" target="_blank" rel="noopener">原文</a>]</h3><p>小样本学习的边缘标签图神经网络</p><p>将样本映射到图中的节点,边表示节点之间的相似度,通过已知样本预测未知样本</p><h4 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h4><p>主要利用邻域聚合框架(通过对相邻节点特征的递归聚合和转换,计算出节点特征)进行表征学习</p><h4 id="Edge-Labeling-Graph"><a href="#Edge-Labeling-Graph" class="headerlink" title="Edge-Labeling Graph"></a>Edge-Labeling Graph</h4><p><strong>Correlation clustering</strong></p><p>相关性聚类分析:一个通过同时最大化簇内相似性和簇间差异性来实现边标注推理的图分割算法</p><ul><li>通过结构化向量机实现名词短语聚类和新闻文章聚类</li></ul><h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><p>C-way K-shot:一个meta-task,包括support set随机抽取C个类别,每个类别K个样本(共CK个数据);和batch set从这C各类别种抽取一批样本.</p><font color="red">Mode Based, Metric Based,Optimization Based</font><h4 id="Problem-definition-Few-shot-classification"><a href="#Problem-definition-Few-shot-classification" class="headerlink" title="Problem definition: Few-shot classification"></a>Problem definition: Few-shot classification</h4><ul><li>在每个类只有很少样本的情况下学习一个分类器</li><li>每一个few-shot分类任务$\mathcal T$包括support set $\mathcal S$和query set$\mathcal Q$</li><li>episodic training：在training task中抽样，模拟少样本测试时的场景<ul><li>$\mathcal T=\mathcal S\cup\mathcal U, \mathcal S={(x<em>i,y_i)}</em>{i=1}^{N\times{K}} \text{ and }\mathcal{Q}={(x<em>{i},y</em>{i})}_{i=N\times{K}+1}^{N\times{K}+T}$</li><li>$x<em>i,y_i\in{C_1,…,C_N}=\mathcal C</em>{\mathcal T}\subset\mathcal C$同时$\mathcal C<em>{train}\cap\mathcal C</em>{test} = \empty$</li></ul></li></ul><h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><ol><li><p>通过卷积神经网络提取样本特征,$\theta_{emb}$是卷积网络的参数</p><script type="math/tex; mode=display">\mathbf v_i^0=f_{emb}(x_i;\theta_{emb})</script></li><li><p>图构建:$\mathcal g=(\mathcal V,\mathcal E;\mathcal T)$每一个节点代表一个样本，全连接图，每一条边代表一种关系种类</p><p>其中$\mathcal V :={V<em>i}</em>{i=1,\dots,|\mathcal T|}$,$\mathcal E :={E<em>{ij}}</em>{i,j=1,\dots,|\mathcal T|}$,</p><ol><li>ground truth: edge-label</li></ol><script type="math/tex; mode=display">y_{ij}=\begin{cases}1,\text{if }y_i=y_j,\\0,\text{otherwise.}\end{cases}</script><ol><li>边特征$\mathbf e<em>{ij}={e</em>{ijd}}^2_{d=1} \in [0,1]^2$是一个二维矩阵,表示两个之节点间的簇内关系和簇间关系的强弱,||表示连接</li></ol><script type="math/tex; mode=display">\mathbf e_{ij}^0=\begin{cases}\ \ \ [1||0],\ \ \ \ if \ y_{ij}=1\ \ and\ \ i,j\le N \times K, \\\ \ \ [0||1],\ \ \ \ if \ y_{ij}=0\ \ and\ \ i,j\le N \times K, \\[0.5||0.5],\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ otherwise,\end{cases}</script></li></ol><ol><li><p>传播:设从$\ell -1 $层得到的特征为$\mathbf v<em>i^{\ell-1}$ 和$\mathbf e</em>{ij}^{\ell-1}$</p><ol><li><p>更新节点特征</p><script type="math/tex; mode=display">\mathbf v_i^\ell=f_v^\ell([\sum_j\tilde e_{ij1}^{\ell-1}\mathbf v_j^{\ell-1}||\sum_j\tilde e_{ij2}^{\ell-1}\mathbf v_j^{\ell-1}];\theta_v^\ell)</script><p>其中$\tilde e<em>{ij1}^{\ell-1}=\frac {e</em>{ijd}}{\sum<em>ke</em>{ikd}}$,$f_v^\ell(\theta)$是节点特征的转变网络(MLP).</p></li><li><p>更新边特征</p><script type="math/tex; mode=display">\begin{align}\bar e_{ij1}^\ell \ &=\  \frac {f_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell)e_{ij1}^{\ell-1}}{\sum_kf_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell)e_{ij1}^{\ell-1}/(\sum_ke_{ik1}^{\ell-1})},\\\bar e_{ij2}^\ell \ &=\  \frac {(1-f_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell))e_{ij2}^{\ell-1}}{\sum_k(1-f_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell))e_{ij2}^{\ell-1}/(\sum_ke_{ik2}^{\ell-1})},\\\mathbf e_{ij}^\ell \ &=\ \bar {\mathbf e}_{ij}^\ell/\Vert \bar {\mathbf e}_{ij}^\ell \Vert_1\end{align}</script><p>其中 $f_e^\ell$是<font color="red">metric network</font>计算节点相似度</p></li></ol></li><li><p>输出结果:节点属于某个集合的分布$P(y_i=\mathcal C_k|\mathcal T)=p_i^{(k)}$</p><script type="math/tex; mode=display">p_i^{(k)}=softmax(\sum_{\{j:j\neq i\land(\mathbf x_j,y_j)\in \mathcal S\}}\hat y_{ij}\delta(y_j=\mathcal C_k))</script><script type="math/tex; mode=display">\hat y_{ij}=e_{ij}^L \in [0,1]</script></li></ol><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>其中模型参数有$\theta<em>{emb} \cup{\theta_v^\ell,\theta_e^\ell}</em>{\ell=1}^L$,对于M个训练任务${\mathcal T<em>m^{train}}</em>{m=1}^M$,损失函数:</p><script type="math/tex; mode=display">\mathcal L=\sum_{\ell=1}^L\sum_{m=1}^M\lambda_\ell\mathcal L(Y_{m,e},\hat Y_{m,e}^\ell)</script><p>其中$Y<em>{m,e}$和$\hat Y</em>{m,e}^\ell$是m个任务,e条边的真实值和预测值(第$\mathcal L$层的),$\mathcal L$是交叉熵损失函数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文&quot;&gt;&lt;a href=&quot;#Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文&quot; 
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://www.vhcffh.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="图网络" scheme="https://www.vhcffh.com/tags/%E5%9B%BE%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>记一次GRUB引导修复</title>
    <link href="https://www.vhcffh.com/2019/%E8%AE%B0%E4%B8%80%E6%AC%A1GRUB%E5%BC%95%E5%AF%BC%E4%BF%AE%E5%A4%8D/"/>
    <id>https://www.vhcffh.com/2019/记一次GRUB引导修复/</id>
    <published>2019-09-03T11:13:00.000Z</published>
    <updated>2020-01-25T13:35:34.632Z</updated>
    
    <content type="html"><![CDATA[<p>使用Arch Linux已经2年多了，基本上已经习惯了，可前段时间英雄联盟也出了“自走棋”，想要体验一把。<br>于是就在硬盘剩余的60多个G上装了Windows 10，用了一两个月没有问题，最后再一次Windows 10更新后，蓝屏了。又手惨的点了Windows 的修复功能。等了半天，结果是修复，重启，蓝屏无限循环。而且Arch Linux的引导全都没了，感激应该是Windows的修复动了efi分区。<br>无奈只能新修复引导，各种方法尝试了好多边都是卡在了<code>GRUB _</code>一直闪这个状态，最终在官网wiki找到了办法<br><a href="https://wiki.archlinux.org/index.php/GRUB_#Default/fallback_boot_path" target="_blank" rel="noopener">缺省/后备启动路径</a><br>grub 安装时添加<code>--removable</code>参数</p><pre><code class="lang-sh">grub-install --target=x86_64-efi --efi-directory=esp --removable</code></pre><p>或者手动移动</p><pre><code class="lang-sh">mv esp/EFI/grub esp/EFI/BOOTmv esp/EFI/BOOT/grubx64.efi esp/EFI/BOOT/BOOTX64.EFI</code></pre><p>grub出现bug的原因应该会千奇百怪，不要尽可能多的尝试网上的各种修改方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用Arch Linux已经2年多了，基本上已经习惯了，可前段时间英雄联盟也出了“自走棋”，想要体验一把。&lt;br&gt;于是就在硬盘剩余的60多个G上装了Windows 10，用了一两个月没有问题，最后再一次Windows 10更新后，蓝屏了。又手惨的点了Windows 的修复功
      
    
    </summary>
    
      <category term="Linux" scheme="https://www.vhcffh.com/categories/Linux/"/>
    
    
      <category term="grub" scheme="https://www.vhcffh.com/tags/grub/"/>
    
  </entry>
  
  <entry>
    <title>深度学习在中文分词和词性标注中的应用</title>
    <link href="https://www.vhcffh.com/2019/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%92%8C%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>https://www.vhcffh.com/2019/深度学习在中文分词和词性标注中的应用/</id>
    <published>2019-08-25T02:37:36.000Z</published>
    <updated>2019-10-04T06:54:51.331Z</updated>
    
    <content type="html"><![CDATA[<p>得到字向量-&gt;通过窗口方法得到字与上下文有关的向量(矩阵)-&gt;通过两个线性层和一个非线性激活函数-&gt;字的标注得分(窗口方法)-&gt;一个句子的评分矩阵$f<em>\theta (c</em>{[1:n]})​$ (句子中的第$i​$ 个子为标签$t​$ 的得分)-&gt;定义转换分数$A_{ij}​$,得到tag path 得分</p><script type="math/tex; mode=display">s(c_{[1:N]},t_{[1:N]},\theta)=\sum_{i=1}^n(A_{t_{i-1}t_i}+f_\theta (t_i\vert i))</script><p>-&gt;最大化得分,得到最优tag path</p><p>log likelihood</p><script type="math/tex; mode=display">\sum_{\forall(c,t)\in R}\log p(t\vert c,\theta)</script><p>将目标函数转换为条件概率</p><script type="math/tex; mode=display">p(t\vert c,\theta)=\frac {e^{s(c,t,\theta)}}{\sum_{\tilde t}{e^{s(c,\tilde t,\theta)}}}</script><p>取对数</p><script type="math/tex; mode=display">\log p(t\vert c,\theta) = s(c,t,\theta)-log\sum_{\tilde t}{e^{s(c,\tilde t,\theta)}}</script><p>维特比算法(viterbi)</p><p>一种动态规划算法(穷举法,A*算法,beam search,Viterbi算法)</p><font color="red" face="微软雅黑">A\*算法和Viterbi算法的区别?</font><p>新的训练方法</p><p>将维特比算法在当前参数下得出的最优路径结果与正确结果进行比较对比,定义出损失函数对$A<em>{t</em>{i-1}t<em>i}$ 和$f</em>\theta (t_i|i)$ 的偏导数,通过后向传播更新参数</p><p>收敛性的证明:Discriminative training methods for hidden Markov models</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;得到字向量-&amp;gt;通过窗口方法得到字与上下文有关的向量(矩阵)-&amp;gt;通过两个线性层和一个非线性激活函数-&amp;gt;字的标注得分(窗口方法)-&amp;gt;一个句子的评分矩阵$f&lt;em&gt;\theta (c&lt;/em&gt;{[1:n]})​$ (句子中的第$i​$ 个子为标签$t​$ 
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="分词" scheme="https://www.vhcffh.com/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="深度学习" scheme="https://www.vhcffh.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词性标注" scheme="https://www.vhcffh.com/tags/%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>OWL基础</title>
    <link href="https://www.vhcffh.com/2019/OWL%E5%9F%BA%E7%A1%80/"/>
    <id>https://www.vhcffh.com/2019/OWL基础/</id>
    <published>2019-08-22T03:53:16.000Z</published>
    <updated>2019-08-22T04:02:33.451Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OWL基础"><a href="#OWL基础" class="headerlink" title="OWL基础"></a>OWL基础</h2><p>网络本体语言Web Ontologoy Language</p><p>OWL Lite -&gt;OWL DL-&gt;OWL Full 递进关系</p><h3 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h3><ol><li><p>类(Class)</p><p>任何东西都是类owl:Thing 的一个成员(实例);子类:Subclass;相等关系:equivalentClass</p></li><li><p>个体(Individual)</p><p>与实例的概念差不多</p><p>一个个体可以属于多个类(没有指名是哪个类)</p><p>某个类的实例(指名了类)</p><p>子类与个体是不同的东西</p></li><li><p>属性(Property)</p><p>一个二元关系,OWL中包括两种属性:</p><ol><li>类型属性（datatype properties）：描述类与其实例之间关系的属性。</li><li>对象属性（object properties）：描述两个不同类的实例之间关系的属性。</li></ol><p>属性有两个端点:起点和终点,都应该是两个个体(实例)</p><p>用原集(domain)起点的实例的类,用象集(range)描述终点的实例的类.</p><p>属性也有子属性(Subproperty)</p></li></ol><p>公理和约束</p><h3 id="rdf资源描述框架-Resource-Description-Framework"><a href="#rdf资源描述框架-Resource-Description-Framework" class="headerlink" title="rdf资源描述框架(Resource Description Framework)"></a>rdf资源描述框架(Resource Description Framework)</h3><p>W3C提出的一组标记语言的技术规范,是一种数据模型,rdf数据集的序列化方法</p><p>OWL是由DAML(DARPA Agent Markup Language)+OIL(Ontology Inference Layer)演变而来。</p><p>OWL是RDF的扩张，为我们提供了更广泛的定义RDFS词汇的功能，更广泛意指可以定义词汇之间的关系，类与类间的关系，属性与属性之间的关系等</p><p>foaf(Friend-of-a-Friend)是一种XML/RDF词汇表,不管通过那种那种方法表示数据资源,都要指定词汇表</p><ol><li><p>RDF/XML</p><p>XML的技术程序;格式太冗长,不便于阅读</p></li><li><p><a href="https://www.w3.org/TR/n-triples/" target="_blank" rel="noopener">N-Triples</a></p><p>三元组表示;开放领域知识图谱<a href="https://wiki.dbpedia.org/" target="_blank" rel="noopener">DBpedia</a>通常使用这种格式发布数据.</p></li><li><p><a href="https://www.w3.org/TR/turtle/" target="_blank" rel="noopener">Turtle</a></p><p>使用最多的一种RDF序列化方法,比RDF/XML紧凑,可读性比N-Triples好</p></li><li><p>RDFa</p><p>The Resource Description Framework in Attributes,HTML5的一个扩展,不改变任何显示效果的情况下,然网站更容易被搜索引擎解析</p></li><li><p>JSON-LD</p><p>JSON for Linking Data,用键值对的方法来存储RDF</p></li></ol><h3 id="RDF-XML"><a href="#RDF-XML" class="headerlink" title="RDF/XML"></a>RDF/XML</h3><pre><code class="lang-xml">&lt;?xml version=&quot;1.0&quot;?&gt;&lt;rdf:RDFxmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;xmlns:cd=&quot;http://www.recshop.fake/cd#&quot;&gt;&lt;rdf:Descriptionrdf:about=&quot;http://www.recshop.fake/cd/Empire Burlesque&quot;&gt;  &lt;cd:artist&gt;Bob Dylan&lt;/cd:artist&gt;  &lt;cd:country&gt;USA&lt;/cd:country&gt;  &lt;cd:company&gt;Columbia&lt;/cd:company&gt;  &lt;cd:price&gt;10.90&lt;/cd:price&gt;  &lt;cd:year&gt;1985&lt;/cd:year&gt;&lt;/rdf:Description&gt;&lt;rdf:Descriptionrdf:about=&quot;http://www.recshop.fake/cd/Hide your heart&quot;&gt;  &lt;cd:artist&gt;Bonnie Tyler&lt;/cd:artist&gt;  &lt;cd:country&gt;UK&lt;/cd:country&gt;  &lt;cd:company&gt;CBS Records&lt;/cd:company&gt;  &lt;cd:price&gt;9.90&lt;/cd:price&gt;  &lt;cd:year&gt;1988&lt;/cd:year&gt;&lt;/rdf:Description&gt;...&lt;/rdf:RDF&gt;</code></pre><p> RDF 文档的第一行是 XML 声明。这个 XML 声明之后是 RDF 文档的根元素：<em><rdf:rdf></rdf:rdf></em>。</p><p><em>xmlns:rdf</em> 命名空间，规定了带有前缀 rdf 的元素来自命名空间 “<a href="http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;。" target="_blank" rel="noopener">http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;。</a></p><p><em>xmlns:cd</em> 命名空间，规定了带有前缀 cd 的元素来自命名空间 “<a href="http://www.recshop.fake/cd#&quot;。" target="_blank" rel="noopener">http://www.recshop.fake/cd#&quot;。</a></p><p><em><rdf:description></rdf:description></em> 元素包含了对被 <em>rdf:about</em> 属性标识的资源的描述。</p><p>元素：<em><cd:artist></cd:artist></em>、<em><cd:country></cd:country></em>、<em><cd:company></cd:company></em> 等是此资源的属性。</p><h3 id="Turtle序列化方法总结"><a href="#Turtle序列化方法总结" class="headerlink" title="Turtle序列化方法总结"></a>Turtle序列化方法总结</h3><ul><li><p>URI用 <strong>&lt;&gt;</strong> 描述</p><pre><code>&lt;http://example.org/path/&gt;&lt;http://example.org/path/#fragment&gt;</code></pre></li><li><p>前缀缩写(类似于RDF/XML的命名空间)</p><pre><code>@prefix foo:&lt;http://example.org/ns#&gt;@prefix  : &lt;http://example.org/ns1#&gt;:a :b :c</code></pre></li><li><p>字面量</p><p>一行或者多行， <code>@en</code>限定其语言， <code>^^xsd:decimal</code>限定其数据类型</p><pre><code>&quot;string&quot;&quot;&quot;&quot;many lines of stringmany lines of stringmany lines of string&quot;&quot;&quot;&quot;chat&quot;@en&quot;chat&quot;@fr&quot;10&quot;^^xsd:decimal</code></pre></li><li><p>空节点(RDF模型可能会存在未命名的空节点)</p><p><code>_:me</code>,<code>_a1234</code>分别代表一个空节点</p><pre><code>_:me_:a1234</code></pre></li><li><p>base URI</p><p>base URI定义后，接下来的<strong>URI, 前缀缩写，qualified names 和base URI</strong>都要受其作用</p><pre><code># this is a complete turtle document@base &lt;http://example.org/ns/&gt; .# base URIs 是 http://example.org/ns/@base &lt;foo/&gt; .# base URI 是 http://example.org/ns/foo/@prefix : &lt;bar#&gt; .:a4 :b4 :c4.</code></pre></li><li><p>对三元组进行缩写</p><pre><code>:a :b :c,      :d.#the last triple is :a :b :d.</code></pre></li><li><p>一个简单的完整turtle标准文件</p><p><img src="/images/2019/8_22_example.png" alt="img"></p></li></ul><pre><code>@prefix info: &lt;http://zy.example.com/info#&gt;@prefix rel: &lt;http://zy.example.com/rel#&gt;@prefix person: &lt;http://zy.example.com/person#&gt;person:Tom info:name &quot;Tom&quot;;          info:job &quot;worker&quot;;          info:age 56;          rel:fatherof person:Jim.person:Jim info:name &quot;Jim&quot;;          info:job &quot;programmer&quot;;          info:age 28;          rel:fatherof person:Cherry.person:Cherry info:name &quot;Cherry&quot;;             info:age 8;             .</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;OWL基础&quot;&gt;&lt;a href=&quot;#OWL基础&quot; class=&quot;headerlink&quot; title=&quot;OWL基础&quot;&gt;&lt;/a&gt;OWL基础&lt;/h2&gt;&lt;p&gt;网络本体语言Web Ontologoy Language&lt;/p&gt;
&lt;p&gt;OWL Lite -&amp;gt;OWL DL-&amp;
      
    
    </summary>
    
      <category term="自然语言处理" scheme="https://www.vhcffh.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="OWL" scheme="https://www.vhcffh.com/tags/OWL/"/>
    
      <category term="rdf" scheme="https://www.vhcffh.com/tags/rdf/"/>
    
      <category term="Turtle" scheme="https://www.vhcffh.com/tags/Turtle/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中的广义线性模型</title>
    <link href="https://www.vhcffh.com/2019/sklearn%E4%B8%AD%E7%9A%84%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>https://www.vhcffh.com/2019/sklearn中的广义线性模型/</id>
    <published>2019-08-20T10:13:01.000Z</published>
    <updated>2019-08-20T10:17:11.854Z</updated>
    
    <content type="html"><![CDATA[<p>模型的通用公式</p><script type="math/tex; mode=display">\hat y(w,x)=w_0+w_1x_1+\dots+w_px_p</script><p>其中$w=(w<em>1,\dots,w_p)$ 作为coef\</em>;$w<em>0​$作为intercepr\</em></p><h3 id="普通最小二乘法"><a href="#普通最小二乘法" class="headerlink" title="普通最小二乘法"></a>普通最小二乘法</h3><script type="math/tex; mode=display">w=\min_w{\Vert Xw-y\Vert_2}^2</script><p>LinearRegression</p><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><script type="math/tex; mode=display">w=\min_w{\Vert Xw-y\Vert_2}^2+\alpha {\Vert w \Vert_2}^2</script><p>$\alpha$ 是控制系数收缩量的复杂性参数:$\alpha$ 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。</p><p>共线性:线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真</p><p>Ridge,RigdeCV:广义交叉验证(GCV),默认留一验证(LOO-CV)</p><h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><script type="math/tex; mode=display">w = \min_w\frac{1}{2n_{samples}}\Vert Xw-y \Vert_2^2 + \alpha \Vert w \Vert_1</script><p>$\alpha$ 是常数,$\Vert w \Vert<em>1$ 是参数向量的$l</em>{1-norm}$范数</p><p>Lasso,lasso_path:通过搜索所有可能的路径上的值来计算系数</p><p>LassoCV,LassoLarsCV,LassoLarsIC</p><h2 id="多任务Lasso"><a href="#多任务Lasso" class="headerlink" title="多任务Lasso"></a>多任务Lasso</h2><script type="math/tex; mode=display">w = \min_w\frac{1}{2n_{samples}}\Vert XW-Y\Vert_{Fro}^2+\alpha\Vert W \Vert_{21}</script><script type="math/tex; mode=display">\Vert A \Vert_{Fro}=\sqrt{\sum_{ij}a_{ij}^2}</script><script type="math/tex; mode=display">\Vert A \Vert_{21}= \sum_i\sqrt{\sum_j a_{ij}^2}</script><p>MultiTaskLasso</p><h3 id="弹性网络"><a href="#弹性网络" class="headerlink" title="弹性网络"></a>弹性网络</h3><script type="math/tex; mode=display">w = \min_w\frac{1}{2n_{samples}}\Vert Xw-Y\Vert_2^2+\alpha\rho\Vert w \Vert_{1}+\frac{\alpha(1-\rho)}{2}\Vert w\Vert_2^2</script><p>ElasticNetCV通过交叉验证来设置参数<code>alpha</code>($\alpha$)和<code>l1_rati0</code>($\rho$)</p><h3 id="多任务弹性网络"><a href="#多任务弹性网络" class="headerlink" title="多任务弹性网络"></a>多任务弹性网络</h3><script type="math/tex; mode=display">W = \min_W\frac{1}{2n_{samples}}\Vert XW-Y\Vert_{Fro}^2+\alpha\rho\Vert W \Vert_{21}+\frac{\alpha(1-\rho)}{2}\Vert w\Vert_{Fro}^2</script><p>MultiTaskElasticNet</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;模型的通用公式&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat y(w,x)=w_0+w_1x_1+\dots+w_px_p&lt;/script&gt;&lt;p&gt;其中$w=(w&lt;em&gt;1,\dots,w_p)$ 作为coef\&lt;/em&gt;;$
      
    
    </summary>
    
      <category term="机器学习" scheme="https://www.vhcffh.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="sklearn" scheme="https://www.vhcffh.com/tags/sklearn/"/>
    
      <category term="领回归" scheme="https://www.vhcffh.com/tags/%E9%A2%86%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>集成学习算法总结</title>
    <link href="https://www.vhcffh.com/2019/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://www.vhcffh.com/2019/集成学习算法总结/</id>
    <published>2019-08-19T10:39:18.000Z</published>
    <updated>2019-10-02T11:29:24.860Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成学习算法"><a href="#集成学习算法" class="headerlink" title="集成学习算法"></a>集成学习算法</h2><p>使用多个分类器提高整体的泛化能力</p><h3 id="1-Bagging（Bootstrap-Aggregating）算法"><a href="#1-Bagging（Bootstrap-Aggregating）算法" class="headerlink" title="1.Bagging（Bootstrap Aggregating）算法"></a>1.Bagging（Bootstrap Aggregating）算法</h3><p>通过组合随机生成的训练集而改进分类的集成算法(bootstrap)</p><p>使用训练集中的某个子集作为当前训练集（有放回随机抽样）;经过T次训练后,得到T个不同的分类器</p><p>调用这T个分类器,把这T个分类结果中出现次数多的类赋予测试样例</p><p>有效减少噪声影响</p><h3 id="2-Boosting算法"><a href="#2-Boosting算法" class="headerlink" title="2.Boosting算法"></a>2.Boosting算法</h3><p>初始化样本权重,生成一个弱分类器;</p><p>利用弱分类器增加分类错误的样本的权重;</p><p>不断重复,生成T个弱分类器;</p><p>对噪声敏感</p><p>改进算法-AdaBoosting算法</p><ul><li>对每一次的训练数据样本赋予一个权重，并且每一次样本的权重分布依赖上一次的分类结果</li><li>基分类器之间采用序列的线性加权方式来组合</li></ul><h3 id="3-Gradient-Boosting"><a href="#3-Gradient-Boosting" class="headerlink" title="3.Gradient Boosting"></a>3.Gradient Boosting</h3><p>1,初始化</p><script type="math/tex; mode=display">f_0(x)=\arg\min_\gamma\sum_{i=1}^NL(y_i,\gamma)</script><p>2.1计算负梯度</p><script type="math/tex; mode=display">\widetilde y_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, i=1,2,\cdots N</script><p>2.2用基学习器$h_m(x)$ 拟合$\widetilde y_i$</p><script type="math/tex; mode=display">w_m=\mathop{\arg\min}\limits_w\sum_{i=1}^N[\widetilde y_i-h_m(x_i;w)]^2</script><p>2.3确定步长$\rho_m$ </p><script type="math/tex; mode=display">\rho_m = \mathop{\arg\min}\limits_{\rho} \sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \rho h_m(x_i\,;\,w_m))</script><p>2.4更新$f_m(x)$ 最终得到$f_M(x)$ </p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(x) + \rho_m h_m(x\,;\,w_m)</script><p>Bagging + 决策树 = 随机森林</p><p>AdaBoost + 决策树 = 提升树</p><p>Gradient Boosting + 决策树 = GBDT</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;集成学习算法&quot;&gt;&lt;a href=&quot;#集成学习算法&quot; class=&quot;headerlink&quot; title=&quot;集成学习算法&quot;&gt;&lt;/a&gt;集成学习算法&lt;/h2&gt;&lt;p&gt;使用多个分类器提高整体的泛化能力&lt;/p&gt;
&lt;h3 id=&quot;1-Bagging（Bootstrap-Aggre
      
    
    </summary>
    
      <category term="神经网络" scheme="https://www.vhcffh.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Bagging" scheme="https://www.vhcffh.com/tags/Bagging/"/>
    
      <category term="Boosting" scheme="https://www.vhcffh.com/tags/Boosting/"/>
    
      <category term="Gradient Boosting" scheme="https://www.vhcffh.com/tags/Gradient-Boosting/"/>
    
  </entry>
  
  <entry>
    <title>信息论的一些基本概念</title>
    <link href="https://www.vhcffh.com/2019/%E4%BF%A1%E6%81%AF%E8%AE%BA%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.vhcffh.com/2019/信息论的一些基本概念/</id>
    <published>2019-08-18T05:56:00.000Z</published>
    <updated>2020-01-25T10:03:15.402Z</updated>
    
    <content type="html"><![CDATA[<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><script type="math/tex; mode=display">H(X)=E[I(X)]=E[-ln(P(X))]</script><p>其中$P$ 为$X$的概率质量函数,$E$为期望函数,而$I(x)$是$X$ 的信息量(又称自信息).</p><script type="math/tex; mode=display">H(X)=\sum_iP(x_i)I(x_i)=-\sum_iP(x_i)\log_bP(x_i)</script><script type="math/tex; mode=display">\begin{matrix}b & 熵的单位\\2 & bit\\e & nat\\10 & Hart\end{matrix}</script><h3 id="条件熵-Conditional-Entropy"><a href="#条件熵-Conditional-Entropy" class="headerlink" title="条件熵(Conditional Entropy)"></a>条件熵(Conditional Entropy)</h3><p>特征$x$ 固定为$x_i$时:$H(c|x_i)$ </p><p>特征$x$ 整体分布已知时:$H(x|X)$ </p><h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益(Information Gain)"></a>信息增益(Information Gain)</h3><script type="math/tex; mode=display">IG(X) = H(c)-H(c|X)</script><h3 id="基尼系数-基尼不纯度Gini-impurity"><a href="#基尼系数-基尼不纯度Gini-impurity" class="headerlink" title="基尼系数(基尼不纯度Gini impurity)"></a>基尼系数(基尼不纯度Gini impurity)</h3><script type="math/tex; mode=display">Gini(D)=1-\sum_i^np_i^2</script><script type="math/tex; mode=display">Gini(D|A)=\sum_i^n\frac {D_i}{D}</script><h3 id="信息增益比率-Information-Gain-Ratio-与分裂信息-Split-information"><a href="#信息增益比率-Information-Gain-Ratio-与分裂信息-Split-information" class="headerlink" title="信息增益比率(Information Gain Ratio)与分裂信息(Split information)"></a>信息增益比率(Information Gain Ratio)与分裂信息(Split information)</h3><script type="math/tex; mode=display">GR(D|A)=\frac {IG(D|A)}{SI(D|A)}</script><script type="math/tex; mode=display">SI(D|A)=-\sum_i^n\frac {N_i}{N}\log_2\frac{N_i}{N}</script><h3 id="边界熵-boundary-entropy"><a href="#边界熵-boundary-entropy" class="headerlink" title="边界熵(boundary entropy)"></a>边界熵(boundary entropy)</h3><script type="math/tex; mode=display">BE(w_1w_2\cdots w_k) = -\sum_{w \in C}p(w\vert w_1w_2\cdots w_k)\log p(w\vert w_1w_2\cdots w_k)</script><p>$w$是邻接于$w_1w_2\cdots w_k$ 的字符.</p><h3 id="边界多样性-Accessor-veriety-AV"><a href="#边界多样性-Accessor-veriety-AV" class="headerlink" title="边界多样性(Accessor veriety,AV)"></a>边界多样性(Accessor veriety,AV)</h3><script type="math/tex; mode=display">AV(w_1w_2\cdots w_k)=\log RL_{av}(w_1w_2\cdots w_k)</script><p>$RL_{av}$ 表示邻接于字符串$w_1w_2\cdots w_k$的不同字符个数.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;信息熵&quot;&gt;&lt;a href=&quot;#信息熵&quot; class=&quot;headerlink&quot; title=&quot;信息熵&quot;&gt;&lt;/a&gt;信息熵&lt;/h3&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
H(X)=E[I(X)]=E[-ln(P(X))]&lt;/scr
      
    
    </summary>
    
      <category term="基础知识" scheme="https://www.vhcffh.com/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="信息论" scheme="https://www.vhcffh.com/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
      <category term="熵" scheme="https://www.vhcffh.com/tags/%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>mathjax配置问题</title>
    <link href="https://www.vhcffh.com/2019/mathjax%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"/>
    <id>https://www.vhcffh.com/2019/mathjax配置问题/</id>
    <published>2019-08-10T15:07:00.000Z</published>
    <updated>2020-01-25T13:38:40.269Z</updated>
    
    <content type="html"><![CDATA[<p>使用hexo时，想要实现网页中公式的渲染<br>发现不管怎么改，都不能渲染单行公式<br>最后发现是在mathjax的2.3版本以后，配置方法变了</p><h3 id="mathjax的配置方法"><a href="#mathjax的配置方法" class="headerlink" title="mathjax的配置方法"></a>mathjax的配置方法</h3><p>一般网上大部分的mathjax的配置如下</p><pre><code class="lang-html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;  MathJax.Hub.Config({    tex2jax: {      inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],      processEscapes: true    }  });&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;path-to-MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;</code></pre><p>其中配置中这一句主要是增加对单行公式的渲染</p><pre><code class="lang-javascript">inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],</code></pre><p>有时候会发现无论如修改单行公式总是不能渲染<br>原因是在mathjax的2.3版本以后，应该这样配置</p><pre><code class="lang-html">&lt;script type=&quot;text/javascript&quot;&gt;  window.MathJax = {    tex2jax: {      inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],      processEscapes: true    }  };&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;path-to-MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;</code></pre><p>对于<code>hexo</code>默认转义规则使单行公式显示错误的问题,查看<a href="https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/" target="_blank" rel="noopener">这篇博客</a></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1,<a href="https://docs.mathjax.org/en/latest/configuration.html" target="_blank" rel="noopener">https://docs.mathjax.org/en/latest/configuration.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用hexo时，想要实现网页中公式的渲染&lt;br&gt;发现不管怎么改，都不能渲染单行公式&lt;br&gt;最后发现是在mathjax的2.3版本以后，配置方法变了&lt;/p&gt;
&lt;h3 id=&quot;mathjax的配置方法&quot;&gt;&lt;a href=&quot;#mathjax的配置方法&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Linux" scheme="https://www.vhcffh.com/categories/Linux/"/>
    
    
      <category term="mathjax" scheme="https://www.vhcffh.com/tags/mathjax/"/>
    
  </entry>
  
  <entry>
    <title>pytorch使用和损失函数</title>
    <link href="https://www.vhcffh.com/2019/pytorch%E4%BD%BF%E7%94%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://www.vhcffh.com/2019/pytorch使用和损失函数/</id>
    <published>2019-07-26T09:17:00.000Z</published>
    <updated>2020-01-25T10:07:19.498Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h2><pre><code class="lang-python">#激活函数x=np.arange(-12.5,12.5,0.05)tanh = (np.power(np.e,x)-np.power(np.e,-x))/(np.power(np.e,x)+np.power(np.e,-x))relu = np.maximum(0.0,x)sigmoid = 1.0/(1.0+np.power(np.e,-x))</code></pre><pre><code class="lang-python">torch.nn.Sigmoid()</code></pre><script type="math/tex; mode=display">Sigmoid(x)=\frac {1}{1+e^{-x}}</script><script type="math/tex; mode=display">\frac {1}{1+e^{-x}}-\frac{1}{2}=\frac{1-e^{-x}}{2(1+e^{-x})}=-\frac{1-e^x}{2(1+e^x)}</script><pre><code class="lang-python">torch.nn.Tanh</code></pre><script type="math/tex; mode=display">Tanh(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><script type="math/tex; mode=display">\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{1-e^{-2x}}{1+e^{-2x}}=\frac{2}{1+e^{-2x}}-1=2Sigmoid(2x)-1</script><p>BatchNorm2d</p><p>对每一个特征进行正则</p><script type="math/tex; mode=display">y=\frac {x-E[x]} {\sqrt{Var[x]+\varepsilon}}*\gamma+\beta \ \ \ \ \ (\varepsilon=10^{-5})</script><p>pytorch中的正则化函数</p><pre><code class="lang-python">torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)</code></pre><script type="math/tex; mode=display">v=\frac {v}{max(\lVert v \rVert_p,\varepsilon)}</script><pre><code class="lang-python">torch.norm(input, p=&#39;fro&#39;, dim=None, keepdim=False, out=None, dtype=None)# 对维度dim求p范数</code></pre><pre><code class="lang-python">torch.Tensor.squeeze()-&gt;Tensor#维度压缩torch.cat(tensors, dim=0, out=None)-&gt;Tensor #维度拼接torch.stack(tensors,dim=0,out=None)-&gt;Tensor #张量拼接# cat是把多张纸拼成一张纸,stack是把纸摞起来torch.Tensor.repeat()-&gt;Tensor #矩阵扩展torch.Tensor.transpose()-&gt;Tensor #矩阵转置torch.eq() #张量比较torch.chunk() #张量分块torch.split(tensor, split_size_or_sections, dim=0) #张量分块</code></pre><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss"></a>L1Loss</h3><pre><code class="lang-python">torch.nn.L1Loss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = \left| x_n - y_n \right|,</script><script type="math/tex; mode=display">        \ell(x, y) =        \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss"></a>MSELoss</h3><pre><code class="lang-python">torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = \left( x_n - y_n \right)^2,</script><script type="math/tex; mode=display">\ell(x, y) =        \begin{cases}            \operatorname{mean}(L), &  \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  &  \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p>交叉熵损失函数,多分类</p><pre><code class="lang-python">torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)</script><script type="math/tex; mode=display">        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</script><pre><code class="lang-python">torch.nn.CTCLoss(blank=0, reduction=&#39;mean&#39;, zero_infinity=False)</code></pre><h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><p>负对数似然损失函数(Negative Loss Likelihood),多分类</p><pre><code class="lang-python">torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = - w_{y_n} x_{n,y_n}, \quad        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\},</script><script type="math/tex; mode=display">        \ell(x, y) = \begin{cases}            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &            \text{if reduction} = \text{'mean';}\\            \sum_{n=1}^N l_n,  &            \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="PoissonNLLLoss"><a href="#PoissonNLLLoss" class="headerlink" title="PoissonNLLLoss"></a>PoissonNLLLoss</h3><pre><code class="lang-python">torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{target} \sim \mathrm{Poisson}(\text{input})        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})                                    + \log(\text{target!})</script><h3 id="KLDivLoss"><a href="#KLDivLoss" class="headerlink" title="KLDivLoss"></a>KLDivLoss</h3><p>KL散度,又叫相对熵,计算两个分布之间的距离,越相近越接近零</p><pre><code class="lang-python">torch.nn.KLDivLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        l(x,y) = L = \{ l_1,\dots,l_N \}, \quad        l_n = y_n \cdot \left( \log y_n - x_n \right)</script><script type="math/tex; mode=display">\ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';} \\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h3><p>二分类用的交叉熵</p><pre><code class="lang-python">torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</script><script type="math/tex; mode=display">\ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="BCEWithLogitsLoss"><a href="#BCEWithLogitsLoss" class="headerlink" title="BCEWithLogitsLoss"></a>BCEWithLogitsLoss</h3><p>增加了一个Sigmoid层</p><pre><code class="lang-python">torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;, pos_weight=None)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</script><script type="math/tex; mode=display">        \ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="MarginRankingLoss"><a href="#MarginRankingLoss" class="headerlink" title="MarginRankingLoss"></a>MarginRankingLoss</h3><p>评价相似度的损失</p><pre><code class="lang-python">torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})</script><h3 id="HingeEmbeddingLoss"><a href="#HingeEmbeddingLoss" class="headerlink" title="HingeEmbeddingLoss"></a>HingeEmbeddingLoss</h3><p>用于学习非线性嵌入或半监督学习</p><pre><code class="lang-python">torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">L = \{l_1,\dots,l_N\}^\top, \quadl_n = \begin{cases}            x_n, & \text{if}\; y_n = 1,\\            \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,        \end{cases}</script><script type="math/tex; mode=display">        \ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="MultiLabelMarginLoss"><a href="#MultiLabelMarginLoss" class="headerlink" title="MultiLabelMarginLoss"></a>MultiLabelMarginLoss</h3><p>多类别多分类的Hinge损失</p><pre><code class="lang-python">torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</script><p>其中$x \in \left{0, \; \cdots , \; \text{x.size}(0) - 1\right}$,$y \in \left{0, \; \cdots , \; \text{y.size}(0) - 1\right}$,$0 \leq y[j] \leq \text{x.size}(0)-1$,$i \neq y[j]$</p><h3 id="SmoothL1Loss"><a href="#SmoothL1Loss" class="headerlink" title="SmoothL1Loss"></a>SmoothL1Loss</h3><p>也叫Huber Loss,误差在(-1,1)上是平方损失,其他情况是L1损失</p><pre><code class="lang-python">torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</script><script type="math/tex; mode=display">z_{i} =        \begin{cases}        0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\        |x_i - y_i| - 0.5, & \text{otherwise }        \end{cases}</script><h3 id="SoftMarginLoss"><a href="#SoftMarginLoss" class="headerlink" title="SoftMarginLoss"></a>SoftMarginLoss</h3><p>多标签二分类问题</p><pre><code class="lang-python">torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</script><h3 id="MultiLabelSoftMarginLoss"><a href="#MultiLabelSoftMarginLoss" class="headerlink" title="MultiLabelSoftMarginLoss"></a>MultiLabelSoftMarginLoss</h3><p>多标签多分类</p><pre><code class="lang-python">torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)</script><p>其中$i \in \left{0, \; \cdots , \; \text{x.nElement}() - 1\right}$,$y[i] \in \left{0, \; 1\right}$</p><h3 id="CosineEmbeddingLoss"><a href="#CosineEmbeddingLoss" class="headerlink" title="CosineEmbeddingLoss"></a>CosineEmbeddingLoss</h3><p>余玄相似度损失</p><pre><code class="lang-python">torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        \text{loss}(x, y) =        \begin{cases}        1 - \cos(x_1, x_2), & \text{if } y = 1 \\        \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1        \end{cases}</script><h3 id="MultiMarginLoss"><a href="#MultiMarginLoss" class="headerlink" title="MultiMarginLoss"></a>MultiMarginLoss</h3><p>多分类的Hinge损失</p><pre><code>torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \frac{\sum_i \max(0, (\text{margin} - x[y] + x[i])^p)}{\text{x.size}(0)}</script><p>其中$x \in \left{0, \; \cdots , \; \text{x.size}(0) - 1\right}$,$i \neq y$</p><script type="math/tex; mode=display">\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i])^p)}{\text{x.size}(0)}</script><h3 id="TripletMarginLoss"><a href="#TripletMarginLoss" class="headerlink" title="TripletMarginLoss"></a>TripletMarginLoss</h3><pre><code class="lang-python">torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</script><script type="math/tex; mode=display">d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;激励函数&quot;&gt;&lt;a href=&quot;#激励函数&quot; class=&quot;headerlink&quot; title=&quot;激励函数&quot;&gt;&lt;/a&gt;激励函数&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;#激活函数
x=np.arange(-12.5,12.5,0.05)
      
    
    </summary>
    
      <category term="神经网络" scheme="https://www.vhcffh.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="损失函数" scheme="https://www.vhcffh.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
      <category term="tensor变换" scheme="https://www.vhcffh.com/tags/tensor%E5%8F%98%E6%8D%A2/"/>
    
  </entry>
  
</feed>
