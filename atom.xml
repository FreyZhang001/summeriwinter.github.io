<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Frey&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.vhcffh.com/"/>
  <updated>2019-09-09T12:01:03.496Z</updated>
  <id>https://www.vhcffh.com/</id>
  
  <author>
    <name>Frey</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>nvidia显卡驱动错误</title>
    <link href="https://www.vhcffh.com/2019/nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E9%94%99%E8%AF%AF/"/>
    <id>https://www.vhcffh.com/2019/nvidia显卡驱动错误/</id>
    <published>2019-09-09T11:40:03.000Z</published>
    <updated>2019-09-09T12:01:03.496Z</updated>
    
    <content type="html"><![CDATA[<p>系统使用<code>bumblebee</code>实现双显卡(N卡和集成显卡)切换，<br>同时有N卡的开源驱动nouveau和专有显卡驱动nvidia</p><p>使用命令</p><pre><code class="lang-sh">optirun glxspheres64# 或者optirun glxspher</code></pre><p>出现错误<code>[XORG] (EE) Failed to load module &quot;nouveau&quot;</code><br>通过命令<code>lsmod |grep nouveau</code><br>显示模块已经加载</p><p>通过<code>/etc/modprobe.d/blacklist.conf</code>文件禁用<code>nouveau</code>(需要重建内核<code>mkinitcpio -P</code>并重启),但<code>nouveau</code>驱动仍然加载了，<code>nvidia</code>未加载</p><p>解决方法修改文件<code>/etc/bumblebee/xorg.conf.nouveau</code><br>去掉<code>BusID &quot;PCI:01:00:0&quot;</code>的注释<br><code>PCI:01:00:0</code>通过命令<code>lspci | egrep (3D|VGA)</code></p><p>重启后<code>nvidia</code>驱动加载成功,<code>nouveau</code>没有加载</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;系统使用&lt;code&gt;bumblebee&lt;/code&gt;实现双显卡(N卡和集成显卡)切换，&lt;br&gt;同时有N卡的开源驱动nouveau和专有显卡驱动nvidia&lt;/p&gt;
&lt;p&gt;使用命令&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;lang-sh&quot;&gt;optirun glxspher
      
    
    </summary>
    
      <category term="软件使用" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/"/>
    
      <category term="Linux" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/Linux/"/>
    
    
      <category term="nvidia" scheme="https://www.vhcffh.com/tags/nvidia/"/>
    
      <category term="nouveau" scheme="https://www.vhcffh.com/tags/nouveau/"/>
    
  </entry>
  
  <entry>
    <title>基于统计的分词方法</title>
    <link href="https://www.vhcffh.com/2019/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95/"/>
    <id>https://www.vhcffh.com/2019/基于统计的分词方法/</id>
    <published>2019-09-08T10:26:38.000Z</published>
    <updated>2019-09-09T12:13:11.557Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-n元语言模型-n-gram"><a href="#1-n元语言模型-n-gram" class="headerlink" title="1.n元语言模型(n-gram)"></a>1.n元语言模型(n-gram)</h2><p>假设$S$表示长度为$i$，由$(w_1,w_2,\dots,w_m)$字序列组成的句子，则代表$S$的概率为：</p><script type="math/tex; mode=display">P(S) = P(w_1,w_2,\dots,w_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_2,w_1)\cdots P(w_i|w_1,w_2,\dots,w_{m-1})</script><ul><li><p>$n=1$,uni-gram</p><script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i)</script></li><li><p>$n=2$,bi-gram</p><script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i|w_{i-1})</script></li><li><p>$n=3$,tri-gram</p><script type="math/tex; mode=display">P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i|w_{i-2}w_{i-1})</script></li></ul><h2 id="2-基于HMM的分词"><a href="#2-基于HMM的分词" class="headerlink" title="2.基于HMM的分词"></a>2.基于HMM的分词</h2><p><strong>HMM的参数</strong></p><p>观测序列(输出状态序列),状态序列(隐藏状态序列),初始概率,转移概率(转移概率矩阵),发射概率(发射概率矩阵)<br><img src="/images/2019/9_8_ma.png"><br><strong>数学定义</strong></p><p>状态值集合(隐藏状态):$Q=\{q_1,q_2,\cdots,q_N\}$,$N$为可能的状态数,对应状态序列$I$</p><p>观测值集合(输出状态):$V=\{v_1,v_2,\cdots,v_M\}$,$M$为可能的观测数,对应观测序列$O$</p><p>转移概率矩阵:$A=[a_{ij}];i,j\in\{1,2,\cdots,N\}$,从$i$状态到$j$状态的转换概率($\sum_{j=1}^Na_{ij}=1$)</p><p>发射概率矩阵(观测概率矩阵):$B=[b_j(k)];j\in\{1,2,\cdots,N\},k\in\{1,2,\cdots,M\}$,从状态$j$生成观测$k$的概率</p><p>初始状态分布:$\pi$</p><p>模型:$\lambda=(A,B,\lambda)$,状态序列$I$,观测序列$O$</p><p><strong>HMM中的三个问题:</strong></p><ul><li>概率计算问题:已知模型,求观测序列$O$出现的概率;前向后向算法<br><img src="/images/2019/9_8_ma.png"></li></ul><ul><li><p>学习问题:已知观测序列,求模型参数,最大化$P(O|\lambda)$;鲍姆-韦尔奇(Baum-Welch)算法</p><ul><li><p>已知隐藏序列和观测序列(通过频数估计)</p><p>$A_{ij}$表示隐藏状态$q_i$转移到$q_j$的频率计数</p><script type="math/tex; mode=display">A=[a_{ij}];a_{ij}=\frac{A_{ij}}{\sum_{s=1}^NA_{is}}</script><p>$B_{jk}$表示隐藏状态$q_j$转移到观测状态$v_k$的频率计数</p><script type="math/tex; mode=display">B=[b_j(k)];b_j(k)=\frac{B_{jk}}{\sum_{s=1}^MB_{js}}</script><p>$C(i)$为所有样本中初始隐藏状态$q_j$的频率计数</p><script type="math/tex; mode=display">\Pi = \pi(i)=\frac{C(i)}{\sum_{s=1}^NC(s)}</script></li><li><p>仅知观测序列(鲍姆-韦尔奇算法,EM算法)</p><p>EM算法:最大似然估计用于没有隐变量的概率模型,EM算法可以用于有隐变量的算法模型</p><p>模型参数:</p><script type="math/tex; mode=display">\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)</script></li></ul></li></ul><ul><li>解码问题:已知模型$\lambda$与观测序列$O$,求状态序列$I$最大化$P(I|O)$;维特比(Viterbi)算法</li></ul><p>HMM是一种序列模型,不仅可以用到自然语言处理这个领域,其他领域的应用也很常见:<a href="http://tecdat.cn/%e7%94%a8%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%86%e5%88%ab%e4%b8%8d%e6%96%ad%e5%8f%98%e5%8c%96%e7%9a%84%e8%82%a1%e5%b8%82%e7%8a%b6%e5%86%b5-%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab/" target="_blank" rel="noopener">股指预测</a>,<a href="https://www.jianshu.com/p/16fc3712fdf6" target="_blank" rel="noopener">语音识别</a>,<a href="http://www.c-a-m.org.cn/CN/abstract/abstract4637.shtml" target="_blank" rel="noopener">网络安全</a>,<a href="https://arxiv.org/abs/1901.06286" target="_blank" rel="noopener">基因序列</a>等方面</p><p>在自然语言处理中,HMM可以应用在分词,词性标注,命名实体识别等各个方面.</p><p>在分词方面可以这样理解HMM</p><p>观测序列(输出状态序列)—-序列构成的句子或短文</p><p>状态序列(隐藏状态序列)—-标注</p><p>初始概率—-统计的第一个字序的概率</p><p>转移概率(转移概率矩阵)—-第$i$个字序到第$i+1$个自序的标注变换概率</p><p>发射概率(发射概率矩阵)—-从隐藏状态到输出状态的转换概率</p><font color="red">如果把观测序列看作标注,状态序列看作句子,从解码问题转变成学习问题会怎样</font><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.cnblogs.com/hiyoung/archive/2018/09/25/9703976.html" target="_blank" rel="noopener">https://www.cnblogs.com/hiyoung/archive/2018/09/25/9703976.html</a></p><p>2.<a href="http://www.52nlp.cn/hmm-learn-best-practices-one-introduction" target="_blank" rel="noopener">http://www.52nlp.cn/hmm-learn-best-practices-one-introduction</a></p><p>3.<a href="https://www.cnblogs.com/en-heng/p/6164145.html?utm_source=debugrun&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/en-heng/p/6164145.html?utm_source=debugrun&amp;utm_medium=referral</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-n元语言模型-n-gram&quot;&gt;&lt;a href=&quot;#1-n元语言模型-n-gram&quot; class=&quot;headerlink&quot; title=&quot;1.n元语言模型(n-gram)&quot;&gt;&lt;/a&gt;1.n元语言模型(n-gram)&lt;/h2&gt;&lt;p&gt;假设$S$表示长度为$i$，由$
      
    
    </summary>
    
      <category term="自然语言处理" scheme="https://www.vhcffh.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="分词" scheme="https://www.vhcffh.com/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="统计学" scheme="https://www.vhcffh.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>神经图灵机</title>
    <link href="https://www.vhcffh.com/2019/%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA/"/>
    <id>https://www.vhcffh.com/2019/神经图灵机/</id>
    <published>2019-09-07T02:56:31.000Z</published>
    <updated>2019-09-07T02:57:22.270Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Neural-Turing-Machines-原文"><a href="#Neural-Turing-Machines-原文" class="headerlink" title="Neural Turing Machines[原文]"></a>Neural Turing Machines[<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">原文</a>]</h3><ul><li><p>读记忆(Read Heads)</p><p>把时刻$t$的记忆看作是一个$N \times M$的矩阵$M_t$,读的过成首先生成长度为$N$的定位权重向量$w_t$,表示$N$个记忆位置的权值大小,读出的记忆向量为$r_t$:</p><script type="math/tex; mode=display">r_t=\sum_i^Nw_t(i)M_t(i) \ \ \ 其中 \sum_iw_t(i)=1</script><p>对$N$条记忆进行加权求和</p></li><li><p>写记忆(Write Heads)</p><p>类似LSTM:擦除向量$e_t$,增加向量$a_t$</p><ul><li><p>擦除操作:</p><script type="math/tex; mode=display">M_t'(i)=M_{t-1}(1-w_t(i)e_t(i))</script></li><li><p>增加操作:</p><script type="math/tex; mode=display">M_t(i)=M_t'(i)+w_t(i)a_t(i)</script></li></ul><p>神经图灵机的关键是定位向量$w_t$,其它的是由控制器(LSTM,MLP)输出</p></li><li><p>定位机制(Addressing Mechanism)</p><p>结合了基于内容和基于位置的两种方法</p><ul><li><p>基于内容(Content-based Addressing)</p><script type="math/tex; mode=display">w_t^c(i)=\frac {\exp(\beta_tK[k_t,M_t(i)])}{\sum_j\exp(\beta_tK[k_t,M_t(j)])}</script><p>$K[.,.]$是余弦相似度计算:</p><script type="math/tex; mode=display">K[u,v]=\frac{u \cdot v}{\Vert u\Vert \cdot \Vert v\Vert}</script><p>$\beta_t$是控制器输出</p></li><li><p>基于位置(Location-based Addressing)</p><ul><li><p>插值(Interpolation)</p><script type="math/tex; mode=display">w_t^g=g_tw_t^c+(1-g_t)w_{t-1}</script><p>$g_t$有控制器生成</p></li><li><p>偏移(shift)</p><script type="math/tex; mode=display">\tilde w_t(i)=\sum_{j=0}^{N-1}w_t^g(j)s_t(i-j)</script><p>每一个$\tilde w_t(i)$都与相邻元素有关</p></li><li><p>重塑(Sharping)</p><script type="math/tex; mode=display">w_t(i)=\frac{\tilde w_t(i)^{\gamma_t}}{\sum_j\tilde w_t(j)^{\gamma_t}}</script></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Neural-Turing-Machines-原文&quot;&gt;&lt;a href=&quot;#Neural-Turing-Machines-原文&quot; class=&quot;headerlink&quot; title=&quot;Neural Turing Machines[原文]&quot;&gt;&lt;/a&gt;Neural Tur
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="图灵机" scheme="https://www.vhcffh.com/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"/>
    
      <category term="神经网络" scheme="https://www.vhcffh.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>运用自注意力机制检测相关语境进行多论对话</title>
    <link href="https://www.vhcffh.com/2019/%E8%BF%90%E7%94%A8%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A3%80%E6%B5%8B%E7%9B%B8%E5%85%B3%E8%AF%AD%E5%A2%83%E8%BF%9B%E8%A1%8C%E5%A4%9A%E8%AE%BA%E5%AF%B9%E8%AF%9D/"/>
    <id>https://www.vhcffh.com/2019/运用自注意力机制检测相关语境进行多论对话/</id>
    <published>2019-09-07T02:49:03.000Z</published>
    <updated>2019-09-07T02:55:33.049Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-forMulti-turn-Dialogue-Generation-原文"><a href="#ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-forMulti-turn-Dialogue-Generation-原文" class="headerlink" title="ReCoSa: Detecting the Relevant Contexts with Self-Attention forMulti-turn Dialogue Generation[原文]"></a>ReCoSa: Detecting the Relevant Contexts with Self-Attention forMulti-turn Dialogue Generation[<a href="https://arxiv.org/abs/1907.05339" target="_blank" rel="noopener">原文</a>]</h3><p>模型</p><h4 id="Word-level-Encoder"><a href="#Word-level-Encoder" class="headerlink" title="Word-level Encoder"></a>Word-level Encoder</h4><ul><li>文本集</li></ul><script type="math/tex; mode=display">C = \{s_1,\dots,s_N\}</script><ul><li>句子</li></ul><script type="math/tex; mode=display">s_i=\{x_1,\dots,x_M\}</script><ul><li><p>输入句子$s_i$,编码输出$h_M$</p><script type="math/tex; mode=display">i_k=\sigma(W_i[h_{k-1},w_k]),\ \ f_k=\sigma(W_f[h_{k-1},w_k]), \\o_k=\sigma(W_o[h_{k-1},w_k]),\ \ l_k= tanh (W_l[h_{k-1},w_k]), \\c_k=f_kc_{k-1}+i_kl_k,\ \ h_i=o_ktanh(c_k)</script><ul><li>$i_k$:输入,$f_k$:记忆,$o_k$:输出.</li><li>$w_k$是$x_k$的词嵌入</li><li>$W_i,W_f,W_o,W_l$是模型参数</li><li>用$h_M$代表一个句子,得到句子表示集$\{h^{s_1},\dots,h^{s_N}\}$</li></ul></li><li><p>定义位置嵌入向量$P_i \in \mathbb R^d,i=1,\dots,N$</p><p>得到句子表示$\{(h^{s_1},P_1),\dots,(h^{s_N},P_N)\}$</p></li></ul><h4 id="注意力计算-参见论文attention-is-all-you-need"><a href="#注意力计算-参见论文attention-is-all-you-need" class="headerlink" title="注意力计算(参见论文attention-is-all-you-need)"></a>注意力计算(参见论文<a href="./7181-attention-is-all-you-need.pdf">attention-is-all-you-need</a>)</h4><ul><li><p>定义注意力分数</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac {QK^T}{\sqrt d}V)</script><p>其中$Q\in\mathbb R^{n \times d},K\in\mathbb R^{n\times d},V\in \mathbb R^{n\times d}$</p><p>$d$是隐藏单元</p></li><li><p><font color="red">multi-head attention</font>(多层scaled dot-product attention)</p><script type="math/tex; mode=display">M_i=Attention(QW_i^Q,KW_i^K,VW_i^V) \\M=Concat(M_1,\dots,M_H) \\O=MW</script><p>其中$W_i^Q\in\mathbb R^{n \times d/H},W_i^K\in\mathbb R^{n\times d/H},W_i^V\in \mathbb R^{n\times d/H}$,$M\in\mathbb R^{n\times d},W\in \mathbb R^{d\times d}$</p><p>$\{(h^{s_1},P_1),\dots,(h^{s_N},P_N)\}$-&gt;$O_s$-&gt;<font color="red">feed-forward network</font>-&gt;$O_s^f$</p><p>是否可以将语境检测运用到机器翻译中?</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-forMulti-turn-Dialogue-Generation-原文&quot;&gt;&lt;a href=&quot;#ReCoSa-Detecting-the-Rele
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="多论对话" scheme="https://www.vhcffh.com/tags/%E5%A4%9A%E8%AE%BA%E5%AF%B9%E8%AF%9D/"/>
    
      <category term="Self-Attention" scheme="https://www.vhcffh.com/tags/Self-Attention/"/>
    
  </entry>
  
  <entry>
    <title>小样本学习的边缘标签图神经网络</title>
    <link href="https://www.vhcffh.com/2019/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BE%B9%E7%BC%98%E6%A0%87%E7%AD%BE%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://www.vhcffh.com/2019/小样本学习的边缘标签图神经网络/</id>
    <published>2019-09-06T12:31:53.000Z</published>
    <updated>2019-09-07T02:53:59.941Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文"><a href="#Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文" class="headerlink" title="Edge-Labeling Graph Neural Network for Few-shot Learning[原文]"></a>Edge-Labeling Graph Neural Network for Few-shot Learning[<a href="https://arxiv.org/abs/1905.01436" target="_blank" rel="noopener">原文</a>]</h3><p>小样本学习的边缘标签图神经网络</p><p>将样本映射到图中的节点,边表示节点之间的相似度,通过已知样本预测未知样本</p><h4 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h4><p>主要利用邻域聚合框架(通过对相邻节点特征的递归聚合和转换,计算出节点特征)进行表征学习</p><h4 id="Edge-Labeling-Graph"><a href="#Edge-Labeling-Graph" class="headerlink" title="Edge-Labeling Graph"></a>Edge-Labeling Graph</h4><p><strong>Correlation clustering</strong></p><p>相关性聚类分析:一个通过同时最大化簇内相似性和簇间差异性来实现边标注推理的图分割算法</p><ul><li>通过结构化向量机实现名词短语聚类和新闻文章聚类</li></ul><h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><p>C-way K-shot:一个meta-task,包括support set随机抽取C个类别,每个类别K个样本(共CK个数据);和batch set从这C各类别种抽取一批样本.</p><font color="red">Mode Based, Metric Based,Optimization Based</font><h4 id="Problem-definition-Few-shot-classification"><a href="#Problem-definition-Few-shot-classification" class="headerlink" title="Problem definition: Few-shot classification"></a>Problem definition: Few-shot classification</h4><ul><li>在每个类只有很少样本的情况下学习一个分类器</li><li>每一个few-shot分类任务$\mathcal T$包括support set $\mathcal S$和query set$\mathcal Q$</li><li>episodic training：在training task中抽样，模拟少样本测试时的场景<ul><li>$\mathcal T=\mathcal S\cup\mathcal U, \mathcal S=\{(x_i,y_i)\}_{i=1}^{N\times{K}} \text{ and }\mathcal{Q}=\{(x_{i},y_{i})\}_{i=N\times{K}+1}^{N\times{K}+T}$</li><li>$x_i,y_i\in\{C_1,…,C_N\}=\mathcal C_{\mathcal T}\subset\mathcal C$同时$\mathcal C_{train}\cap\mathcal C_{test} = \empty$</li></ul></li></ul><h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><ol><li><p>通过卷积神经网络提取样本特征,$\theta_{emb}$是卷积网络的参数</p><script type="math/tex; mode=display">\mathbf v_i^0=f_{emb}(x_i;\theta_{emb})</script></li><li><p>图构建:$\mathcal g=(\mathcal V,\mathcal E;\mathcal T)$每一个节点代表一个样本，全连接图，每一条边代表一种关系种类</p><p>其中$\mathcal V :=\{V_i\}_{i=1,\dots,|\mathcal T|}$,$\mathcal E :=\{E_{ij}\}_{i,j=1,\dots,|\mathcal T|}$,</p><ol><li>ground truth: edge-label</li></ol><script type="math/tex; mode=display">y_{ij}=\begin{cases}1,\text{if }y_i=y_j,\\0,\text{otherwise.}\end{cases}</script><ol><li>边特征$\mathbf e_{ij}=\{e_{ijd}\}^2_{d=1} \in [0,1]^2$是一个二维矩阵,表示两个之节点间的簇内关系和簇间关系的强弱,||表示连接</li></ol><script type="math/tex; mode=display">\mathbf e_{ij}^0=\begin{cases}\ \ \ [1||0],\ \ \ \ if \ y_{ij}=1\ \ and\ \ i,j\le N \times K, \\\ \ \ [0||1],\ \ \ \ if \ y_{ij}=0\ \ and\ \ i,j\le N \times K, \\[0.5||0.5],\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ otherwise,\end{cases}</script></li></ol><ol><li><p>传播:设从$\ell -1 $层得到的特征为$\mathbf v_i^{\ell-1}$ 和$\mathbf e_{ij}^{\ell-1}$</p><ol><li><p>更新节点特征</p><script type="math/tex; mode=display">\mathbf v_i^\ell=f_v^\ell([\sum_j\tilde e_{ij1}^{\ell-1}\mathbf v_j^{\ell-1}||\sum_j\tilde e_{ij2}^{\ell-1}\mathbf v_j^{\ell-1}];\theta_v^\ell)</script><p>其中$\tilde e_{ij1}^{\ell-1}=\frac {e_{ijd}}{\sum_ke_{ikd}}$,$f_v^\ell(\theta)$是节点特征的转变网络(MLP).</p></li><li><p>更新边特征</p><script type="math/tex; mode=display">\begin{align}\bar e_{ij1}^\ell \ &=\  \frac {f_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell)e_{ij1}^{\ell-1}}{\sum_kf_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell)e_{ij1}^{\ell-1}/(\sum_ke_{ik1}^{\ell-1})},\\\bar e_{ij2}^\ell \ &=\  \frac {(1-f_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell))e_{ij2}^{\ell-1}}{\sum_k(1-f_e^\ell(\mathbf v_j^\ell,\mathbf v_j^\ell;\theta_e^\ell))e_{ij2}^{\ell-1}/(\sum_ke_{ik2}^{\ell-1})},\\\mathbf e_{ij}^\ell \ &=\ \bar {\mathbf e}_{ij}^\ell/\Vert \bar {\mathbf e}_{ij}^\ell \Vert_1\end{align}</script><p>其中 $f_e^\ell$是<font color="red">metric network</font>计算节点相似度</p></li></ol></li><li><p>输出结果:节点属于某个集合的分布$P(y_i=\mathcal C_k|\mathcal T)=p_i^{(k)}$</p><script type="math/tex; mode=display">p_i^{(k)}=softmax(\sum_{\{j:j\neq i\land(\mathbf x_j,y_j)\in \mathcal S\}}\hat y_{ij}\delta(y_j=\mathcal C_k))</script><script type="math/tex; mode=display">\hat y_{ij}=e_{ij}^L \in [0,1]</script></li></ol><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>其中模型参数有$\theta_{emb} \cup\{\theta_v^\ell,\theta_e^\ell\}_{\ell=1}^L$,对于M个训练任务$\{\mathcal T_m^{train}\}_{m=1}^M$,损失函数:</p><script type="math/tex; mode=display">\mathcal L=\sum_{\ell=1}^L\sum_{m=1}^M\lambda_\ell\mathcal L(Y_{m,e},\hat Y_{m,e}^\ell)</script><p>其中$Y_{m,e}$和$\hat Y_{m,e}^\ell$是m个任务,e条边的真实值和预测值(第$\mathcal L$层的),$\mathcal L$是交叉熵损失函数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文&quot;&gt;&lt;a href=&quot;#Edge-Labeling-Graph-Neural-Network-for-Few-shot-Learning-原文&quot; 
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://www.vhcffh.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="图网络" scheme="https://www.vhcffh.com/tags/%E5%9B%BE%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>记一次GRUB引导修复</title>
    <link href="https://www.vhcffh.com/2019/%E8%AE%B0%E4%B8%80%E6%AC%A1GRUB%E5%BC%95%E5%AF%BC%E4%BF%AE%E5%A4%8D/"/>
    <id>https://www.vhcffh.com/2019/记一次GRUB引导修复/</id>
    <published>2019-09-03T11:13:42.000Z</published>
    <updated>2019-09-03T11:39:20.792Z</updated>
    
    <content type="html"><![CDATA[<p>使用Arch Linux已经2年多了，基本上已经习惯了，可前段时间英雄联盟也出了“自走棋”，想要体验一把。<br>于是就在硬盘剩余的60多个G上装了Windows 10，用了一两个月没有问题，最后再一次Windows 10更新后，蓝屏了。又手惨的点了Windows 的修复功能。等了半天，结果是修复，重启，蓝屏无限循环。而且Arch Linux的引导全都没了，感激应该是Windows的修复动了efi分区。<br>无奈只能新修复引导，各种方法尝试了好多边都是卡在了<code>GRUB _</code>一直闪这个状态，最终在官网wiki找到了办法<br><a href="https://wiki.archlinux.org/index.php/GRUB_#Default/fallback_boot_path" target="_blank" rel="noopener">缺省/后备启动路径</a><br>grub 安装时添加<code>--removable</code>参数</p><pre><code class="lang-sh">grub-install --target=x86_64-efi --efi-directory=esp --removable</code></pre><p>或者手动移动</p><pre><code class="lang-sh">mv esp/EFI/grub esp/EFI/BOOTmv esp/EFI/BOOT/grubx64.efi esp/EFI/BOOT/BOOTX64.EFI</code></pre><p>grub出现bug的原因应该会千奇百怪，不要尽可能多的尝试网上的各种修改方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用Arch Linux已经2年多了，基本上已经习惯了，可前段时间英雄联盟也出了“自走棋”，想要体验一把。&lt;br&gt;于是就在硬盘剩余的60多个G上装了Windows 10，用了一两个月没有问题，最后再一次Windows 10更新后，蓝屏了。又手惨的点了Windows 的修复功
      
    
    </summary>
    
      <category term="软件使用" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/"/>
    
      <category term="Linux" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/Linux/"/>
    
    
      <category term="grub" scheme="https://www.vhcffh.com/tags/grub/"/>
    
  </entry>
  
  <entry>
    <title>深度学习在中文分词和词性标注中的应用</title>
    <link href="https://www.vhcffh.com/2019/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%92%8C%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>https://www.vhcffh.com/2019/深度学习在中文分词和词性标注中的应用/</id>
    <published>2019-08-25T02:37:36.000Z</published>
    <updated>2019-08-25T02:39:56.515Z</updated>
    
    <content type="html"><![CDATA[<p>得到字向量-&gt;通过窗口方法得到字与上下文有关的向量(矩阵)-&gt;通过两个线性层和一个非线性激活函数-&gt;字的标注得分(窗口方法)-&gt;一个句子的评分矩阵$f_\theta (c_{[1:n]})$ (句子中的第$i$ 个子为标签$t$ 的得分)-&gt;定义转换分数$A_{ij}$,得到tag path 得分</p><script type="math/tex; mode=display">s(c_{[1:N]},t_{[1:N]},\theta)=\sum_{i=1}^n(A_{t_{i-1}t_i}+f_\theta (t_i\vert i))</script><p>-&gt;最大化得分,得到最优tag path</p><p>log likelihood</p><script type="math/tex; mode=display">\sum_{\forall(c,t)\in R}\log p(t\vert c,\theta)</script><p>将目标函数转换为条件概率</p><script type="math/tex; mode=display">p(t\vert c,\theta)=\frac {e^{s(c,t,\theta)}}{\sum_{\tilde t}{e^{s(c,\tilde t,\theta)}}}</script><p>取对数</p><script type="math/tex; mode=display">\log p(t\vert c,\theta) = s(c,t,\theta)-log\sum_{\tilde t}{e^{s(c,\tilde t,\theta)}}</script><p>维特比算法(viterbi)</p><p>一种动态规划算法(穷举法,A*算法,beam search,Viterbi算法)</p><font color="red" face="微软雅黑">A\*算法和Viterbi算法的区别?</font><p>新的训练方法</p><p>将维特比算法在当前参数下得出的最优路径结果与正确结果进行比较对比,定义出损失函数对$A_{t_{i-1}t_i}$ 和$f_\theta (t_i|i)$ 的偏导数,通过后向传播更新参数</p><p>收敛性的证明:Discriminative training methods for hidden Markov models</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;得到字向量-&amp;gt;通过窗口方法得到字与上下文有关的向量(矩阵)-&amp;gt;通过两个线性层和一个非线性激活函数-&amp;gt;字的标注得分(窗口方法)-&amp;gt;一个句子的评分矩阵$f_\theta (c_{[1:n]})$ (句子中的第$i$ 个子为标签$t$ 的得分)-&amp;gt;定
      
    
    </summary>
    
      <category term="论文笔记" scheme="https://www.vhcffh.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="分词" scheme="https://www.vhcffh.com/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="深度学习" scheme="https://www.vhcffh.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词性标注" scheme="https://www.vhcffh.com/tags/%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>OWL基础</title>
    <link href="https://www.vhcffh.com/2019/OWL%E5%9F%BA%E7%A1%80/"/>
    <id>https://www.vhcffh.com/2019/OWL基础/</id>
    <published>2019-08-22T03:53:16.000Z</published>
    <updated>2019-08-22T04:02:33.451Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OWL基础"><a href="#OWL基础" class="headerlink" title="OWL基础"></a>OWL基础</h2><p>网络本体语言Web Ontologoy Language</p><p>OWL Lite -&gt;OWL DL-&gt;OWL Full 递进关系</p><h3 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h3><ol><li><p>类(Class)</p><p>任何东西都是类owl:Thing 的一个成员(实例);子类:Subclass;相等关系:equivalentClass</p></li><li><p>个体(Individual)</p><p>与实例的概念差不多</p><p>一个个体可以属于多个类(没有指名是哪个类)</p><p>某个类的实例(指名了类)</p><p>子类与个体是不同的东西</p></li><li><p>属性(Property)</p><p>一个二元关系,OWL中包括两种属性:</p><ol><li>类型属性（datatype properties）：描述类与其实例之间关系的属性。</li><li>对象属性（object properties）：描述两个不同类的实例之间关系的属性。</li></ol><p>属性有两个端点:起点和终点,都应该是两个个体(实例)</p><p>用原集(domain)起点的实例的类,用象集(range)描述终点的实例的类.</p><p>属性也有子属性(Subproperty)</p></li></ol><p>公理和约束</p><h3 id="rdf资源描述框架-Resource-Description-Framework"><a href="#rdf资源描述框架-Resource-Description-Framework" class="headerlink" title="rdf资源描述框架(Resource Description Framework)"></a>rdf资源描述框架(Resource Description Framework)</h3><p>W3C提出的一组标记语言的技术规范,是一种数据模型,rdf数据集的序列化方法</p><p>OWL是由DAML(DARPA Agent Markup Language)+OIL(Ontology Inference Layer)演变而来。</p><p>OWL是RDF的扩张，为我们提供了更广泛的定义RDFS词汇的功能，更广泛意指可以定义词汇之间的关系，类与类间的关系，属性与属性之间的关系等</p><p>foaf(Friend-of-a-Friend)是一种XML/RDF词汇表,不管通过那种那种方法表示数据资源,都要指定词汇表</p><ol><li><p>RDF/XML</p><p>XML的技术程序;格式太冗长,不便于阅读</p></li><li><p><a href="https://www.w3.org/TR/n-triples/" target="_blank" rel="noopener">N-Triples</a></p><p>三元组表示;开放领域知识图谱<a href="https://wiki.dbpedia.org/" target="_blank" rel="noopener">DBpedia</a>通常使用这种格式发布数据.</p></li><li><p><a href="https://www.w3.org/TR/turtle/" target="_blank" rel="noopener">Turtle</a></p><p>使用最多的一种RDF序列化方法,比RDF/XML紧凑,可读性比N-Triples好</p></li><li><p>RDFa</p><p>The Resource Description Framework in Attributes,HTML5的一个扩展,不改变任何显示效果的情况下,然网站更容易被搜索引擎解析</p></li><li><p>JSON-LD</p><p>JSON for Linking Data,用键值对的方法来存储RDF</p></li></ol><h3 id="RDF-XML"><a href="#RDF-XML" class="headerlink" title="RDF/XML"></a>RDF/XML</h3><pre><code class="lang-xml">&lt;?xml version=&quot;1.0&quot;?&gt;&lt;rdf:RDFxmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;xmlns:cd=&quot;http://www.recshop.fake/cd#&quot;&gt;&lt;rdf:Descriptionrdf:about=&quot;http://www.recshop.fake/cd/Empire Burlesque&quot;&gt;  &lt;cd:artist&gt;Bob Dylan&lt;/cd:artist&gt;  &lt;cd:country&gt;USA&lt;/cd:country&gt;  &lt;cd:company&gt;Columbia&lt;/cd:company&gt;  &lt;cd:price&gt;10.90&lt;/cd:price&gt;  &lt;cd:year&gt;1985&lt;/cd:year&gt;&lt;/rdf:Description&gt;&lt;rdf:Descriptionrdf:about=&quot;http://www.recshop.fake/cd/Hide your heart&quot;&gt;  &lt;cd:artist&gt;Bonnie Tyler&lt;/cd:artist&gt;  &lt;cd:country&gt;UK&lt;/cd:country&gt;  &lt;cd:company&gt;CBS Records&lt;/cd:company&gt;  &lt;cd:price&gt;9.90&lt;/cd:price&gt;  &lt;cd:year&gt;1988&lt;/cd:year&gt;&lt;/rdf:Description&gt;...&lt;/rdf:RDF&gt;</code></pre><p> RDF 文档的第一行是 XML 声明。这个 XML 声明之后是 RDF 文档的根元素：<em><rdf:rdf></rdf:rdf></em>。</p><p><em>xmlns:rdf</em> 命名空间，规定了带有前缀 rdf 的元素来自命名空间 “<a href="http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;。" target="_blank" rel="noopener">http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;。</a></p><p><em>xmlns:cd</em> 命名空间，规定了带有前缀 cd 的元素来自命名空间 “<a href="http://www.recshop.fake/cd#&quot;。" target="_blank" rel="noopener">http://www.recshop.fake/cd#&quot;。</a></p><p><em><rdf:description></rdf:description></em> 元素包含了对被 <em>rdf:about</em> 属性标识的资源的描述。</p><p>元素：<em><cd:artist></cd:artist></em>、<em><cd:country></cd:country></em>、<em><cd:company></cd:company></em> 等是此资源的属性。</p><h3 id="Turtle序列化方法总结"><a href="#Turtle序列化方法总结" class="headerlink" title="Turtle序列化方法总结"></a>Turtle序列化方法总结</h3><ul><li><p>URI用 <strong>&lt;&gt;</strong> 描述</p><pre><code>&lt;http://example.org/path/&gt;&lt;http://example.org/path/#fragment&gt;</code></pre></li><li><p>前缀缩写(类似于RDF/XML的命名空间)</p><pre><code>@prefix foo:&lt;http://example.org/ns#&gt;@prefix  : &lt;http://example.org/ns1#&gt;:a :b :c</code></pre></li><li><p>字面量</p><p>一行或者多行， <code>@en</code>限定其语言， <code>^^xsd:decimal</code>限定其数据类型</p><pre><code>&quot;string&quot;&quot;&quot;&quot;many lines of stringmany lines of stringmany lines of string&quot;&quot;&quot;&quot;chat&quot;@en&quot;chat&quot;@fr&quot;10&quot;^^xsd:decimal</code></pre></li><li><p>空节点(RDF模型可能会存在未命名的空节点)</p><p><code>_:me</code>,<code>_a1234</code>分别代表一个空节点</p><pre><code>_:me_:a1234</code></pre></li><li><p>base URI</p><p>base URI定义后，接下来的<strong>URI, 前缀缩写，qualified names 和base URI</strong>都要受其作用</p><pre><code># this is a complete turtle document@base &lt;http://example.org/ns/&gt; .# base URIs 是 http://example.org/ns/@base &lt;foo/&gt; .# base URI 是 http://example.org/ns/foo/@prefix : &lt;bar#&gt; .:a4 :b4 :c4.</code></pre></li><li><p>对三元组进行缩写</p><pre><code>:a :b :c,      :d.#the last triple is :a :b :d.</code></pre></li><li><p>一个简单的完整turtle标准文件</p><p><img src="/images/2019/8_22_example.png" alt="img"></p></li></ul><pre><code>@prefix info: &lt;http://zy.example.com/info#&gt;@prefix rel: &lt;http://zy.example.com/rel#&gt;@prefix person: &lt;http://zy.example.com/person#&gt;person:Tom info:name &quot;Tom&quot;;          info:job &quot;worker&quot;;          info:age 56;          rel:fatherof person:Jim.person:Jim info:name &quot;Jim&quot;;          info:job &quot;programmer&quot;;          info:age 28;          rel:fatherof person:Cherry.person:Cherry info:name &quot;Cherry&quot;;             info:age 8;             .</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;OWL基础&quot;&gt;&lt;a href=&quot;#OWL基础&quot; class=&quot;headerlink&quot; title=&quot;OWL基础&quot;&gt;&lt;/a&gt;OWL基础&lt;/h2&gt;&lt;p&gt;网络本体语言Web Ontologoy Language&lt;/p&gt;
&lt;p&gt;OWL Lite -&amp;gt;OWL DL-&amp;
      
    
    </summary>
    
      <category term="自然语言处理" scheme="https://www.vhcffh.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="OWL" scheme="https://www.vhcffh.com/tags/OWL/"/>
    
      <category term="rdf" scheme="https://www.vhcffh.com/tags/rdf/"/>
    
      <category term="Turtle" scheme="https://www.vhcffh.com/tags/Turtle/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中的广义线性模型</title>
    <link href="https://www.vhcffh.com/2019/sklearn%E4%B8%AD%E7%9A%84%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>https://www.vhcffh.com/2019/sklearn中的广义线性模型/</id>
    <published>2019-08-20T10:13:01.000Z</published>
    <updated>2019-08-20T10:17:11.854Z</updated>
    
    <content type="html"><![CDATA[<p>模型的通用公式</p><script type="math/tex; mode=display">\hat y(w,x)=w_0+w_1x_1+\dots+w_px_p</script><p>其中$w=(w_1,\dots,w_p)$ 作为coef_;$w_0​$作为intercepr_</p><h3 id="普通最小二乘法"><a href="#普通最小二乘法" class="headerlink" title="普通最小二乘法"></a>普通最小二乘法</h3><script type="math/tex; mode=display">w=\min_w{\Vert Xw-y\Vert_2}^2</script><p>LinearRegression</p><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><script type="math/tex; mode=display">w=\min_w{\Vert Xw-y\Vert_2}^2+\alpha {\Vert w \Vert_2}^2</script><p>$\alpha$ 是控制系数收缩量的复杂性参数:$\alpha$ 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。</p><p>共线性:线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真</p><p>Ridge,RigdeCV:广义交叉验证(GCV),默认留一验证(LOO-CV)</p><h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><script type="math/tex; mode=display">w = \min_w\frac{1}{2n_{samples}}\Vert Xw-y \Vert_2^2 + \alpha \Vert w \Vert_1</script><p>$\alpha$ 是常数,$\Vert w \Vert_1$ 是参数向量的$l_{1-norm}$范数</p><p>Lasso,lasso_path:通过搜索所有可能的路径上的值来计算系数</p><p>LassoCV,LassoLarsCV,LassoLarsIC</p><h2 id="多任务Lasso"><a href="#多任务Lasso" class="headerlink" title="多任务Lasso"></a>多任务Lasso</h2><script type="math/tex; mode=display">w = \min_w\frac{1}{2n_{samples}}\Vert XW-Y\Vert_{Fro}^2+\alpha\Vert W \Vert_{21}</script><script type="math/tex; mode=display">\Vert A \Vert_{Fro}=\sqrt{\sum_{ij}a_{ij}^2}</script><script type="math/tex; mode=display">\Vert A \Vert_{21}= \sum_i\sqrt{\sum_j a_{ij}^2}</script><p>MultiTaskLasso</p><h3 id="弹性网络"><a href="#弹性网络" class="headerlink" title="弹性网络"></a>弹性网络</h3><script type="math/tex; mode=display">w = \min_w\frac{1}{2n_{samples}}\Vert Xw-Y\Vert_2^2+\alpha\rho\Vert w \Vert_{1}+\frac{\alpha(1-\rho)}{2}\Vert w\Vert_2^2</script><p>ElasticNetCV通过交叉验证来设置参数<code>alpha</code>($\alpha$)和<code>l1_rati0</code>($\rho$)</p><h3 id="多任务弹性网络"><a href="#多任务弹性网络" class="headerlink" title="多任务弹性网络"></a>多任务弹性网络</h3><script type="math/tex; mode=display">W = \min_W\frac{1}{2n_{samples}}\Vert XW-Y\Vert_{Fro}^2+\alpha\rho\Vert W \Vert_{21}+\frac{\alpha(1-\rho)}{2}\Vert w\Vert_{Fro}^2</script><p>MultiTaskElasticNet</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;模型的通用公式&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat y(w,x)=w_0+w_1x_1+\dots+w_px_p&lt;/script&gt;&lt;p&gt;其中$w=(w_1,\dots,w_p)$ 作为coef_;$w_0​$作为i
      
    
    </summary>
    
      <category term="机器学习" scheme="https://www.vhcffh.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="sklearn" scheme="https://www.vhcffh.com/tags/sklearn/"/>
    
      <category term="领回归" scheme="https://www.vhcffh.com/tags/%E9%A2%86%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>集成学习算法总结</title>
    <link href="https://www.vhcffh.com/2019/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://www.vhcffh.com/2019/集成学习算法总结/</id>
    <published>2019-08-19T10:39:18.000Z</published>
    <updated>2019-08-19T12:28:04.831Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成学习算法"><a href="#集成学习算法" class="headerlink" title="集成学习算法"></a>集成学习算法</h2><p>使用多个分类器提高整体的泛化能力</p><h3 id="1-Bagging（Bootstrap-Aggregating）算法"><a href="#1-Bagging（Bootstrap-Aggregating）算法" class="headerlink" title="1.Bagging（Bootstrap Aggregating）算法"></a>1.Bagging（Bootstrap Aggregating）算法</h3><p>通过组合随机生成的训练集而改进分类的集成算法(bootstrap)</p><p>使用训练集中的某个子集作为当前训练集（有放回随机抽样）;经过T次训练后,得到T个不同的分类器</p><p>调用这T个分类器,把这T个分类结果中出现次数多的类赋予测试样例</p><p>有效减少噪声影响</p><h3 id="2-Boosting算法"><a href="#2-Boosting算法" class="headerlink" title="2.Boosting算法"></a>2.Boosting算法</h3><p>初始化样本权重,生成一个弱分类器;</p><p>利用弱分类器增加分类错误的样本的权重;</p><p>不断重复,生成T个弱分类器;</p><p>对噪声敏感</p><p>改进算法-AdaBoosting算法</p><ul><li>对每一次的训练数据样本赋予一个权重，并且每一次样本的权重分布依赖上一次的分类结果</li><li>基分类器之间采用序列的线性加权方式来组合</li></ul><h3 id="3-Gradient-Boosting"><a href="#3-Gradient-Boosting" class="headerlink" title="3.Gradient Boosting"></a>3.Gradient Boosting</h3><p>1,初始化</p><script type="math/tex; mode=display">f_0(x)=\arg\min_\gamma\sum_{i=1}^NL(y_i,\gamma)</script><p>2.1计算负梯度</p><script type="math/tex; mode=display">\widetilde y_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, i=1,2,\cdots N</script><p>2.2用基学习器$h_m(x)$ 拟合$\widetilde y_i$</p><script type="math/tex; mode=display">w_m=\mathop{\arg\min}\limits_w\sum_{i=1}^N[\widetilde y_i-h_m(x_i;w)]^2</script><p>2.3确定步长$\rho_m$ </p><script type="math/tex; mode=display">\rho_m = \mathop{\arg\min}\limits_{\rho} \sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \rho h_m(x_i\,;\,w_m))</script><p>2.4更新$f_m(x)$ 最终得到$f_M(x)$ </p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(x) + \rho_m h_m(x\,;\,w_m)</script><p>Bagging + 决策树 = 随机森林</p><p>AdaBoost + 决策树 = 提升树</p><p>Gradient Boosting + 决策树 = GBDT</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;集成学习算法&quot;&gt;&lt;a href=&quot;#集成学习算法&quot; class=&quot;headerlink&quot; title=&quot;集成学习算法&quot;&gt;&lt;/a&gt;集成学习算法&lt;/h2&gt;&lt;p&gt;使用多个分类器提高整体的泛化能力&lt;/p&gt;
&lt;h3 id=&quot;1-Bagging（Bootstrap-Aggre
      
    
    </summary>
    
      <category term="算法总结" scheme="https://www.vhcffh.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Bagging" scheme="https://www.vhcffh.com/tags/Bagging/"/>
    
      <category term="Boosting" scheme="https://www.vhcffh.com/tags/Boosting/"/>
    
      <category term="Gradient Boosting" scheme="https://www.vhcffh.com/tags/Gradient-Boosting/"/>
    
  </entry>
  
  <entry>
    <title>信息论的一些基本概念</title>
    <link href="https://www.vhcffh.com/2019/%E4%BF%A1%E6%81%AF%E8%AE%BA%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.vhcffh.com/2019/信息论的一些基本概念/</id>
    <published>2019-08-18T05:56:19.000Z</published>
    <updated>2019-08-18T06:00:38.502Z</updated>
    
    <content type="html"><![CDATA[<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><script type="math/tex; mode=display">H(X)=E[I(X)]=E[-ln(P(X))]</script><p>其中$P$ 为$X$的概率质量函数,$E$为期望函数,而$I(x)$是$X$ 的信息量(又称自信息).</p><script type="math/tex; mode=display">H(X)=\sum_iP(x_i)I(x_i)=-\sum_iP(x_i)\log_bP(x_i)</script><script type="math/tex; mode=display">\begin{matrix}b & 熵的单位\\2 & bit\\e & nat\\10 & Hart\end{matrix}</script><h3 id="条件熵-Conditional-Entropy"><a href="#条件熵-Conditional-Entropy" class="headerlink" title="条件熵(Conditional Entropy)"></a>条件熵(Conditional Entropy)</h3><p>特征$x$ 固定为$x_i$时:$H(c|x_i)$ </p><p>特征$x$ 整体分布已知时:$H(x|X)$ </p><h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益(Information Gain)"></a>信息增益(Information Gain)</h3><script type="math/tex; mode=display">IG(X) = H(c)-H(c|X)</script><h3 id="基尼系数-基尼不纯度Gini-impurity"><a href="#基尼系数-基尼不纯度Gini-impurity" class="headerlink" title="基尼系数(基尼不纯度Gini impurity)"></a>基尼系数(基尼不纯度Gini impurity)</h3><script type="math/tex; mode=display">Gini(D)=1-\sum_i^np_i^2</script><script type="math/tex; mode=display">Gini(D|A)=\sum_i^n\frac {D_i}{D}</script><h3 id="信息增益比率-Information-Gain-Ratio-与分裂信息-Split-information"><a href="#信息增益比率-Information-Gain-Ratio-与分裂信息-Split-information" class="headerlink" title="信息增益比率(Information Gain Ratio)与分裂信息(Split information)"></a>信息增益比率(Information Gain Ratio)与分裂信息(Split information)</h3><script type="math/tex; mode=display">GR(D|A)=\frac {IG(D|A)}{SI(D|A)}</script><script type="math/tex; mode=display">SI(D|A)=-\sum_i^n\frac {N_i}{N}\log_2\frac{N_i}{N}</script><h3 id="边界熵-boundary-entropy"><a href="#边界熵-boundary-entropy" class="headerlink" title="边界熵(boundary entropy)"></a>边界熵(boundary entropy)</h3><script type="math/tex; mode=display">BE(w_1w_2\cdots w_k) = -\sum_{w \in C}p(w\vert w_1w_2\cdots w_k)\log p(w\vert w_1w_2\cdots w_k)</script><p>$w$是邻接于$w_1w_2\cdots w_k$ 的字符.</p><h3 id="边界多样性-Accessor-veriety-AV"><a href="#边界多样性-Accessor-veriety-AV" class="headerlink" title="边界多样性(Accessor veriety,AV)"></a>边界多样性(Accessor veriety,AV)</h3><script type="math/tex; mode=display">AV(w_1w_2\cdots w_k)=\log RL_{av}(w_1w_2\cdots w_k)</script><p>$RL_{av}$ 表示邻接于字符串$w_1w_2\cdots w_k$的不同字符个数.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;信息熵&quot;&gt;&lt;a href=&quot;#信息熵&quot; class=&quot;headerlink&quot; title=&quot;信息熵&quot;&gt;&lt;/a&gt;信息熵&lt;/h3&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
H(X)=E[I(X)]=E[-ln(P(X))]&lt;/scr
      
    
    </summary>
    
      <category term="数学" scheme="https://www.vhcffh.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="基础知识" scheme="https://www.vhcffh.com/categories/%E6%95%B0%E5%AD%A6/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="信息论" scheme="https://www.vhcffh.com/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
      <category term="熵" scheme="https://www.vhcffh.com/tags/%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>mathjax配置问题</title>
    <link href="https://www.vhcffh.com/2019/mathjax%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/"/>
    <id>https://www.vhcffh.com/2019/mathjax配置问题/</id>
    <published>2019-08-10T15:07:51.000Z</published>
    <updated>2019-08-20T10:34:17.490Z</updated>
    
    <content type="html"><![CDATA[<p>使用hexo时，想要实现网页中公式的渲染<br>发现不管怎么改，都不能渲染单行公式<br>最后发现是在mathjax的2.3版本以后，配置方法变了</p><h3 id="mathjax的配置方法"><a href="#mathjax的配置方法" class="headerlink" title="mathjax的配置方法"></a>mathjax的配置方法</h3><p>一般网上大部分的mathjax的配置如下</p><pre><code class="lang-html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;  MathJax.Hub.Config({    tex2jax: {      inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],      processEscapes: true    }  });&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;path-to-MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;</code></pre><p>其中配置中这一句主要是增加对单行公式的渲染</p><pre><code class="lang-javascript">inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],</code></pre><p>有时候会发现无论如修改单行公式总是不能渲染<br>原因是在mathjax的2.3版本以后，应该这样配置</p><pre><code class="lang-html">&lt;script type=&quot;text/javascript&quot;&gt;  window.MathJax = {    tex2jax: {      inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\(&quot;,&quot;\\)&quot;] ],      processEscapes: true    }  };&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;path-to-MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;</code></pre><p>对于<code>hexo</code>默认转义规则使单行公式显示错误的问题,查看<a href="https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/" target="_blank" rel="noopener">这篇博客</a></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1,<a href="https://docs.mathjax.org/en/latest/configuration.html" target="_blank" rel="noopener">https://docs.mathjax.org/en/latest/configuration.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用hexo时，想要实现网页中公式的渲染&lt;br&gt;发现不管怎么改，都不能渲染单行公式&lt;br&gt;最后发现是在mathjax的2.3版本以后，配置方法变了&lt;/p&gt;
&lt;h3 id=&quot;mathjax的配置方法&quot;&gt;&lt;a href=&quot;#mathjax的配置方法&quot; class=&quot;header
      
    
    </summary>
    
      <category term="软件使用" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="mathjax" scheme="https://www.vhcffh.com/tags/mathjax/"/>
    
  </entry>
  
  <entry>
    <title>pytorch使用和损失函数</title>
    <link href="https://www.vhcffh.com/2019/pytorch%E4%BD%BF%E7%94%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://www.vhcffh.com/2019/pytorch使用和损失函数/</id>
    <published>2019-07-26T09:17:27.000Z</published>
    <updated>2019-08-11T09:20:41.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h2><pre><code class="lang-python">#激活函数x=np.arange(-12.5,12.5,0.05)tanh = (np.power(np.e,x)-np.power(np.e,-x))/(np.power(np.e,x)+np.power(np.e,-x))relu = np.maximum(0.0,x)sigmoid = 1.0/(1.0+np.power(np.e,-x))</code></pre><pre><code class="lang-python">torch.nn.Sigmoid()</code></pre><script type="math/tex; mode=display">Sigmoid(x)=\frac {1}{1+e^{-x}}</script><script type="math/tex; mode=display">\frac {1}{1+e^{-x}}-\frac{1}{2}=\frac{1-e^{-x}}{2(1+e^{-x})}=-\frac{1-e^x}{2(1+e^x)}</script><pre><code class="lang-python">torch.nn.Tanh</code></pre><script type="math/tex; mode=display">Tanh(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><script type="math/tex; mode=display">\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{1-e^{-2x}}{1+e^{-2x}}=\frac{2}{1+e^{-2x}}-1=2Sigmoid(2x)-1</script><p>BatchNorm2d</p><p>对每一个特征进行正则</p><script type="math/tex; mode=display">y=\frac {x-E[x]} {\sqrt{Var[x]+\varepsilon}}*\gamma+\beta \ \ \ \ \ (\varepsilon=10^{-5})</script><p>pytorch中的正则化函数</p><pre><code class="lang-python">torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)</code></pre><script type="math/tex; mode=display">v=\frac {v}{max(\lVert v \rVert_p,\varepsilon)}</script><pre><code class="lang-python">torch.norm(input, p=&#39;fro&#39;, dim=None, keepdim=False, out=None, dtype=None)# 对维度dim求p范数</code></pre><pre><code class="lang-python">torch.Tensor.squeeze()-&gt;Tensor#维度压缩torch.cat(tensors, dim=0, out=None)-&gt;Tensor #维度拼接torch.stack(tensors,dim=0,out=None)-&gt;Tensor #张量拼接# cat是把多张纸拼成一张纸,stack是把纸摞起来torch.Tensor.repeat()-&gt;Tensor #矩阵扩展torch.Tensor.transpose()-&gt;Tensor #矩阵转置torch.eq() #张量比较torch.chunk() #张量分块torch.split(tensor, split_size_or_sections, dim=0) #张量分块</code></pre><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss"></a>L1Loss</h3><pre><code class="lang-python">torch.nn.L1Loss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = \left| x_n - y_n \right|,</script><script type="math/tex; mode=display">        \ell(x, y) =        \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss"></a>MSELoss</h3><pre><code class="lang-python">torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = \left( x_n - y_n \right)^2,</script><script type="math/tex; mode=display">\ell(x, y) =        \begin{cases}            \operatorname{mean}(L), &  \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  &  \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p>交叉熵损失函数,多分类</p><pre><code class="lang-python">torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)</script><script type="math/tex; mode=display">        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</script><pre><code class="lang-python">torch.nn.CTCLoss(blank=0, reduction=&#39;mean&#39;, zero_infinity=False)</code></pre><h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><p>负对数似然损失函数(Negative Loss Likelihood),多分类</p><pre><code class="lang-python">torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = - w_{y_n} x_{n,y_n}, \quad        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\},</script><script type="math/tex; mode=display">        \ell(x, y) = \begin{cases}            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &            \text{if reduction} = \text{'mean';}\\            \sum_{n=1}^N l_n,  &            \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="PoissonNLLLoss"><a href="#PoissonNLLLoss" class="headerlink" title="PoissonNLLLoss"></a>PoissonNLLLoss</h3><pre><code class="lang-python">torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{target} \sim \mathrm{Poisson}(\text{input})        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})                                    + \log(\text{target!})</script><h3 id="KLDivLoss"><a href="#KLDivLoss" class="headerlink" title="KLDivLoss"></a>KLDivLoss</h3><p>KL散度,又叫相对熵,计算两个分布之间的距离,越相近越接近零</p><pre><code class="lang-python">torch.nn.KLDivLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        l(x,y) = L = \{ l_1,\dots,l_N \}, \quad        l_n = y_n \cdot \left( \log y_n - x_n \right)</script><script type="math/tex; mode=display">\ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';} \\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h3><p>二分类用的交叉熵</p><pre><code class="lang-python">torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</script><script type="math/tex; mode=display">\ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="BCEWithLogitsLoss"><a href="#BCEWithLogitsLoss" class="headerlink" title="BCEWithLogitsLoss"></a>BCEWithLogitsLoss</h3><p>增加了一个Sigmoid层</p><pre><code class="lang-python">torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;, pos_weight=None)</code></pre><script type="math/tex; mode=display">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</script><script type="math/tex; mode=display">        \ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="MarginRankingLoss"><a href="#MarginRankingLoss" class="headerlink" title="MarginRankingLoss"></a>MarginRankingLoss</h3><p>评价相似度的损失</p><pre><code class="lang-python">torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})</script><h3 id="HingeEmbeddingLoss"><a href="#HingeEmbeddingLoss" class="headerlink" title="HingeEmbeddingLoss"></a>HingeEmbeddingLoss</h3><p>用于学习非线性嵌入或半监督学习</p><pre><code class="lang-python">torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">L = \{l_1,\dots,l_N\}^\top, \quadl_n = \begin{cases}            x_n, & \text{if}\; y_n = 1,\\            \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,        \end{cases}</script><script type="math/tex; mode=display">        \ell(x, y) = \begin{cases}            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}        \end{cases}</script><h3 id="MultiLabelMarginLoss"><a href="#MultiLabelMarginLoss" class="headerlink" title="MultiLabelMarginLoss"></a>MultiLabelMarginLoss</h3><p>多类别多分类的Hinge损失</p><pre><code class="lang-python">torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</script><p>其中$x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}$,$y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}$,$0 \leq y[j] \leq \text{x.size}(0)-1$,$i \neq y[j]$</p><h3 id="SmoothL1Loss"><a href="#SmoothL1Loss" class="headerlink" title="SmoothL1Loss"></a>SmoothL1Loss</h3><p>也叫Huber Loss,误差在(-1,1)上是平方损失,其他情况是L1损失</p><pre><code class="lang-python">torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</script><script type="math/tex; mode=display">z_{i} =        \begin{cases}        0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\        |x_i - y_i| - 0.5, & \text{otherwise }        \end{cases}</script><h3 id="SoftMarginLoss"><a href="#SoftMarginLoss" class="headerlink" title="SoftMarginLoss"></a>SoftMarginLoss</h3><p>多标签二分类问题</p><pre><code class="lang-python">torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</script><h3 id="MultiLabelSoftMarginLoss"><a href="#MultiLabelSoftMarginLoss" class="headerlink" title="MultiLabelSoftMarginLoss"></a>MultiLabelSoftMarginLoss</h3><p>多标签多分类</p><pre><code class="lang-python">torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)</script><p>其中$i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}$,$y[i] \in \left\{0, \; 1\right\}$</p><h3 id="CosineEmbeddingLoss"><a href="#CosineEmbeddingLoss" class="headerlink" title="CosineEmbeddingLoss"></a>CosineEmbeddingLoss</h3><p>余玄相似度损失</p><pre><code class="lang-python">torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">        \text{loss}(x, y) =        \begin{cases}        1 - \cos(x_1, x_2), & \text{if } y = 1 \\        \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1        \end{cases}</script><h3 id="MultiMarginLoss"><a href="#MultiMarginLoss" class="headerlink" title="MultiMarginLoss"></a>MultiMarginLoss</h3><p>多分类的Hinge损失</p><pre><code>torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">\text{loss}(x, y) = \frac{\sum_i \max(0, (\text{margin} - x[y] + x[i])^p)}{\text{x.size}(0)}</script><p>其中$x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}$,$i \neq y$</p><script type="math/tex; mode=display">\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i])^p)}{\text{x.size}(0)}</script><h3 id="TripletMarginLoss"><a href="#TripletMarginLoss" class="headerlink" title="TripletMarginLoss"></a>TripletMarginLoss</h3><pre><code class="lang-python">torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></pre><script type="math/tex; mode=display">L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</script><script type="math/tex; mode=display">d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;激励函数&quot;&gt;&lt;a href=&quot;#激励函数&quot; class=&quot;headerlink&quot; title=&quot;激励函数&quot;&gt;&lt;/a&gt;激励函数&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;#激活函数
x=np.arange(-12.5,12.5,0.05)
      
    
    </summary>
    
      <category term="深度学习" scheme="https://www.vhcffh.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pytorch" scheme="https://www.vhcffh.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/"/>
    
    
      <category term="损失函数" scheme="https://www.vhcffh.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
      <category term="tensor变换" scheme="https://www.vhcffh.com/tags/tensor%E5%8F%98%E6%8D%A2/"/>
    
  </entry>
  
  <entry>
    <title>常见损失函数</title>
    <link href="https://www.vhcffh.com/2019/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://www.vhcffh.com/2019/常见损失函数/</id>
    <published>2019-07-21T08:58:19.000Z</published>
    <updated>2019-08-11T09:17:00.137Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-损失函数"><a href="#1-损失函数" class="headerlink" title="1.损失函数"></a>1.损失函数</h3><blockquote><p><strong>损失函数</strong>是指一种将一个事件（在一个<a href="https://wiki.mbalib.com/wiki/%E6%A0%B7%E6%9C%AC" target="_blank" rel="noopener">样本</a>空间中的一个元素）映射到一个表达与其事件<a href="https://wiki.mbalib.com/wiki/%E7%9B%B8%E5%85%B3" target="_blank" rel="noopener">相关</a>的<a href="https://wiki.mbalib.com/wiki/%E7%BB%8F%E6%B5%8E%E6%88%90%E6%9C%AC" target="_blank" rel="noopener">经济成本</a>或<a href="https://wiki.mbalib.com/wiki/%E6%9C%BA%E4%BC%9A%E6%88%90%E6%9C%AC" target="_blank" rel="noopener">机会成本</a>的实数上的一种函数,较常运用在<a href="https://wiki.mbalib.com/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6" target="_blank" rel="noopener">统计学</a>，<a href="https://wiki.mbalib.com/wiki/%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA" target="_blank" rel="noopener">统计决策理论</a>和<a href="https://wiki.mbalib.com/wiki/%E7%BB%8F%E6%B5%8E%E5%AD%A6" target="_blank" rel="noopener">经济学</a>中。损失函数参数的真值为（θ），<a href="https://wiki.mbalib.com/wiki/%E5%86%B3%E7%AD%96" target="_blank" rel="noopener">决策</a>的结果为<em>d</em> ，两者的不一致会带来一定的损失，这种损失是一个<a href="https://wiki.mbalib.com/wiki/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F" target="_blank" rel="noopener">随机变量</a>，用<em>L</em>(θ,<em>d</em>)表示。</p></blockquote><h4 id="1-1-0-1损失函数-0-1-loss-function"><a href="#1-1-0-1损失函数-0-1-loss-function" class="headerlink" title="1.1 0-1损失函数(0-1 loss function)"></a>1.1 0-1损失函数(0-1 loss function)</h4><script type="math/tex; mode=display">L(Y,f(X))=\begin{cases}1,Y \neq f(X)\\0,Y = f(X)\end{cases}</script><h4 id="1-2平方损失函数-quadratic-loss-function"><a href="#1-2平方损失函数-quadratic-loss-function" class="headerlink" title="1.2平方损失函数(quadratic loss function)"></a>1.2平方损失函数(quadratic loss function)</h4><script type="math/tex; mode=display">L(Y,f(X))=(Y-f(X))^2</script><h4 id="1-3绝对损失函数-absolute-loss-function"><a href="#1-3绝对损失函数-absolute-loss-function" class="headerlink" title="1.3绝对损失函数(absolute loss function)"></a>1.3绝对损失函数(absolute loss function)</h4><script type="math/tex; mode=display">L(Y,f(X))=|Y-f(X)|</script><h4 id="1-4对数损失函数-logarithmic-loss-function-或对数似然损失函数-log-likelihood-loss-function"><a href="#1-4对数损失函数-logarithmic-loss-function-或对数似然损失函数-log-likelihood-loss-function" class="headerlink" title="1.4对数损失函数(logarithmic loss function)或对数似然损失函数(log likelihood loss function)"></a>1.4对数损失函数(logarithmic loss function)或对数似然损失函数(log likelihood loss function)</h4><script type="math/tex; mode=display">L(Y,P(Y|X))=-logP(Y|X)</script><h3 id="2-风险函数"><a href="#2-风险函数" class="headerlink" title="2.风险函数"></a>2.风险函数</h3><p><strong>风险函数</strong>是损失函数的期望值，表示为：</p><script type="math/tex; mode=display">R(\theta,d)=E[L(d,\theta)]*R*(θ,*d*) = *E*[*L*(*d*,θ)]。</script><p>决策的目标是要找出一个决策方案$d$，使其对各个自然状态风险值均为最小。应用时，常常对θ(参数的真值)确定一个概率分布，并使其平均的风险值$r(d,\theta)$达到最小，其中：</p><script type="math/tex; mode=display">r(d,\theta) = E[R(d,\theta)]=\sum_{j=1}^LR(d,\theta)p(\theta_j)</script><p>有结构风险函数和经验风险函数</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1,<a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-损失函数&quot;&gt;&lt;a href=&quot;#1-损失函数&quot; class=&quot;headerlink&quot; title=&quot;1.损失函数&quot;&gt;&lt;/a&gt;1.损失函数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;损失函数&lt;/strong&gt;是指一种将一个事件（在一个&lt;a href
      
    
    </summary>
    
    
      <category term="损失函数" scheme="https://www.vhcffh.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的前向传播与后向传播</title>
    <link href="https://www.vhcffh.com/2019/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>https://www.vhcffh.com/2019/神经网络中的前向传播与后向传播/</id>
    <published>2019-07-10T13:22:25.000Z</published>
    <updated>2019-08-10T14:08:23.265Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2019/7_10_neu.png" alt="img"></p><p>$f(z)$为激励函数，关于激励函数(又称激活函数)的总结<br>隐藏层1输入</p><script type="math/tex; mode=display">z^{(1)}=W^{(1)}x^T+b^{(1)}\label{1}\tag{1}</script><p>隐藏层1输出</p><script type="math/tex; mode=display">n^{(1)}=f^{(1)}(z^{(1)})\label{2}\tag{2}</script><p>隐藏层2输入</p><script type="math/tex; mode=display">z^{(2)}=W^{(2)}n^{(1)}+b^{(2)}\label{3}\tag{3}</script><p>隐藏层2输出</p><script type="math/tex; mode=display">n^{(2)}=f^{(2)}(z^{(2)})\label{4}\tag{4}</script><p>隐藏层3输入</p><script type="math/tex; mode=display">z^{(3)}=W^{(3)}n^{(2)}+b^{(3)}\label{5}\tag{5}</script><p>隐藏层3输出即输出层</p><script type="math/tex; mode=display">\widehat y = n^{(3)}= f^{(3)}(z{(3)})\label{6}\tag{6}</script><p>损失函数</p><script type="math/tex; mode=display">L(y,\widehat y)\label{7}\tag{7}</script><p>即隐藏层k+1输入</p><script type="math/tex; mode=display">z^{(k+1)}=W^{(k+1)}n^{(k)}+b^{(k+1)}\label{8}\tag{8}</script><p>隐藏层k+1输出</p><script type="math/tex; mode=display">n^{(k+1)}= f^{(k+1)}(z{(k+1)})\label{9}\tag{9}</script><p>对损失函数进行总结<a href="https://blog.csdn.net/lien0906/article/details/78429768" target="_blank" rel="noopener">https://blog.csdn.net/lien0906/article/details/78429768</a><br>计算偏导数</p><script type="math/tex; mode=display">\frac {\partial z^{(k)}}{\partial b^{(k)}}=\begin{bmatrix}\frac{\partial (W^{(k)}_{[1,:]}*n^{(k-1)}+b_1)}{\partial b_1} & \ldots & \frac{\partial (W^{(k)}_{[1,:]}*n^{(k-1)}+b_1)}{\partial b_m}\\\vdots & \ddots & \vdots\\\frac{\partial (W^{(k)}_{[m,:]}*n^{(k-1)}+b_m)}{\partial b_1}  &\ldots & \frac{\partial (W^{(k)}_{[m,:]}*n^{(k-1)}+b_m)}{\partial b_m}\end{bmatrix}=diag(1,1, \ldots ,1)\label{10}\tag{10}</script><p>列向量对列向量求导参见矩阵中的求导</p><p>计算偏导数$\frac {\partial L(y,\widehat y)}{\partial z^{(k)}}​$ </p><p>偏导数$\frac {\partial L(y,\widehat y)}{\partial z^{(k)}}$ 又称误差项(error term,也称“灵敏度”),一般用$\delta$ 表示,用$\delta^{(k)}$ 表示第k层神经元的误差项,其值的大小代表了<strong>第k层神经元对最终总误差的影响大小</strong> </p><script type="math/tex; mode=display">\begin{align}\delta^{(k)} & = \frac {\partial L(y,\widehat y)}{\partial z^{(k)}}\\& =\frac {\partial n^{(k)}}{\partial z^{(k)}}*\frac {\partial z^{(k+1)}}{\partial n^{(k)}}*\frac {\partial L(y,\widehat y)}{\partial z^{(k+1)}}\\& = {f^{(k)}}^{'}(z^{(k)}) * (W^{(k+1)})^T * \delta^{(k+1)}\end{align}\label{11}\tag{11}</script><p>最终需要用的两个导数</p><script type="math/tex; mode=display">\frac {\partial L(y,\widehat y)}{\partial W^{(k)}}=\frac {\partial L(y,\widehat y)}{\partial z^{(k)}}*\frac {\partial z^{(k)}}{\partial W^{(k)}}=\delta^{(k)}*(n^{(k-1)})^T\label{12}\tag{12}</script><script type="math/tex; mode=display">\frac {\partial L(y,\widehat y)}{\partial b^{(k)}}=\frac {\partial L(y,\widehat y)}{\partial z^{(k)}}*\frac {\partial z^{(k)}}{\partial b^{(k)}}=\delta^{(k)}\label{13}\tag{13}</script><p>后向传播参数更新</p><script type="math/tex; mode=display">W^{(k)} = W^{(k)} - \alpha(\delta^{(k)}(n^{(k-1)})^T + W^{(k)})\label{14}\tag{14}\\</script><script type="math/tex; mode=display">b^{(k)} = b^{(k)}-\alpha\delta^{(k)}\label{15}\tag{15}</script><p>其中$\alpha$ 是学习率</p><p>后向传播中的正则化,L1正则化,L2正则化</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/2019/7_10_neu.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;$f(z)$为激励函数，关于激励函数(又称激活函数)的总结&lt;br&gt;隐藏层1输入&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
      
    
    </summary>
    
      <category term="深度学习" scheme="https://www.vhcffh.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="BP算法" scheme="https://www.vhcffh.com/tags/BP%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中Keras vs Pytorch</title>
    <link href="https://www.vhcffh.com/2019/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADKeras%20vs%20Pytorch/"/>
    <id>https://www.vhcffh.com/2019/深度学习中Keras vs Pytorch/</id>
    <published>2019-07-09T00:11:00.000Z</published>
    <updated>2019-08-10T12:58:17.469Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习框架Keras与Pytorch的区别与优劣<a href="https://towardsdatascience.com/keras-vs-pytorch-for-deep-learning-a013cb63870d" target="_blank" rel="noopener">翻译</a><br><img src="/images/2019/7_11_pytorchvskears.png" alt="img" title="Keras vs PyTorch"></p><p>对于许多科学家，工程师和开发人员来说，TensorFlow是他们的第一个深度学习框架。 TensorFlow 1.0于2017年2月发布;但它对用户来说不是很友好。</p><p>在过去几年中，两个主要的深度学习库已经获得了巨大的普及，主要是因为它们比TensorFlow更容易使用：<strong>Keras</strong>和<strong>Pytorch</strong>。</p><p>本文将介绍Keras与Pytorch的4个不同点，以及为什么您可以选择一个库而不是另一个库。</p><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><p>Keras本身不是一个框架，但实际上是一个位于其他Deep Learning框架之上的高级API。目前它支持<a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a>，<a href="http://deeplearning.net/software/theano/" target="_blank" rel="noopener">Theano</a>和<a href="https://github.com/microsoft/CNTK" target="_blank" rel="noopener">CNTK</a>。</p><p>Keras的优势在于它的易用性。它是迄今为止最容易去快速启动和运行的框架。定义神经网络非常直观，使用功能API允许人们将层定义为函数。</p><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p>Pytorch是由Facebook的AI研究小组开发的深度学习框架（如TensorFlow）。像Keras一样，它也对深度网络编程中比较复杂的一大部分进行了抽象。</p><p>在高级和低级编码风格方面，Pytorch位于Keras和TensorFlow之间。你比Keras有更多的灵活性和控制力，但与此同时你不必做任何疯狂的声明性编程。</p><p>深度学习练习者整天都在争论应该使用哪个框架。一般来说，这取决于个人喜好。但是在选择时你应该记住Keras和Pytorch的一些特性。</p><p><img src="/images/2019/7_10_keras.jpeg" alt="img" title="It&#39;s keras"></p><p><img src="/images/2019/7_10_pytorch.jpeg" alt="img" title="It&#39;s Pytorch!"></p><h2 id="1-用于定义模型的-类-Pytorch-vs-函数-Keras"><a href="#1-用于定义模型的-类-Pytorch-vs-函数-Keras" class="headerlink" title="(1)用于定义模型的 类(Pytorch) vs 函数(Keras)"></a>(1)用于定义模型的 类(Pytorch) vs 函数(Keras)</h2><p>要定义深度学习模型，Keras提供Functional API。 使用Functional API，神经网络被定义为一组顺序函数，一个接一个地应用。 例如，第一层的输出是第二层的输入。</p><pre><code class="lang-python">img_input = layers.Input(shape=input_shape)x = layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)(img_input)    x = layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)(x)    x = layers.MaxPooling2D((2, 2), strides=(2, 2))(x)</code></pre><p>在Pytorch中，您将网络设置为一个类，该类继承了Torch库中的torch.nn.Module。 与Keras类似，Pytorch为您提供了层作为构建块，但由于它们位于Python类中，因此它们在类的__init__()方法中引用，并由类的forward()方法执行。</p><pre><code class="lang-python">class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.conv1 = nn.Conv2d(3, 64, 3)        self.conv2 = nn.Conv2d(64, 64, 3)        self.pool = nn.MaxPool2d(2, 2)    def forward(self, x):        x = F.relu(self.conv1(x))        x = self.pool(F.relu(self.conv2(x)))        return xmodel = Net()</code></pre><p>因为Pytorch允许您访问所有Python的类功能而不是简单的函数调用，所以定义网络可以更清晰，更优雅地包含。 这真的没什么不好的，除非你觉得最重要的是尽可能快地编写代码，那么Keras会更容易使用。</p><h2 id="2-张量和计算图-Pytorch-vs-标准阵列-Keras"><a href="#2-张量和计算图-Pytorch-vs-标准阵列-Keras" class="headerlink" title="(2)张量和计算图(Pytorch) vs 标准阵列(Keras)"></a>(2)张量和计算图(Pytorch) vs 标准阵列(Keras)</h2><p>Keras API对程序员隐藏了的许多复杂细节。定义网络层非常直观，默认设置通常足以让您入门。</p><p>只有当你需要实现一个相当尖端或“异国情调”的模型时，你才真正需要去了解底层的TensorFlow。</p><p>棘手的部分是，当你真正去了解底层的TensorFlow代码时，你将获得随之而来的所有非常具有挑战性的部分！ 您需要确保所有矩阵乘法都排成一行。 哦，甚至不要去考虑尝试打印出图层的一个输出，因为您只能在终端上打印出一个漂亮的Tensor定义。</p><p>Pytorch在这些方面倾向于更加方便。 您需要知道每个层的输入和输出大小，但这可以很快掌握。 您不必处理构建一个您无法在调试中看到的抽象计算图。</p><p>Pytorch的另一个好处是你可以在Torch Tensors和Numpy阵列之间来回滑动。 如果你需要实现自定义的东西，那么在TF张量和Numpy阵列之间来回转换可能会很麻烦，需要开发人员对TensorFlow的Session有充分的了解。</p><p>Pytorch的交互性比想象中要简单得多。 您只需要知道两个操作：一个将Torch Tensor（一个Variable对象）转变到Numpy，另一个是相反的转换。</p><pre><code class="lang-python">a = torch.Tensor(2,2)print(a)b = a.numpy() #tensor 变为numpyprint(b)print(torch.from_numpy(b)) # numpy 变为tensor</code></pre><p>当然，如果你不需要实现任何花哨的东西，那么Keras会做得很好，因为你不会遇到任何TensorFlow障碍。 但如果你这样做，那么Pytorch可能会更顺畅。</p><h2 id="3-Training-models"><a href="#3-Training-models" class="headerlink" title="(3) Training models"></a>(3) Training models</h2><p>在Keras训练模型非常容易！ 只是一个简单的.fit()函数，你可以很轻松的进行训练。</p><pre><code class="lang-python">history = model.fit_generator(    generator=train_generator,    epochs=10,    validation_data=validation_generator)</code></pre><p>在Pytorch中训练一些模型需要一些步骤</p><ul><li>每一批次的训练开始时初始化梯度</li><li>在模型中运行前向传播</li><li>运行后向传播</li><li>计算损失和更新权重</li></ul><p>所以，就训练模型来说，PyTorch 较为繁琐。</p><pre><code class="lang-python">for epoch in range(2):  # loop over the dataset multiple times    running_loss = 0.0    for i, data in enumerate(trainloader, 0):        # Get the inputs; data is a list of [inputs, labels]        inputs, labels = data        # (1) Initialise gradients        optimizer.zero_grad()        # (2) Forward pass        outputs = net(inputs)        loss = criterion(outputs, labels)        # (3) Backward        loss.backward()        # (4) Compute the loss and update the weights        optimizer.step()</code></pre><h2 id="4-控制-CPU-vs-GPU-模式"><a href="#4-控制-CPU-vs-GPU-模式" class="headerlink" title="(4)控制 CPU vs GPU 模式"></a>(4)控制 CPU vs GPU 模式</h2><p>如果你已经安装了 tensorflow-gpu，则在 Keras 中能够使用 GPU 并且会默认完成。然后，如果你想要将某些运算转移至 CPU，则可以以单行方式完成。</p><pre><code class="lang-python">with tf.device(&#39;/cpu:0&#39;):    y = apply_non_max_suppression(x)</code></pre><p>但对于 PyTorch 来说，你必须显式地为每个 torch 张量和 numpy 变量启动 GPU。这样代码会比较混乱。并且如果你想在 CPU 和 GPU 之间来回移动以执行不同运算，则很容易出错。</p><p>例如，为了将之前的模型转移到 GPU 上运行，则需要以下步骤：</p><pre><code class="lang-python"># Get the GPU devicedevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# Transfer the network to GPUnet.to(device)# Transfer the inputs and labels to GPUinputs, labels = data[0].to(device), data[1].to(device)</code></pre><p>因而，Keras 在简洁性和默认设置方面优于 PyTorch。</p><h2 id="选择-Keras-或-PyTorch-的一般性建议"><a href="#选择-Keras-或-PyTorch-的一般性建议" class="headerlink" title="选择 Keras 或 PyTorch 的一般性建议"></a>选择 Keras 或 PyTorch 的一般性建议</h2><p>作者通常建议初学者从 Keras 开始。Keras 绝对是理解和使用起来最简单的框架，能够很快地上手运行。你完全不需要担心 GPU 设置、处理抽象代码以及其他任何复杂的事情。你甚至可以在不接触任何 TensorFlow 单行代码的情况下，实现自定义层和损失函数。</p><p>但如果你开始深度了解到深度网络的更细粒度层面或者正在实现一些非标准的事情，则 PyTorch 是你的首选库。使用 PyTorch 需要进行一些额外操作，但这不会减缓你的进程。你依然能够快速实现、训练和测试网络，并享受简单调试带来的额外益处。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度学习框架Keras与Pytorch的区别与优劣&lt;a href=&quot;https://towardsdatascience.com/keras-vs-pytorch-for-deep-learning-a013cb63870d&quot; target=&quot;_blank&quot; rel=&quot;no
      
    
    </summary>
    
      <category term="深度学习" scheme="https://www.vhcffh.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Keras" scheme="https://www.vhcffh.com/tags/Keras/"/>
    
      <category term="Pytorch" scheme="https://www.vhcffh.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>优化算法总结</title>
    <link href="https://www.vhcffh.com/2019/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://www.vhcffh.com/2019/优化算法总结/</id>
    <published>2019-07-04T13:16:52.000Z</published>
    <updated>2019-08-11T15:29:09.636Z</updated>
    
    <content type="html"><![CDATA[<h3 id="AdaGrad-Adaptive-Gradient-自适应梯度"><a href="#AdaGrad-Adaptive-Gradient-自适应梯度" class="headerlink" title="AdaGrad(Adaptive Gradient)自适应梯度"></a>AdaGrad(Adaptive Gradient)自适应梯度</h3><script type="math/tex; mode=display">g_t = \nabla_\theta J(\theta_{t-1})</script><a id="more"></a><script type="math/tex; mode=display">\theta_{t+1} = \theta_t-\alpha\cdot g_t/\sqrt{\sum_{i=1}^tg_t^2} \ \ \ \ \ (\alpha=0.01)</script><p>随梯度的变化改变学习率</p><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><script type="math/tex; mode=display">g_t = \nabla_\theta J(\theta_{t-1})</script><script type="math/tex; mode=display">v_t= \gamma(v_{t-1})+(1-\gamma)g_t^2 \ \ \ \ \ (\gamma=0.9)</script><script type="math/tex; mode=display">\theta_{t} = \theta_{t-1}-\alpha* g_t/(\sqrt v_t + \varepsilon) \ \ \ \ \ (\alpha=0.001,\varepsilon=10^{-8})</script><p>结合梯度平方的指数移动平均数来调节学习率的变化</p><p>克服AdaGrad梯度急剧减小的问题</p><h3 id="Adam优化器"><a href="#Adam优化器" class="headerlink" title="Adam优化器"></a>Adam优化器</h3><script type="math/tex; mode=display">g_t = \nabla_\theta J(\theta_{t-1})</script><script type="math/tex; mode=display">m_t = \beta_1m_{t-1}+(1-\beta_1)g_t \ \ \ \ \ (\beta_1=0.9,m_0=0)</script><script type="math/tex; mode=display">v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2 \ \ \ \ \ (\beta_2=0.999,v_0=0)</script><script type="math/tex; mode=display">\hat m_t = m_t/(1-\beta_1^t)</script><script type="math/tex; mode=display">\hat v_t = v_t/(1-\beta_2^t)</script><script type="math/tex; mode=display">\theta_t = \theta_{t-1}-\alpha * \hat m_t/(\hat v_t + \varepsilon) \ \ \ \ \ (\alpha=0.001,\varepsilon=10^{-8})</script><p>对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second</p><p>Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。</p><h3 id="下面是对几种梯度下降算法的模拟"><a href="#下面是对几种梯度下降算法的模拟" class="headerlink" title="下面是对几种梯度下降算法的模拟"></a>下面是对几种梯度下降算法的模拟</h3><p>其中包括随机梯度下降算法SGD，小批量梯度下降算法MSGD，批量梯度下降算法BGD</p><p>都是根据损失函数计算参数梯度，更新参数;不一样的地方是他们使用使用训练集的方式</p><pre><code class="lang-python"># 随机梯度下降算法SGDx=np.random.rand(10000,5)w=np.array([[1,2,3,4,5],[20,30,40,50,60]])b=np.array([[10],[100]])y=(w.dot(x.T)+b).T #100,2lr=0.5ww = np.random.rand(2,5)bb = np.random.rand(2,1)mb=master_bar(range(1000000))mb.names=[&#39;loss&#39;]lossy=[]for i in mb:    r=np.random.randint(1000)    xi=x[r:r+1,:] #1,5    yi=y[r:r+1,:] #1,2    yy = ww.dot(xi.T)+bb #2,1    loss = np.mean(np.power(yy-yi.T,2)/2)    lossy.append(loss)    graphs = [[np.arange(len(lossy)),lossy]]    mb.update_graph(graphs)    ww=  ww - lr*(yy-yi.T).dot(xi)    bb=  bb - lr*(yy-yi.T)    if loss &lt; 1e-10:        break    #print(loss)print(ww,bb)print(w,b)# 小批量梯度下降算法MSGDx=np.random.rand(1000,5)w=np.array([[1,2,3,4,5],[20,30,40,50,60]])b=np.array([[10],[100]])y=(w.dot(x.T)+b).T #100,2lr=0.3ww = np.random.rand(2,5)bb = np.random.rand(2,1)mb=master_bar(range(200*3))mb.names=[&#39;loss&#39;]lossy=[]batch = 50for i in mb:    lossg,wwg,bbg=0,0,0    for j in progress_bar(range(batch),parent=mb):        r=np.random.randint(1000)        xi=x[r:r+1,:] #1,5        yi=y[r:r+1,:] #1,2        yy = ww.dot(xi.T)+bb #2,10        lossg += np.mean(np.power(yy-yi.T,2)/2)        wwg += (yy-yi.T).dot(xi)        bbg += (yy-yi.T)    lossy.append(lossg/batch)    graphs = [[np.arange(len(lossy)),lossy]]    mb.update_graph(graphs)    ww=  ww - lr*wwg/batch    bb=  bb - lr*bbg/batch    if lossg/batch &lt; 1e-4:        break    #print(loss)print(ww,bb)print(w,b)# 批量梯度下降算法BGDx=np.random.rand(1000,5)w=np.array([[1,2,3,4,5],[20,30,40,50,60]])b=np.array([[10],[100]])y=(w.dot(x.T)+b).T #100,2lr=0.3#ww = np.random.rand(2,5)#bb = np.random.rand(2,1)mb=master_bar(range(200*3))mb.names=[&#39;loss&#39;]lossy=[]batch = 1000for i in mb:    lossg,wwg,bbg=0,0,0    for j in progress_bar(range(batch),parent=mb):        r=j        xi=x[r:r+1,:] #1,5        yi=y[r:r+1,:] #1,2        yy = ww.dot(xi.T)+bb #2,10        lossg += np.mean(np.power(yy-yi.T,2)/2)        wwg += (yy-yi.T).dot(xi)        bbg += (yy-yi.T)    lossy.append(lossg/batch)    graphs = [[np.arange(len(lossy)),lossy]]    mb.update_graph(graphs)    ww=  ww - lr*wwg/batch    bb=  bb - lr*bbg/batch    if lossg/batch &lt; 1e-4:        break    #print(loss)print(ww,bb)print(w,b)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;AdaGrad-Adaptive-Gradient-自适应梯度&quot;&gt;&lt;a href=&quot;#AdaGrad-Adaptive-Gradient-自适应梯度&quot; class=&quot;headerlink&quot; title=&quot;AdaGrad(Adaptive Gradient)自适应梯度&quot;&gt;&lt;/a&gt;AdaGrad(Adaptive Gradient)自适应梯度&lt;/h3&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
g_t = \nabla_\theta J(\theta_{t-1})&lt;/script&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://www.vhcffh.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="AdaGrad" scheme="https://www.vhcffh.com/tags/AdaGrad/"/>
    
      <category term="RMSProp" scheme="https://www.vhcffh.com/tags/RMSProp/"/>
    
      <category term="Adam" scheme="https://www.vhcffh.com/tags/Adam/"/>
    
  </entry>
  
  <entry>
    <title>Python中遇到的问题</title>
    <link href="https://www.vhcffh.com/2019/python%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://www.vhcffh.com/2019/python中遇到的问题/</id>
    <published>2019-05-31T00:35:00.000Z</published>
    <updated>2019-08-10T10:27:56.857Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-列表的初始化"><a href="#1-列表的初始化" class="headerlink" title="1.列表的初始化"></a>1.列表的初始化</h3><p>当初始化一个n×n的列表时不能使用如下方法，</p><pre><code class="lang-python">In [1]: l=[[0]*3]*3 #如此初始化会导致其它行仅是第一行的引用而不是copyIn [2]: lOut[2]: [[0, 0, 0], [0, 0, 0], [0, 0, 0]In [3]: l[0][0]=1 #改变其中一行的某个元素In [4]: lOut[4]: [[1, 0, 0], [1, 0, 0], [1, 0, 0]] #其他行跟着改变</code></pre><p>正确的方法应该如下</p><pre><code class="lang-python">In [5]: l=[[0 for _ in range(3)] for _ in range(3)] #或者l=[[0]*3 for _ in range(3)]In [6]: lOut[6]: [[0, 0, 0], [0, 0, 0], [0, 0, 0]]In [7]: l[0][0]=1In [8]: lOut[8]: [[1, 0, 0], [0, 0, 0], [0, 0, 0]]</code></pre><h3 id="2-a-is-b与a-b-的区别"><a href="#2-a-is-b与a-b-的区别" class="headerlink" title="2.a is b与a==b 的区别"></a>2.a is b与a==b 的区别</h3><pre><code>a=&#39;vhcffh.com&#39;b=&#39;vhcffh.com&#39;a==b # True,a和b对应实例的内容是相同的a is b # False,a和b指向不同的实例b=aa is b # True,a和b指向同一个实例</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-列表的初始化&quot;&gt;&lt;a href=&quot;#1-列表的初始化&quot; class=&quot;headerlink&quot; title=&quot;1.列表的初始化&quot;&gt;&lt;/a&gt;1.列表的初始化&lt;/h3&gt;&lt;p&gt;当初始化一个n×n的列表时不能使用如下方法，&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;l
      
    
    </summary>
    
      <category term="编程" scheme="https://www.vhcffh.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Python" scheme="https://www.vhcffh.com/categories/%E7%BC%96%E7%A8%8B/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Python常用方法</title>
    <link href="https://www.vhcffh.com/2019/python%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://www.vhcffh.com/2019/python常用方法/</id>
    <published>2019-05-29T21:26:00.000Z</published>
    <updated>2019-08-10T10:28:18.177Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-sorted排序"><a href="#1-sorted排序" class="headerlink" title="1.sorted排序"></a>1.sorted排序</h3><pre><code class="lang-python">sorted(iterable[, cmp[, key[, reverse]]])</code></pre><p>参数说明：</p><ul><li>iterable — 可迭代对象。</li><li>cmp — 比较的函数，这个具有两个参数，参数的值都是从可迭代对象中取出，此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。</li><li>key — 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。</li><li>reverse — 排序规则，reverse = True 降序 ， reverse = False 升序（默认）。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-sorted排序&quot;&gt;&lt;a href=&quot;#1-sorted排序&quot; class=&quot;headerlink&quot; title=&quot;1.sorted排序&quot;&gt;&lt;/a&gt;1.sorted排序&lt;/h3&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;sorted(ite
      
    
    </summary>
    
      <category term="编程" scheme="https://www.vhcffh.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Python" scheme="https://www.vhcffh.com/categories/%E7%BC%96%E7%A8%8B/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux常用命令</title>
    <link href="https://www.vhcffh.com/2019/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://www.vhcffh.com/2019/linux常用命令/</id>
    <published>2019-05-20T21:22:00.000Z</published>
    <updated>2019-08-10T10:27:34.127Z</updated>
    
    <content type="html"><![CDATA[<p>Linux中的一些常用命令,tar<br><a id="more"></a></p><h2 id="1-tar解压打包相关"><a href="#1-tar解压打包相关" class="headerlink" title="1.tar解压打包相关"></a>1.tar解压打包相关</h2><pre><code class="lang-shell">tar -cvf log.tar log2019.log    #仅打包，不压缩tar -zcvf log.tar.gz log2019.log   #打包后，以 gzip 压缩tar -jcvf log.tar.bz2 log2019.log  #打包后，以 bzip2 压缩tar -ztvf log.tar.gz #gzip查阅tar -jtvf log.tar.gz #bzip2查阅tar -zxvf log.tar.gz #gzip解压tar -jxvf log.tar.gz #bzip2解压unzip log.zip -d dirname #zip解压到dirname目录</code></pre><p>补充</p><p>c: 建立压缩档案<br>-x：解压<br>-t：查看内容<br>-r：向压缩归档文件末尾追加文件<br>-u：更新原压缩包中的文件</p><p>这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。</p><p>-z：有gzip属性的<br>-j：有bz2属性的<br>-Z：有compress属性的<br>-v：显示所有过程<br>-O：将文件解开到标准输出 </p><h2 id="2-wget设置代理"><a href="#2-wget设置代理" class="headerlink" title="2.wget设置代理"></a>2.wget设置代理</h2><h3 id="2-1环境变量中设置"><a href="#2-1环境变量中设置" class="headerlink" title="2.1环境变量中设置"></a>2.1环境变量中设置</h3><pre><code class="lang-shell">export http_proxy=http://127.0.0.1:8087</code></pre><h3 id="2-2使用配置文件"><a href="#2-2使用配置文件" class="headerlink" title="2.2使用配置文件"></a>2.2使用配置文件</h3><pre><code class="lang-shell"># cp /etc/wgetrc ~/.wgetrc# vim ~/.wgetrc# You can set the default proxies for Wget to use for http, https, and ftp.# They will override the value in the environment.https_proxy = http://127.0.0.1:8087/http_proxy = http://127.0.0.1:8087/ftp_proxy = http://127.0.0.1:8087/# If you do not want to use proxy at all, set this to off.use_proxy = on</code></pre><h2 id="3-ps相关命令"><a href="#3-ps相关命令" class="headerlink" title="3.ps相关命令"></a>3.ps相关命令</h2><pre><code class="lang-shell">ps -ef # 查看进程信息和执行的命令</code></pre><h2 id="4-用户文件管理相关"><a href="#4-用户文件管理相关" class="headerlink" title="4.用户文件管理相关"></a>4.用户文件管理相关</h2><h3 id="4-1Linux系统用户账号的管理"><a href="#4-1Linux系统用户账号的管理" class="headerlink" title="4.1Linux系统用户账号的管理"></a>4.1Linux系统用户账号的管理</h3><h4 id="4-1-1添加新的用户账号"><a href="#4-1-1添加新的用户账号" class="headerlink" title="4.1.1添加新的用户账号"></a>4.1.1添加新的用户账号</h4><pre><code class="lang-shell">useradd [-d dirname] username# [-d dirname]指定用户主目录</code></pre><h4 id="4-1-2删除帐号"><a href="#4-1-2删除帐号" class="headerlink" title="4.1.2删除帐号"></a>4.1.2删除帐号</h4><pre><code class="lang-shell">userdel [-r] username# [-R]把用户的主目录一起删除</code></pre><h4 id="4-1-3修改帐号"><a href="#4-1-3修改帐号" class="headerlink" title="4.1.3修改帐号"></a>4.1.3修改帐号</h4><pre><code class="lang-shell">usermod [] username</code></pre><h4 id="4-1-4用户密码管理"><a href="#4-1-4用户密码管理" class="headerlink" title="4.1.4用户密码管理"></a>4.1.4用户密码管理</h4><pre><code class="lang-shell">passwd [-I][-u][-d][-f] username# [-I]锁定密码，即禁用账号。# [-u]密码解锁。# [-d]使账号无密码。# [-f]强迫用户下次登录时修改密码。</code></pre><h3 id="4-2Linux系统用户组的管理"><a href="#4-2Linux系统用户组的管理" class="headerlink" title="4.2Linux系统用户组的管理"></a>4.2Linux系统用户组的管理</h3><h4 id="4-2-1增加一个新的用户组"><a href="#4-2-1增加一个新的用户组" class="headerlink" title="4.2.1增加一个新的用户组"></a>4.2.1增加一个新的用户组</h4><pre><code class="lang-shell">groupadd [-g GID] username# [-g GID]指定新用户组的组标识号（GID）</code></pre><h4 id="4-2-2删除一个已有的用户组"><a href="#4-2-2删除一个已有的用户组" class="headerlink" title="4.2.2删除一个已有的用户组"></a>4.2.2删除一个已有的用户组</h4><pre><code class="lang-shell">groupdel groupname</code></pre><h4 id="4-2-3修改用户组的属性"><a href="#4-2-3修改用户组的属性" class="headerlink" title="4.2.3修改用户组的属性"></a>4.2.3修改用户组的属性</h4><pre><code class="lang-shell">groupmod [-g GID] groupname# [-g GID]指定新用户组的组标识号（GID）</code></pre><h4 id="4-2-4更改用户组"><a href="#4-2-4更改用户组" class="headerlink" title="4.2.4更改用户组"></a>4.2.4更改用户组</h4><p><strong>如果一个用户同时属于多个用户组，那么用户可以在用户组之间切换，以便具有其他用户组的权限。用户可以在登录后，使用命令newgrp切换到其他用户组，这个命令的参数就是目的用户组。</strong></p><pre><code class="lang-shell">newgrp root</code></pre><h3 id="4-3Linux文件用户属性修改"><a href="#4-3Linux文件用户属性修改" class="headerlink" title="4.3Linux文件用户属性修改"></a>4.3Linux文件用户属性修改</h3><pre><code class="lang-shell">chown [-R] user_name filename_OR_dirname  # 更改文件或目录的所有者chgrp [-R] group_name filename_OR_dirname  # 更改文件或目录所在组# [-R]参数递归更改目录下所有文件的用户属性</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="http://man.linuxde.net/tar" target="_blank" rel="noopener">http://man.linuxde.net/tar</a></p><p>2.<a href="https://www.cnblogs.com/cloud2rain/archive/2013/03/22/2976337.html" target="_blank" rel="noopener">https://www.cnblogs.com/cloud2rain/archive/2013/03/22/2976337.html</a></p><p>3.<a href="https://www.cnblogs.com/52php/p/5677628.html" target="_blank" rel="noopener">https://www.cnblogs.com/52php/p/5677628.html</a></p><p>4.<a href="https://www.jb51.net/LINUXjishu/43356.html" target="_blank" rel="noopener">https://www.jb51.net/LINUXjishu/43356.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux中的一些常用命令,tar&lt;br&gt;
    
    </summary>
    
      <category term="软件使用" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/"/>
    
      <category term="Linux" scheme="https://www.vhcffh.com/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/Linux/"/>
    
    
      <category term="linux" scheme="https://www.vhcffh.com/tags/linux/"/>
    
  </entry>
  
</feed>
