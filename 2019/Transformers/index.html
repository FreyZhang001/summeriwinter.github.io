<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Transformers | Frey's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script></head><script src="https://www.googletagmanager.com/gtag/js?id=UA-111205240-3" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-111205240-3');
</script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: "ca-pub-3597458182538053",
  enable_page_level_ads: true
});</script><script custom-element="amp-ad" src="https://cdn.ampproject.org/v0/amp-ad-0.1.js" async></script><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">Frey's blog</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">Frey's blog</a></h1></div><p class="m-desc">空悲眼界高，敢怨人间小。<br>越不爱人间，越觉人间好。</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" action="/search" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"></form></li></ul><div id="local-search-result"></div></div><amp-ad width="100vw" height="320" type="adsense" data-ad-client="ca-pub-3597458182538053" data-ad-slot="7295042689" data-auto-format="rspv" data-full-width><div overflow></div></amp-ad></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Transformers</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2019/Transformers/">2019-12-21</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/论文笔记/">论文笔记</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p><a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a>（与<em>pytorch-transformers</em>和<em>pytorch-pretrained-bert</em>相似）是python的一个库，它提供了用于自然语言理解(NLU)和自然语言生成(NLG)的多种预训练模型（BERT，GPT-2，RoBERTa，XLM，DistilBert，XLNet…..），为100多种语言提供了超过32种的预训练模型，并实现Tensorflow 2.0和Pytorch的深度互操作。</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>与pytorch-transformers一样容易使用</li>
<li>如Keras一样强大简洁</li>
<li>在NLU和NLG任务上有很好的表现</li>
<li>容易学习</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ol>
<li><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">BERT</a> (from Google) released with the paper <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li>
<li><a href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener">GPT</a> (from OpenAI) released with the paper <a href="https://blog.openai.com/language-unsupervised" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.</li>
<li><a href="https://blog.openai.com/better-language-models" target="_blank" rel="noopener">GPT-2</a> (from OpenAI) released with the paper <a href="https://blog.openai.com/better-language-models" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a> by Alec Radford<em>, Jeffrey Wu</em>, Rewon Child, David Luan, Dario Amodei<strong> and Ilya Sutskever</strong>.</li>
<li><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">Transformer-XL</a> (from Google/CMU) released with the paper <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> by Zihang Dai<em>, Zhilin Yang</em>, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</li>
<li><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">XLNet</a> (from Google/CMU) released with the paper <a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> by Zhilin Yang<em>, Zihang Dai</em>, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.</li>
<li><a href="https://github.com/facebookresearch/XLM" target="_blank" rel="noopener">XLM</a> (from Facebook) released together with the paper <a href="https://arxiv.org/abs/1901.07291" target="_blank" rel="noopener">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li>
<li><a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank" rel="noopener">RoBERTa</a> (from Facebook), released together with the paper a <a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener">Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li>
<li><a href="https://huggingface.co/transformers/model_doc/distilbert.html" target="_blank" rel="noopener">DistilBERT</a> (from HuggingFace) released together with the paper <a href="https://arxiv.org/abs/1910.01108" target="_blank" rel="noopener">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into <a href="https://github.com/huggingface/transformers/tree/master/examples/distillation" target="_blank" rel="noopener">DistilGPT2</a>.</li>
<li><a href="https://github.com/pytorch/fairseq/tree/master/examples/ctrl" target="_blank" rel="noopener">CTRL</a> (from Salesforce), released together with the paper <a href="https://www.github.com/salesforce/ctrl" target="_blank" rel="noopener">CTRL: A Conditional Transformer Language Model for Controllable Generation</a> by Nitish Shirish Keskar<em>, Bryan McCann</em>, Lav R. Varshney, Caiming Xiong and Richard Socher.</li>
<li><a href="https://huggingface.co/transformers/model_doc/camembert.html" target="_blank" rel="noopener">CamemBERT</a> (from FAIR, Inria, Sorbonne Université) released together with the paper <a href="https://arxiv.org/abs/1911.03894" target="_blank" rel="noopener">CamemBERT: a Tasty French Language Model</a> by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suarez, Yoann Dupont, Laurent Romary, Eric Villemonte de la Clergerie, Djame Seddah, and Benoît Sagot.</li>
<li><a href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener">ALBERT</a> (from Google Research), released together with the paper a <a href="https://arxiv.org/abs/1909.11942" target="_blank" rel="noopener">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.</li>
<li><a href="https://github.com/pytorch/fairseq/tree/master/examples/xlmr" target="_blank" rel="noopener">XLM-RoBERTa</a> (from Facebook AI), released together with the paper <a href="https://arxiv.org/abs/1911.02116" target="_blank" rel="noopener">Unsupervised Cross-lingual Representation Learning at Scale</a> by Alexis Conneau<em>, Kartikay Khandelwal</em>, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</li>
</ol>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>对于每个模型，这个库主要涉及3个类</p>
<ul>
<li>model classes：提供模型的结构，如：<code>BertModel</code></li>
<li>configuration classes：存储构建模型的所有参数，如<code>BertConfig</code></li>
<li>tokenizer classes：提供模型的词汇表和用于对字符串解码/编码的方法，如<code>BertTokenizer</code></li>
</ul>
<p>所有的实例可以通过<code>from_pretrained()</code>加载预训练模型，通过<code>save_pretrained()</code>保存模型</p>
<h3 id="BERT-example"><a href="#BERT-example" class="headerlink" title="BERT example"></a>BERT example</h3><p>从字符串获得输入向量</p>
<pre><code class="lang-python">import torch
from transformers import BertTokenizer, BertModel, BertForMaskedLM

# OPTIONAL: if you want to have more information on what&#39;s happening under the hood, activate the logger as follows
import logging
logging.basicConfig(level=logging.INFO)

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
# 中文bert模型&#39;bert-base-chinese&#39;
# bert 特殊的符号
# &#39;[MASK]&#39; 用于mask language model
# &#39;[CLS]&#39; 开头，用于分类的标记
# &#39;[PAD]&#39; 
# &#39;[SEP]&#39; 结尾，句子分隔符
# &#39;[UNK]&#39;

# Tokenize input
text = &quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;
tokenized_text = tokenizer.tokenize(text)

# Mask a token that we will try to predict back with `BertForMaskedLM`
masked_index = 8
tokenized_text[masked_index] = &#39;[MASK]&#39;
assert tokenized_text == [&#39;[CLS]&#39;, &#39;who&#39;, &#39;was&#39;, &#39;jim&#39;, &#39;henson&#39;, &#39;?&#39;, &#39;[SEP]&#39;, &#39;jim&#39;, &#39;[MASK]&#39;, &#39;was&#39;, &#39;a&#39;, &#39;puppet&#39;, &#39;##eer&#39;, &#39;[SEP]&#39;]

# Convert token to vocabulary indices
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
</code></pre>
<p>对输入进行编码</p>
<pre><code class="lang-python"># Load pre-trained model (weights)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)

# Set the model in evaluation mode to deactivate the DropOut modules
# This is IMPORTANT to have reproducible results during evaluation!
model.eval()

# If you have a GPU, put everything on cuda
tokens_tensor = tokens_tensor.to(&#39;cuda&#39;)
segments_tensors = segments_tensors.to(&#39;cuda&#39;)
model.to(&#39;cuda&#39;)

# Predict hidden states features for each layer
with torch.no_grad():
    # See the models docstrings for the detail of the inputs
    outputs = model(tokens_tensor, token_type_ids=segments_tensors)
    # Transformers models always output tuples.
    # See the models docstrings for the detail of all the outputs
    # In our case, the first element is the hidden state of the last layer of the Bert model
    encoded_layers = outputs[0]
# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)
assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)
</code></pre>
<p>使用<code>BertForMaskedLM</code>预测masked token</p>
<pre><code class="lang-python"># Load pre-trained model (weights)
model = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)
model.eval()

# If you have a GPU, put everything on cuda
tokens_tensor = tokens_tensor.to(&#39;cuda&#39;)
segments_tensors = segments_tensors.to(&#39;cuda&#39;)
model.to(&#39;cuda&#39;)

# Predict all tokens
with torch.no_grad():
    outputs = model(tokens_tensor, token_type_ids=segments_tensors)
    predictions = outputs[0]

# confirm we were able to predict &#39;henson&#39;
predicted_index = torch.argmax(predictions[0, masked_index]).item()
predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
assert predicted_token == &#39;henson&#39;
</code></pre>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1.<a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">https://huggingface.co/transformers/index.html</a></p>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">Frey</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2019/Transformers/">https://www.vhcffh.com/2019/Transformers/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.vhcffh.com">Frey的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/NLP/">NLP</a><a href="/tags/transformer/">transformer</a></span></div></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/数列求解总结/">&lt; 数列求解总结</a><a class="next" href="/2019/机器学习中的术语/">机器学习笔记 &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'IYkIGtHfOiXLr2xvQwvikmRk-gzGzoHsz',
  appKey:'Hw0Ta8iM7YvN1RscFx37Dwh1',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">Frey's blog</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"></body></html>