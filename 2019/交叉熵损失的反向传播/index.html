<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>交叉熵损失的反向传播 | Frey's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script></head><script src="https://www.googletagmanager.com/gtag/js?id=UA-111205240-3" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-111205240-3');
</script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: "ca-pub-3597458182538053",
  enable_page_level_ads: true
});</script><script custom-element="amp-ad" src="https://cdn.ampproject.org/v0/amp-ad-0.1.js" async></script><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">Frey's blog</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">Frey's blog</a></h1></div><p class="m-desc">空悲眼界高，敢怨人间小。<br>越不爱人间，越觉人间好。</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" action="/search" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"></form></li></ul><div id="local-search-result"></div></div><amp-ad width="100vw" height="320" type="adsense" data-ad-client="ca-pub-3597458182538053" data-ad-slot="7295042689" data-auto-format="rspv" data-full-width><div overflow></div></amp-ad></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">交叉熵损失的反向传播</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2019/交叉熵损失的反向传播/">2019-10-02</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/神经网络/">神经网络</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p>对于一个单标签多分类问题,假设网络的输出层的输入为$Z_{in}=[z_1,\cdots,z_i,\cdots,z_n]$,输出为$\hat Y=[\hat y_1,\cdots,\hat y_i,\cdots,\hat y_n]$,真实类标签为$Y = [y_1,\cdots,y_i,\cdots,y_n]$,$n$为类别数(输出层神经元数),通常有:</p>
<script type="math/tex; mode=display">
\hat Y = Softmax(Z_{in})\label{1}\tag{1}</script><script type="math/tex; mode=display">
\hat y_i = \frac {e^{z_i}}{\sum_{j=0}^n e^{z_j}}\label{2}\tag{2}</script><a id="more"></a>
<p>其中$Softmax​$为:</p>
<script type="math/tex; mode=display">
Softmax(Z_{in}) = [\cdots,\frac {e^{z_i}}{\sum_{j=1}^n e^{z_j}},\cdots]\label{3}\tag{3}</script><h3 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h3><script type="math/tex; mode=display">
Loss(Y,\hat Y) = -\sum_{i=1}^ny_i*\ln(\hat y_i)\label{4}\tag{4}</script><p>损失对神经网络输出的偏导(<a href="https://www.vhcffh.com/2019/矩阵中的求导/">标量对向量求偏导</a>)为:</p>
<script type="math/tex; mode=display">
\frac {\partial Loss(Y,\hat Y)}{\partial \hat Y} = [-\frac {y_1}{\hat y_1},\cdots,-\frac {y_i}{\hat y_i},\cdots,-\frac {y_n}{\hat y_n}]\label{5}\tag{5}</script><p>后向传播推导中遇到的所有量都是变量,最终的目的是找到损失关于某变量的偏导,程序中也只用这个公式求得对应输入点的梯度</p>
<h3 id="Softmax的偏导"><a href="#Softmax的偏导" class="headerlink" title="Softmax的偏导"></a>Softmax的偏导</h3><p>求$\hat y_i​$对$z_i​$的偏导,根据$\eqref{2}​$可得:</p>
<p>求$\hat Y$对$Z$的偏导(<a href="https://www.vhcffh.com/2019/矩阵中的求导/">向量对向量求导</a>)</p>
<p>这里把$y$的坐标写作$k$</p>
<p>$k=i$时:</p>
<script type="math/tex; mode=display">
\begin{split}
\frac {\partial \hat y_i}{\partial z_i}
&= e^{z_i} * \frac {1}{\sum_{j=1}^ne^{z_j}} +
   e^{z_i} * (-\frac {1}{(\sum_{j=1}^n e^{z_j})^2}) * e^{z_i} \\
&= \frac {e^{z_i}}{\sum_{j=1}^n e^{z_j}} - (\frac {e^{z_i}}{\sum_{j=1}^n e^{z_j}})^2 \\
&= \hat y_i - \hat y_i^2
\end{split}
\label{6}\tag{6}</script><p>$k \neq i$时:</p>
<script type="math/tex; mode=display">
\begin{split}
\frac {\partial \hat y_k}{\partial z_i}
&= e^{z_k} * (-\frac {1}{(\sum_{j=1}^n e^{z_j})^2}) * e^{z_i} \\
&= -\frac {e^{z_k}*e^{z_i}}{(\sum_{j=1}^n e^{z_j})^2} \\
&= - \hat y_k\hat y_i
\end{split}
\label{7}\tag{7}</script><p>写成矩阵:</p>
<script type="math/tex; mode=display">
\frac {\partial \hat Y}{\partial Z_{in}} = 
\begin{bmatrix}
\hat y_1-\hat y_1^2 & -\hat y_2\hat y_1 & \cdots & -\hat y_n\hat y_1\\
-\hat y_1\hat y_2 & \hat y_2-\hat y_2^2 & \cdots & -\hat y_n\hat y_2\\
\vdots & \vdots & \ddots & \vdots\\
-\hat y_1\hat y_n & -\hat y_2\hat y_n & \cdots & \hat y_n-\hat y_n^2\\
\end{bmatrix}\label{8}\tag{8}</script><p>这是一个对称矩阵,在链式求导时加不加转置都一样</p>
<p>根据$\eqref{5}$和$\eqref{8}$,损失$L$对输入$Z$的偏导(<a href="https://www.vhcffh.com/2019/矩阵中的求导/">标量对向量求偏导</a>):</p>
<script type="math/tex; mode=display">
\begin{align}
\frac {\partial L(Y,\hat Y)}{\partial Z}
&= \frac {\partial L(Y,\hat Y)}{\partial \hat Y} (\frac {\partial \hat Y}{\partial Z})^T\\
&= [-\frac {y_1}{\hat y_1},\cdots,-\frac {y_i}{\hat y_i},\cdots,-\frac {y_n}{\hat y_n}]
\begin{bmatrix}
\hat y_1-\hat y_1^2 & -\hat y_2\hat y_1 & \cdots & -\hat y_n\hat y_1\\
-\hat y_1\hat y_2 & \hat y_2-\hat y_2^2 & \cdots & -\hat y_n\hat y_2\\
\vdots & \vdots & \ddots & \vdots\\
-\hat y_1\hat y_n & -\hat y_2\hat y_n & \cdots & \hat y_n-\hat y_n^2\\
\end{bmatrix}^T\\
&= [(\hat y_1-1)y_1+\hat y_1y_2+\cdots+\hat y_1y_n,\hat y_2y_1+(\hat y_2-1)y_2+\cdots+\hat y_1y_n,\cdots]\\
&= [\hat y_1\sum_{i=1}^ny_i-y_1,\cdots,\hat y_j\sum_{i=1}^ny_i-y_j,\cdots,\hat y_n\sum_{i=1}^ny_i-y_n]\\
&= [\hat y_1-y_1,\cdots,\hat y_j-y_j,\cdots,\hat y_n-y_n]\ \ \ \ (\sum_{i=1}^ny_i=1)\\
&= \hat Y-Y
\end{align}</script></div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">Frey</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2019/交叉熵损失的反向传播/">https://www.vhcffh.com/2019/交叉熵损失的反向传播/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.vhcffh.com">Frey的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/反向传播/">反向传播</a><a href="/tags/交叉熵损失/">交叉熵损失</a><a href="/tags/Softmax/">Softmax</a></span></div></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2019/矩阵中的求导/">&lt; 矩阵中的求导</a><a class="next" href="/2019/日常笔记-1/">日常笔记-1 &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'IYkIGtHfOiXLr2xvQwvikmRk-gzGzoHsz',
  appKey:'Hw0Ta8iM7YvN1RscFx37Dwh1',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">Frey's blog</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><!--mathjax config similar to math.stackexchange--><script>window.MathJax = {
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
    }
};</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS_HTML" async></script></body></html>