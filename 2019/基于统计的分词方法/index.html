<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>基于统计的分词方法 | Frey's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script></head><script src="https://www.googletagmanager.com/gtag/js?id=UA-111205240-3" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-111205240-3');
</script><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: "ca-pub-3597458182538053",
  enable_page_level_ads: true
});</script><script custom-element="amp-ad" src="https://cdn.ampproject.org/v0/amp-ad-0.1.js" async></script><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">Frey's blog</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">Frey's blog</a></h1></div><p class="m-desc">空悲眼界高，敢怨人间小。<br>越不爱人间，越觉人间好。</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" action="/search" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"></form></li></ul><div id="local-search-result"></div></div><amp-ad width="100vw" height="320" type="adsense" data-ad-client="ca-pub-3597458182538053" data-ad-slot="7295042689" data-auto-format="rspv" data-full-width><div overflow></div></amp-ad></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">基于统计的分词方法</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2019/基于统计的分词方法/">2019-09-08</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/自然语言处理/">自然语言处理</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><h2 id="1-n元语言模型-n-gram"><a href="#1-n元语言模型-n-gram" class="headerlink" title="1.n元语言模型(n-gram)"></a>1.n元语言模型(n-gram)</h2><p>假设$S$表示长度为$i$，由$(w_1,w_2,\dots,w_m)$字序列组成的句子，则代表$S​$的概率为：</p>
<script type="math/tex; mode=display">
P(S) = P(w_1,w_2,\dots,w_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_2,w_1)\cdots P(w_i|w_1,w_2,\dots,w_{m-1})</script><ul>
<li><p>$n=1$,uni-gram</p>
<script type="math/tex; mode=display">
P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i)</script></li>
<li><p>$n=2$,bi-gram</p>
<script type="math/tex; mode=display">
P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i|w_{i-1})</script></li>
<li><p>$n=3$,tri-gram</p>
<script type="math/tex; mode=display">
P(w_1,w_2,\dots,w_m) =\prod_{i=1}^mP(w_i|w_{i-2}w_{i-1})</script></li>
</ul>
<h2 id="2-基于HMM的分词"><a href="#2-基于HMM的分词" class="headerlink" title="2.基于HMM的分词"></a>2.基于HMM的分词</h2><p><strong>HMM的参数</strong></p>
<p>观测序列(输出状态序列),状态序列(隐藏状态序列),初始概率,转移概率(转移概率矩阵),发射概率(发射概率矩阵)<br><img src="/images/2019/9_8_ma.png"><br><strong>数学定义</strong></p>
<p>状态值集合(隐藏状态):$Q={q_1,q_2,\cdots,q_N}$,$N$为可能的状态数,对应状态序列$I​$</p>
<p>观测值集合(输出状态):$V={v_1,v_2,\cdots,v_M}$,$M$为可能的观测数,对应观测序列$O$</p>
<p>转移概率矩阵:$A=[a<em>{ij}];i,j\in{1,2,\cdots,N}$,从$i$状态到$j$状态的转换概率($\sum</em>{j=1}^Na_{ij}=1$)</p>
<p>发射概率矩阵(观测概率矩阵):$B=[b_j(k)];j\in{1,2,\cdots,N},k\in{1,2,\cdots,M}$,从状态$j$生成观测$k$的概率</p>
<p>初始状态分布:$\pi$</p>
<p>模型:$\lambda=(A,B,\lambda)$,状态序列$I$,观测序列$O$</p>
<p><strong>HMM中的三个问题:</strong></p>
<ul>
<li>概率计算问题:已知模型,求观测序列$O$出现的概率;前向后向算法<br><img src="/images/2019/9_8_ma.png"></li>
</ul>
<ul>
<li><p>学习问题:已知观测序列,求模型参数,最大化$P(O|\lambda)$;鲍姆-韦尔奇(Baum-Welch)算法</p>
<ul>
<li><p>已知隐藏序列和观测序列(通过频数估计)</p>
<p>$A_{ij}$表示隐藏状态$q_i$转移到$q_j$的频率计数</p>
<script type="math/tex; mode=display">
A=[a_{ij}];a_{ij}=\frac{A_{ij}}{\sum_{s=1}^NA_{is}}</script><p>$B_{jk}$表示隐藏状态$q_j$转移到观测状态$v_k$的频率计数</p>
<script type="math/tex; mode=display">
B=[b_j(k)];b_j(k)=\frac{B_{jk}}{\sum_{s=1}^MB_{js}}</script><p>$C(i)$为所有样本中初始隐藏状态$q_j$的频率计数</p>
<script type="math/tex; mode=display">
\Pi = \pi(i)=\frac{C(i)}{\sum_{s=1}^NC(s)}</script></li>
<li><p>仅知观测序列(鲍姆-韦尔奇算法,EM算法)</p>
<p>EM算法:最大似然估计用于没有隐变量的概率模型,EM算法可以用于有隐变量的算法模型</p>
<p>模型参数:</p>
<script type="math/tex; mode=display">
\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)</script></li>
</ul>
</li>
</ul>
<ul>
<li>解码问题:已知模型$\lambda$与观测序列$O$,求状态序列$I$最大化$P(I|O)$;维特比(Viterbi)算法</li>
</ul>
<p>HMM是一种序列模型,不仅可以用到自然语言处理这个领域,其他领域的应用也很常见:<a href="http://tecdat.cn/%e7%94%a8%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e8%af%86%e5%88%ab%e4%b8%8d%e6%96%ad%e5%8f%98%e5%8c%96%e7%9a%84%e8%82%a1%e5%b8%82%e7%8a%b6%e5%86%b5-%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab/" target="_blank" rel="noopener">股指预测</a>,<a href="https://www.jianshu.com/p/16fc3712fdf6" target="_blank" rel="noopener">语音识别</a>,<a href="http://www.c-a-m.org.cn/CN/abstract/abstract4637.shtml" target="_blank" rel="noopener">网络安全</a>,<a href="https://arxiv.org/abs/1901.06286" target="_blank" rel="noopener">基因序列</a>等方面</p>
<p>在自然语言处理中,HMM可以应用在分词,词性标注,命名实体识别等各个方面.</p>
<p>在分词方面可以这样理解HMM</p>
<p>观测序列(输出状态序列)—-序列构成的句子或短文</p>
<p>状态序列(隐藏状态序列)—-标注</p>
<p>初始概率—-统计的第一个字序的概率</p>
<p>转移概率(转移概率矩阵)—-第$i$个字序到第$i+1$个自序的标注变换概率</p>
<p>发射概率(发射概率矩阵)—-从隐藏状态到输出状态的转换概率</p>
<font color="red">如果把观测序列看作标注,状态序列看作句子,从解码问题转变成学习问题会怎样</font>

<p>在序列预测问题中</p>
<p>HMM模型:当前tag仅依赖前一个tag,当前输出仅依赖当前tag(文档的单词序列是由隐藏状态的标签决定的)</p>
<p>MEMM(最大熵马尔科夫模型)模型:当前tag取决于观察值x(单词)和前一个tag(序列的标签取决于前一个标签和当前的单词)</p>
<p>CRF模型:计算损失时把一句话看作一个整体计算损失</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.cnblogs.com/hiyoung/archive/2018/09/25/9703976.html" target="_blank" rel="noopener">https://www.cnblogs.com/hiyoung/archive/2018/09/25/9703976.html</a></p>
<p>2.<a href="http://www.52nlp.cn/hmm-learn-best-practices-one-introduction" target="_blank" rel="noopener">http://www.52nlp.cn/hmm-learn-best-practices-one-introduction</a></p>
<p>3.<a href="https://www.cnblogs.com/en-heng/p/6164145.html?utm_source=debugrun&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/en-heng/p/6164145.html?utm_source=debugrun&amp;utm_medium=referral</a></p>
<p>4.<a href="http://www.wanguanglu.com/2017/01/03/crf-introduction/" target="_blank" rel="noopener">http://www.wanguanglu.com/2017/01/03/crf-introduction/</a></p>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">Frey</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2019/基于统计的分词方法/">https://www.vhcffh.com/2019/基于统计的分词方法/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.vhcffh.com">Frey的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/分词/">分词</a><a href="/tags/统计学/">统计学</a></span></div></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2019/nvidia显卡驱动错误/">&lt; nvidia显卡驱动错误</a><a class="next" href="/2019/神经图灵机/">神经图灵机 &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'IYkIGtHfOiXLr2xvQwvikmRk-gzGzoHsz',
  appKey:'Hw0Ta8iM7YvN1RscFx37Dwh1',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2019 <a href="/." rel="nofollow">Frey's blog</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><!--mathjax config similar to math.stackexchange--><script>window.MathJax = {
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
    }
};</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS_HTML" async></script></body></html>