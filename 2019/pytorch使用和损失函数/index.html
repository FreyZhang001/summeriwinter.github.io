<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>pytorch使用和损失函数 | Frey's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script></head><script src="https://www.googletagmanager.com/gtag/js?id=UA-111205240-3" async></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-111205240-3');</script><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><body><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">Frey's blog</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">Frey's blog</a></h1></div><p class="m-desc">空悲眼界高，敢怨人间小。<br>越不爱人间，越觉人间好。</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" action="/search" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">pytorch使用和损失函数</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2019/pytorch使用和损失函数/">2019-07-26</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/深度学习/">深度学习</a>&nbsp;&bull;&nbsp;<a href="/categories/深度学习/pytorch/">pytorch</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><h2 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h2><pre><code class="lang-python">#激活函数
x=np.arange(-12.5,12.5,0.05)
tanh = (np.power(np.e,x)-np.power(np.e,-x))/(np.power(np.e,x)+np.power(np.e,-x))
relu = np.maximum(0.0,x)
sigmoid = 1.0/(1.0+np.power(np.e,-x))
</code></pre>
<pre><code class="lang-python">torch.nn.Sigmoid()
</code></pre>
<script type="math/tex; mode=display">
Sigmoid(x)=\frac {1}{1+e^{-x}}</script><script type="math/tex; mode=display">
\frac {1}{1+e^{-x}}-\frac{1}{2}=\frac{1-e^{-x}}{2(1+e^{-x})}=-\frac{1-e^x}{2(1+e^x)}</script><pre><code class="lang-python">torch.nn.Tanh
</code></pre>
<script type="math/tex; mode=display">
Tanh(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><script type="math/tex; mode=display">
\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{1-e^{-2x}}{1+e^{-2x}}=\frac{2}{1+e^{-2x}}-1=2Sigmoid(2x)-1</script><p>BatchNorm2d</p>
<p>对每一个特征进行正则</p>
<script type="math/tex; mode=display">
y=\frac {x-E[x]} {\sqrt{Var[x]+\varepsilon}}*\gamma+\beta \ \ \ \ \ (\varepsilon=10^{-5})</script><p>pytorch中的正则化函数</p>
<pre><code class="lang-python">torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)
</code></pre>
<script type="math/tex; mode=display">
v=\frac {v}{max(\lVert v \rVert_p,\varepsilon)}</script><pre><code class="lang-python">torch.norm(input, p=&#39;fro&#39;, dim=None, keepdim=False, out=None, dtype=None)
# 对维度dim求p范数
</code></pre>
<pre><code class="lang-python">torch.Tensor.squeeze()-&gt;Tensor#维度压缩
torch.cat(tensors, dim=0, out=None)-&gt;Tensor #维度拼接
torch.stack(tensors,dim=0,out=None)-&gt;Tensor #张量拼接
# cat是把多张纸拼成一张纸,stack是把纸摞起来
torch.Tensor.repeat()-&gt;Tensor #矩阵扩展
torch.Tensor.transpose()-&gt;Tensor #矩阵转置
torch.eq() #张量比较
torch.chunk() #张量分块
torch.split(tensor, split_size_or_sections, dim=0) #张量分块
</code></pre>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss"></a>L1Loss</h3><pre><code class="lang-python">torch.nn.L1Loss(size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = \left| x_n - y_n \right|,</script><script type="math/tex; mode=display">
        \ell(x, y) =
        \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss"></a>MSELoss</h3><pre><code class="lang-python">torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = \left( x_n - y_n \right)^2,</script><script type="math/tex; mode=display">
\ell(x, y) =
        \begin{cases}
            \operatorname{mean}(L), &  \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  &  \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p>交叉熵损失函数,多分类</p>
<pre><code class="lang-python">torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)</script><script type="math/tex; mode=display">
        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</script><pre><code class="lang-python">torch.nn.CTCLoss(blank=0, reduction=&#39;mean&#39;, zero_infinity=False)
</code></pre>
<h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><p>负对数似然损失函数(Negative Loss Likelihood),多分类</p>
<pre><code class="lang-python">torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_{y_n} x_{n,y_n}, \quad
        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\},</script><script type="math/tex; mode=display">
        \ell(x, y) = \begin{cases}
            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &
            \text{if reduction} = \text{'mean';}\\
            \sum_{n=1}^N l_n,  &
            \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="PoissonNLLLoss"><a href="#PoissonNLLLoss" class="headerlink" title="PoissonNLLLoss"></a>PoissonNLLLoss</h3><pre><code class="lang-python">torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\text{target} \sim \mathrm{Poisson}(\text{input})

        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})
                                    + \log(\text{target!})</script><h3 id="KLDivLoss"><a href="#KLDivLoss" class="headerlink" title="KLDivLoss"></a>KLDivLoss</h3><p>KL散度,又叫相对熵,计算两个分布之间的距离,越相近越接近零</p>
<pre><code class="lang-python">torch.nn.KLDivLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
        l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
        l_n = y_n \cdot \left( \log y_n - x_n \right)</script><script type="math/tex; mode=display">
\ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';} \\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h3><p>二分类用的交叉熵</p>
<pre><code class="lang-python">torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</script><script type="math/tex; mode=display">
\ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="BCEWithLogitsLoss"><a href="#BCEWithLogitsLoss" class="headerlink" title="BCEWithLogitsLoss"></a>BCEWithLogitsLoss</h3><p>增加了一个Sigmoid层</p>
<pre><code class="lang-python">torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;, pos_weight=None)
</code></pre>
<script type="math/tex; mode=display">
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</script><script type="math/tex; mode=display">
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="MarginRankingLoss"><a href="#MarginRankingLoss" class="headerlink" title="MarginRankingLoss"></a>MarginRankingLoss</h3><p>评价相似度的损失</p>
<pre><code class="lang-python">torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})</script><h3 id="HingeEmbeddingLoss"><a href="#HingeEmbeddingLoss" class="headerlink" title="HingeEmbeddingLoss"></a>HingeEmbeddingLoss</h3><p>用于学习非线性嵌入或半监督学习</p>
<pre><code class="lang-python">torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \begin{cases}
            x_n, & \text{if}\; y_n = 1,\\
            \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,
        \end{cases}</script><script type="math/tex; mode=display">
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}</script><h3 id="MultiLabelMarginLoss"><a href="#MultiLabelMarginLoss" class="headerlink" title="MultiLabelMarginLoss"></a>MultiLabelMarginLoss</h3><p>多类别多分类的Hinge损失</p>
<pre><code class="lang-python">torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</script><p>其中$x \in \left{0, \; \cdots , \; \text{x.size}(0) - 1\right}$,$y \in \left{0, \; \cdots , \; \text{y.size}(0) - 1\right}$,$0 \leq y[j] \leq \text{x.size}(0)-1$,$i \neq y[j]$</p>
<h3 id="SmoothL1Loss"><a href="#SmoothL1Loss" class="headerlink" title="SmoothL1Loss"></a>SmoothL1Loss</h3><p>也叫Huber Loss,误差在(-1,1)上是平方损失,其他情况是L1损失</p>
<pre><code class="lang-python">torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</script><script type="math/tex; mode=display">
z_{i} =
        \begin{cases}
        0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\
        |x_i - y_i| - 0.5, & \text{otherwise }
        \end{cases}</script><h3 id="SoftMarginLoss"><a href="#SoftMarginLoss" class="headerlink" title="SoftMarginLoss"></a>SoftMarginLoss</h3><p>多标签二分类问题</p>
<pre><code class="lang-python">torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</script><h3 id="MultiLabelSoftMarginLoss"><a href="#MultiLabelSoftMarginLoss" class="headerlink" title="MultiLabelSoftMarginLoss"></a>MultiLabelSoftMarginLoss</h3><p>多标签多分类</p>
<pre><code class="lang-python">torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)</script><p>其中$i \in \left{0, \; \cdots , \; \text{x.nElement}() - 1\right}$,$y[i] \in \left{0, \; 1\right}$</p>
<h3 id="CosineEmbeddingLoss"><a href="#CosineEmbeddingLoss" class="headerlink" title="CosineEmbeddingLoss"></a>CosineEmbeddingLoss</h3><p>余玄相似度损失</p>
<pre><code class="lang-python">torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
        \text{loss}(x, y) =
        \begin{cases}
        1 - \cos(x_1, x_2), & \text{if } y = 1 \\
        \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1
        \end{cases}</script><h3 id="MultiMarginLoss"><a href="#MultiMarginLoss" class="headerlink" title="MultiMarginLoss"></a>MultiMarginLoss</h3><p>多分类的Hinge损失</p>
<pre><code>torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre><script type="math/tex; mode=display">
\text{loss}(x, y) = \frac{\sum_i \max(0, (\text{margin} - x[y] + x[i])^p)}{\text{x.size}(0)}</script><p>其中$x \in \left{0, \; \cdots , \; \text{x.size}(0) - 1\right}$,$i \neq y$</p>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i])^p)}{\text{x.size}(0)}</script><h3 id="TripletMarginLoss"><a href="#TripletMarginLoss" class="headerlink" title="TripletMarginLoss"></a>TripletMarginLoss</h3><pre><code class="lang-python">torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=&#39;mean&#39;)
</code></pre>
<script type="math/tex; mode=display">
L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</script><script type="math/tex; mode=display">
d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</script></div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">Frey</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2019/pytorch使用和损失函数/">https://www.vhcffh.com/2019/pytorch使用和损失函数/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.vhcffh.com">Frey的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/损失函数/">损失函数</a><a href="/tags/tensor变换/">tensor变换</a></span></div></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2019/mathjax配置问题/">&lt; mathjax配置问题</a><a class="next" href="/2019/常见损失函数/">常见损失函数 &gt;</a></div><div id="valine-comment"><style type="text/css">.v * { color: #CECECE; }
.v a { color: #0F9FB4; }
.v a:hover { color: #216C73; }
.v li { list-style: inherit; }
.v .vwrap { border: 1px solid #223441; border-radius: 0; }
.v .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.v .vbtn { border-radius: 0; color: #cecece; background: none; }
.v .vlist .vcard .vh { border-bottom-color: #293D4E; }
.v .vwrap .vheader .vinput { border-bottom-color: #223441; }
.v .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.v code, .v pre,.v .vlist .vcard .vhead .vsys { background: #203240; }
.v .vlist .vcard .vcontent.expand:before { background: linear-gradient(180deg,hsla(206,33%,19%,0),hsla(206,33%,19%,.9)); }
.v .vlist .vcard .vcontent.expand:after { background: hsla(206,33%,19%,.9); }</style><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'IYkIGtHfOiXLr2xvQwvikmRk-gzGzoHsz',
  appKey:'Hw0Ta8iM7YvN1RscFx37Dwh1',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2019 <a href="/." rel="nofollow">Frey's blog</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"></body></html>